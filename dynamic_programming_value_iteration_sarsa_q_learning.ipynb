{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAt_Hjf-gJXv"
      },
      "outputs": [],
      "source": [
        "! pip install openpyxl==3.1.2     # installation of \"oepnpyxl\" library for insertion of data into excel file\n",
        "\n",
        "# restarting after installation\n",
        "import os\n",
        "os._exit(0)\n",
        "\n",
        "# no any particular verion of Python is required to run this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AeHiDlmtav50"
      },
      "outputs": [],
      "source": [
        "# making the experiments as much reproducible as possible (in this notebook, all the experiments will be fully reproducible)\n",
        "seed = 10\n",
        "from numpy.random import seed as sd\n",
        "sd(seed)\n",
        "import random\n",
        "random.seed(seed)\n",
        "# import tensorflow as tf\n",
        "# tf.compat.v1.random.set_random_seed(seed)\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['CHAINER_SEED'] = str(seed)\n",
        "# import chainer\n",
        "# chainer.cuda.cupy.random.seed(seed)\n",
        "# chainer.backends.cuda.get_device_from_id(gpu).use()\n",
        "# chainer.backends.cuda.cupy.random.seed(seed)\n",
        "# pyrandom.seed(seed)\n",
        "# with chainer.cuda.get_device_from_id(gpu):\n",
        "#   chainer.cuda.cupy.random.seed(seed)\n",
        "\n",
        "# importing of required packages and modules\n",
        "import numpy as np\n",
        "from math import floor\n",
        "from math import exp                                                              # Exponential function from the \"math\"library is used to calculate the exponential in the non-linear equations for calculating the deprivation cost and terminal panelty cost.\n",
        "import itertools as it\n",
        "import scipy.stats                                                      # This library is used to generate the demand values using the truncated normal distrubution for a particular episode.\n",
        "import copy\n",
        "import time                                                           # Exponential function from the \"math\"library is used to calculate the exponential in the non-linear equations for calculating the deprivation cost and terminal panelty cost.\n",
        "import openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Parameters**"
      ],
      "metadata": {
        "id": "EehMAfXJR7Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inorder to run different problem instances, change these parameters accordingly\n",
        "\n",
        "accessibality_based_delivery_cost_weight = 1 / 3      # weight used in the weighted sum method for accessibility based delivery cost\n",
        "deprivation_cost_weight = 1 / 3                       # weight used in the weighted sum method for deprivation cost\n",
        "\n",
        "no_aa = 4    # number of affected areas\n",
        "number_of_allocation_time_periods = 12    # number of time periods\n",
        "cap_dist = np.array([2])          # amount of relief in local resposnse center in one time period\n",
        "\n",
        "no_materials = 1                  # number of relief items, considered one in all of my experiments\n",
        "\n",
        "Len = 4       # length (in terms of hours) of one time period\n",
        "\n",
        "aaa = np.array([2.04 for i in range(0, no_aa * no_materials)])                       # first deprivation parameter used in the deprivation cost function\n",
        "bbb = np.array([0.24 for i in range(0, no_aa * no_materials)])                       # Second deprivation parameter used in the deprivation cost function\n",
        "\n",
        "cost_path = np.array([np.array([200 + 50 * i for i in range(0, no_aa * no_materials)]) for i in range(0, number_of_allocation_time_periods)]).reshape(number_of_allocation_time_periods, no_aa * no_materials)      # unit capacity's accessibilities for each affected area and local response center\n",
        "\n",
        "demand_aa_mat = 1.5 * np.array([np.array([1 for i in range(0, no_aa * no_materials)]) for i in range(0, number_of_allocation_time_periods)]).reshape(number_of_allocation_time_periods, no_aa * no_materials)       # robustified demand generated by an affected area for a relief item in one time period"
      ],
      "metadata": {
        "id": "i9WsMWx7QUcw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-f5X-C-wLdm"
      },
      "source": [
        "# **Mounting Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-Yq930zwKZf"
      },
      "outputs": [],
      "source": [
        "# mount your google drive so that experiment logs can be saved\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJkFfpGrm0Qv"
      },
      "source": [
        "# **Markov Decision Process (MDP) Environment for Reinforcement Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UrTuagREi7Ht"
      },
      "outputs": [],
      "source": [
        "no_dcl_loc_aval = 1   # number of local response centers (we are considering just one)\n",
        "cap_consum_mat = np.array([1]).reshape(no_materials, no_dcl_loc_aval)                   # capacity consumption by a particular relief item in the local response center\n",
        "\n",
        "is_loc_dec = False          # we are not using location allocation decisions in this research\n",
        "planning_horizon = Len * number_of_allocation_time_periods\n",
        "\n",
        "if is_loc_dec:\n",
        "  erect_cost = np.array([0])                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "  removal_cost = np.array([0])                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "  # a way to simplify the model and avoid unecessary complications (non-convexities)\n",
        "  fixed_cost = (erect_cost + removal_cost) / 2        # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "\n",
        "episode_counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yV2VwhOdmv9q"
      },
      "outputs": [],
      "source": [
        "# although the following environment is a generic environment, it is to be used for small-scale instances\n",
        "\n",
        "class Envir():\n",
        "  def __init__(self, accessibality_based_delivery_cost_weight = 1 / 2, deprivation_cost_weight = 1 / 2, gamma = 0.99,\n",
        "               reallocate_t = 5, is_eval = False, is_loc_dec = True, is_use_dyn_prog = False):\n",
        "\n",
        "        self.is_eval = is_eval\n",
        "        self.is_loc_dec = is_loc_dec                 # it is a boolean variable which is \"True\" if location allocation decsions are included otherwise it will be \"False\"\n",
        "        self.is_use_dyn_prog = is_use_dyn_prog\n",
        "\n",
        "        self.accessibality_based_delivery_cost_weight = accessibality_based_delivery_cost_weight\n",
        "\n",
        "        global no_aa\n",
        "        global no_materials\n",
        "        global number_of_allocation_time_periods\n",
        "        global no_dcl_loc_aval\n",
        "        global cap_dist\n",
        "        global cap_consum_mat\n",
        "        global Len\n",
        "        global planning_horizon\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          global erect_cost\n",
        "          global removal_cost\n",
        "          # a way to simplify the model and avoid unecessary complications (non-convexities)\n",
        "          global fixed_cost\n",
        "\n",
        "        global aaa\n",
        "        global bbb\n",
        "        global cost_path\n",
        "\n",
        "        global demand_aa_mat\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.fixed_cost_of_dist_centers_weight = accessibality_based_delivery_cost_weight\n",
        "\n",
        "        self.deprivation_cost_weight = deprivation_cost_weight\n",
        "        self.terminal_penality_cost_weight = deprivation_cost_weight\n",
        "\n",
        "        self.gamma = gamma\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.reallocate_t = reallocate_t                               # number of time periods after which location realloction decision for local response centers needs to be taken\n",
        "\n",
        "        self.no_aa = no_aa                                                           # Number of Affected Areas (AAs), to be entered for every instance of the problem.\n",
        "        self.no_dcl_loc_aval = no_dcl_loc_aval                                                          # number of destrubution centers (DCs) or local response centers, to be entered for every instance of the problem.\n",
        "        self.no_materials = no_materials\n",
        "        self.number_of_allocation_time_periods = number_of_allocation_time_periods                                # The number of discrete time periods into which the whole planning time period is to be divided.\n",
        "        self.steps_per_episode = self.number_of_allocation_time_periods           # Steps per Episodes. (same as above)\n",
        "        self.cap_dist = cap_dist                            # Capacities of the Distribution Centers or local response center (Simulated by an RL agent). The size of this array is equal to \"self.no_materials\" times \"self.no_dcl\".\n",
        "        self.cap_consum_mat = cap_consum_mat                   # capacity consumption by a particular relief item in a particular local response center\n",
        "        self.Len = Len       # length (in terms of hours) of one time period\n",
        "        self.planning_horizon = self.Len * self.number_of_allocation_time_periods\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.erect_cost = np.array([0])                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "          self.removal_cost = np.array([0])                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "          # a way to simplify the model and avoid unecessary complications (non-convexities)\n",
        "          self.fixed_cost = (self.erect_cost + self.removal_cost) / 2        # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "\n",
        "        self.episode_counter = 0\n",
        "        self.aaa = aaa                       # The shape is of the form self.no_aa by self.no_materials.                        # In the numpy array the cofficient values for different materials for the same affected area are grouped together. The size of this array is equal to \"self.no_materials\" times \"self.no_dcl_loc_aval\".                                       # A constant used in calculating reward for the Agent (Depreivation cost and Terminal Penality cost).\n",
        "        self.bbb = bbb                       # The shape is of the form self.no_aa by self.no_materials.                        # In the numpy array the cofficient values for different materials for the same affected area are grouped together. The size of this array is equal to \"self.no_materials\" times \"self.no_dcl_loc_aval\".                                      # A constant used in calculating reward for the Agent (Depreivation cost and Terminal Penality cost).\n",
        "        self.cost_path = cost_path\n",
        "\n",
        "        self.demand_aa_mat = demand_aa_mat                 # In each time period, there is demand associated with each affected area and material. For each time period, the demand is arranged by affected areas and material.\n",
        "\n",
        "        # all or nothing disruption probabilities for potential LRC location\n",
        "        if self.is_loc_dec:\n",
        "          self.disrup_prob = np.array([[0, 1]])         # this must be two dimensional array with size equal to number of local response centers time \"2\" (\"2\" because there can be only two possible scenarios for an LRC, either it is disrupted or it is not disrupted)\n",
        "          self.loc_disrup_temp = np.zeros((self.no_dcl_loc_aval,))          # this array will be used in disruption calculations\n",
        "\n",
        "        # enumeration of location decisions for all local response centers or distribution centers\n",
        "        if self.is_loc_dec:\n",
        "          self.loc_dcl = []\n",
        "          a = 0\n",
        "          while a < self.no_dcl_loc_aval:\n",
        "            self.loc_dcl.append([0, 1])\n",
        "            a = a + 1\n",
        "          self.loc_dcl = np.array(self.loc_dcl)\n",
        "          self.loc_dcl_actions = np.array(np.meshgrid(*list(self.loc_dcl))).T.reshape(-1, self.no_dcl_loc_aval)      # len([0, 1]) = 2                     # All the possible actions related to loction are listed in \"self.loc_dcl_actions\".\n",
        "          del self.loc_dcl                                     # Deleting the 'self.loc_dcl' array as it is no longer required, to free up memory. (Make the code memory efficient)\n",
        "          del a\n",
        "\n",
        "          self.dc_loc_alr_const = np.array([1])                             # The length of the array is equal to the number of distrubution centers, \"self.no_dcl_loc_aval\".                   # The array representing the agent's decision for establishing distribution center at a particular avalilable location (to be taken from agent's action array).\n",
        "\n",
        "        # enumeration of resource allocation decision from relief material allocation from local response centers to affected areas\n",
        "        self.counter = 0                                                                  # Its value will increase until all the combinations of the affected areas and distrubuiton centers are exhausted.\n",
        "\n",
        "        self.res_alocat_dc = []\n",
        "        m = 0\n",
        "        while m < self.no_aa:                                             # These nested while loops have the task to generate the numpy array list of all possible allocation aciton, which the agent can do. In it, there may be some actions which are infeasible.\n",
        "          i = 0\n",
        "          while i < self.no_materials:\n",
        "            j = 0\n",
        "            while j < self.no_dcl_loc_aval:\n",
        "              k = 0\n",
        "              self.res_alocat_dc.append([])\n",
        "              while k <= np.floor(self.cap_dist[j] / self.cap_consum_mat[i, j]):\n",
        "                self.res_alocat_dc[self.counter].append(k)\n",
        "                k = k + 1\n",
        "              self.counter = self.counter + 1\n",
        "              j = j + 1\n",
        "            i = i + 1\n",
        "          m = m + 1\n",
        "        self.res_alocat_dc = np.array(self.res_alocat_dc)\n",
        "        self.alot_dcl_actions = np.array(np.meshgrid(*self.res_alocat_dc)).T.reshape(-1, self.no_dcl_loc_aval * self.no_aa * self.no_materials)                           # This statement creates the real numpy array list of all the possible resource allocation decisions. \"self.alot_dcl_actions\" contains all the possible actions related to resource allocation. The arrangement in \"self.alot_dcl_actions\" is self.no_materials by self.no_dcl_loc_aval.\n",
        "\n",
        "        # determination of action validaty (are all constraints satisfied or not?) and the elimination of invalid actions\n",
        "        self.del_el = []                  # array elements to be deleated\n",
        "        # if the allocation capacity of any distribution center location is exceeded by the total allocation made from that distribution ceneter location then the action is eliminated in the following code.\n",
        "        for j in range(0, self.alot_dcl_actions.shape[0]):          # Action shaping implementation. (eliminating some obvious infeasible actions) Ref: A. Kanervisto, C. Scheller and V. Hautamäki, \"Action Space Shaping in Deep Reinforcement Learning,\" 2020 IEEE Conference on Games (CoG), 2020, pp. 479-486, doi: 10.1109/CoG47356.2020.9231687.\n",
        "          self.compare_for_action_shaping = np.sum(np.multiply(np.reshape(np.sum(np.reshape(self.alot_dcl_actions[j], (self.no_aa, self.no_materials * self.no_dcl_loc_aval)), axis = 0), (self.no_materials, self.no_dcl_loc_aval)), self.cap_consum_mat), axis = 0)               # As capacity constraint is the hard constraint so we use action shaping to eliminate the actions which violate this constraint to ensure that the capacity constraints must be satisfied, otherwise it makes no sense that the allocation exceeds the capacity of distribution centers.\n",
        "          for k in range(0, self.compare_for_action_shaping.shape[0]):\n",
        "            if self.compare_for_action_shaping[k] > self.cap_dist[k]:\n",
        "              self.del_el.append(j)\n",
        "              break\n",
        "\n",
        "        self.alot_dcl_actions = np.delete(self.alot_dcl_actions, self.del_el, axis = 0)\n",
        "\n",
        "        del self.counter\n",
        "        del m\n",
        "        del i\n",
        "        del j\n",
        "        del k\n",
        "        del self.res_alocat_dc                                                          # Deleting the variables array as it is no longer required, to free up memory. (Make the code memory efficient)\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          i = 0\n",
        "          while i < 2 ** self.no_dcl_loc_aval:                                          # Number of possible location allocation decisions are equal to 2 raise to power the total number of distrubution centers.\n",
        "            self.temp_arr_loc = np.repeat(self.loc_dcl_actions[i].reshape(1, -1), self.alot_dcl_actions.shape[0], axis = 0)\n",
        "            self.alot_loc_dcl_actions_full = np.concatenate((self.temp_arr_loc, np.copy(self.alot_dcl_actions)), axis = 1)\n",
        "            if i == 0:\n",
        "              self.alot_loc_dcl_actions_full_array = self.alot_loc_dcl_actions_full\n",
        "            else:\n",
        "              self.alot_loc_dcl_actions_full_array = np.concatenate((self.alot_loc_dcl_actions_full_array, self.alot_loc_dcl_actions_full), axis = 0)                              # \"self.alot_loc_dcl_actions_full_array\" numpy array contains all the possible action (location and resoruce allocation actions combined)\n",
        "            i = i + 1\n",
        "\n",
        "          del self.alot_loc_dcl_actions_full\n",
        "\n",
        "          # filteration based on location allocation and reallocation deicsions\n",
        "          self.del_el_loc_constr = []                  # array elements to be deleated\n",
        "          i = 0\n",
        "          # if any allocation is made through the distribution center location such that this distribution center location is not used in a particular action then these actions are eliminated (deleted).\n",
        "          while i < self.alot_loc_dcl_actions_full_array.shape[0]:                                              # Action shaping implementation. (eliminating some obvious infeasible actions) Ref: A. Kanervisto, C. Scheller and V. Hautamäki, \"Action Space Shaping in Deep Reinforcement Learning,\" 2020 IEEE Conference on Games (CoG), 2020, pp. 479-486, doi: 10.1109/CoG47356.2020.9231687.\n",
        "            self.alot_loc_dcl_actions_full_array_loc = self.alot_loc_dcl_actions_full_array[i, 0 : self.no_dcl_loc_aval]\n",
        "            self.alot_loc_dcl_actions_full_array_res = self.alot_loc_dcl_actions_full_array[i, self.no_dcl_loc_aval:].reshape(-1, self.no_dcl_loc_aval)\n",
        "            self.alot_loc_dcl_actions_full_array_res = np.sum(self.alot_loc_dcl_actions_full_array_res, axis = 0)\n",
        "            for j in range(0, self.no_dcl_loc_aval):\n",
        "              if self.alot_loc_dcl_actions_full_array_loc[j] == 0 and self.alot_loc_dcl_actions_full_array_res[j] != 0:\n",
        "                self.del_el_loc_constr.append(i)\n",
        "                break\n",
        "            i = i + 1\n",
        "\n",
        "          self.alot_loc_dcl_actions_full_array = np.delete(self.alot_loc_dcl_actions_full_array, self.del_el_loc_constr, axis = 0)\n",
        "        else:\n",
        "         self.alot_loc_dcl_actions_full_array = self.alot_dcl_actions   # if location allocation and relocation decisions are not included, then all the possible actions are only valid resource allocation actions\n",
        "        # enumeration of environment's state\n",
        "\n",
        "        # the following is an array which will store all the possible states in the environment\n",
        "        if self.is_loc_dec:\n",
        "          # upper bound on the number of possible state\n",
        "          self.upper_bound_state_no = np.prod(self.number_of_allocation_time_periods * np.zeros_like(self.demand_aa_mat[0]) + 1) * (2 ** self.no_dcl_loc_aval) * self.number_of_allocation_time_periods\n",
        "          self.state_storg_arr = np.zeros((self.upper_bound_state_no , self.no_aa * self.no_materials + self.no_dcl_loc_aval + 1))\n",
        "        else:\n",
        "          # upper bound on the number of possible state\n",
        "          self.upper_bound_state_no = np.prod(self.number_of_allocation_time_periods * np.zeros_like(self.demand_aa_mat[0]) + 1 + np.abs(self.number_of_allocation_time_periods * np.zeros_like(self.demand_aa_mat[0]) - np.floor((np.max(self.cap_dist) / np.min(self.cap_consum_mat))) * self.number_of_allocation_time_periods)) * self.number_of_allocation_time_periods\n",
        "          self.state_storg_arr = np.zeros((int(self.upper_bound_state_no) , self.no_aa * self.no_materials + 1))\n",
        "\n",
        "        self.state_lst_storg = [[] for i in range(0, self.number_of_allocation_time_periods + 1)]     # this list of list will store only in the indices of the state arrays in \"self.state_storg_arr\" with respect to each time period\n",
        "        self.init_state = np.array([0 for i in range(0, self.no_aa * self.no_materials)])\n",
        "\n",
        "        if self.is_use_dyn_prog:\n",
        "          self.prob_vec = np.zeros((self.upper_bound_state_no, self.alot_loc_dcl_actions_full_array.shape[0], self.upper_bound_state_no))\n",
        "\n",
        "        counter_1 = 0       # this counter tracks the index in \"self.state_storg_arr\" where the states from the previous time period (which will be used along with actions to enumerate states for the current time period in our finite horizon MDP) starts\n",
        "        counter_2 = 0       # this counter tracks the index in \"self.state_storg_arr\"  where state from the previous the previous time period ends\n",
        "        counter_3 = 1       # this counter starts at the index where the next state (of the current time period) is to entered in \"self.state_storg_arr\"\n",
        "        for i in range(0, self.number_of_allocation_time_periods):           # loop over all the time periods\n",
        "          for j in range(counter_1, counter_2 + 1):      # loop over all the states of the previous time period (i.e., between the index \"counter_1\" and \"counter_2\")\n",
        "            for k in range(0, self.alot_loc_dcl_actions_full_array.shape[0]):       # loop over all the actions which the agent can take\n",
        "              if self.is_loc_dec:\n",
        "                # computation of location allocation decision\n",
        "                if i % self.reallocate_t == 0:\n",
        "                  # the following code will be executed if the feedback policy is being evaluated\n",
        "                  self.loc_state = self.alot_loc_dcl_actions_full_array[k, :self.no_dcl_loc_aval]\n",
        "                else:\n",
        "                  self.loc_state = self.state_storg_arr[j, self.no_aa * self.no_materials : self.no_aa * self.no_materials + self.no_dcl_loc_aval]\n",
        "\n",
        "                # if disruption is not considered the state enumeration is much simpler\n",
        "                if np.sum(self.disrup_prob[:, 0].reshape(-1,), axis = 0) != 0:\n",
        "                  # modification of local response centers' sub state to take into account disruption risk\n",
        "                  if np.sum(self.loc_state) != 0:\n",
        "                    next_state_store = np.array(np.meshgrid(*[[0, 1] for i in range(0, int(np.sum(self.loc_state)))])).T.reshape(-1, int(np.sum(self.loc_state)))\n",
        "                    indx_non_zero_next_st = np.nonzero(self.loc_state)[0]\n",
        "                    for indx_l, l in enumerate(next_state_store):\n",
        "                      prob_temp = 1\n",
        "                      loc_sub_state_st = copy.deepcopy(self.loc_state)\n",
        "                      for m in range(0, len(indx_non_zero_next_st)):\n",
        "                        # print(self.loc_state, \"lllllllll\")\n",
        "                        # print(indx_non_zero_next_st, \"yyyyyyyyyyyy\")\n",
        "                        loc_sub_state_st[int(indx_non_zero_next_st[m])] = l[m]\n",
        "                        prob_temp = prob_temp * self.disrup_prob[int(indx_non_zero_next_st[m]), l[m]]\n",
        "                        # print(self.demand_aa_mat[i], \"333333333333\")\n",
        "                        state_temp_temp = np.concatenate((self.state_storg_arr[j, 0 : self.no_aa * self.no_materials] - np.sum(np.reshape(self.alot_loc_dcl_actions_full_array[k, self.no_dcl_loc_aval:], (self.no_aa * self.no_materials, self.no_dcl_loc_aval)), axis = 1) + self.demand_aa_mat[i], loc_sub_state_st, np.array([i + 1])), axis = 0)\n",
        "                        # print(state_temp_temp, \"state_temp_tempstate_temp_tempstate_temp_temp\")\n",
        "                        # print(self.state_storg_arr[counter_2 : counter_3,:], \"3333333333333333333\")\n",
        "                        if np.sum(np.prod(self.state_storg_arr[counter_2 : counter_3,:] == state_temp_temp, axis = 1)) == 0:\n",
        "                          self.state_storg_arr[counter_3] = state_temp_temp\n",
        "                          self.state_lst_storg[i].append(counter_3)\n",
        "                          if self.is_use_dyn_prog:\n",
        "                            self.prob_vec[j, k, counter_3] = prob_temp     # forming of the state trasition probability matrix (3D array with size as number of states times number of actions times number of states)\n",
        "                            print(self.prob_vec[j, k, counter_3], \"ppppppppppppp\")\n",
        "                            print(j, k, counter_3)\n",
        "                          # print(prob_temp, \"prob_tempprob_tempprob_temp\")\n",
        "                          counter_3 = counter_3 + 1\n",
        "                  else:\n",
        "                    state_temp_temp = np.concatenate((self.state_storg_arr[j, 0 : self.no_aa * self.no_materials] - np.sum(np.reshape(self.alot_loc_dcl_actions_full_array[k, self.no_dcl_loc_aval:], (self.no_aa * self.no_materials, self.no_dcl_loc_aval)), axis = 1) + self.demand_aa_mat[i], self.loc_state, np.array([i + 1])), axis = 0)\n",
        "                    # print(state_temp_temp, \"state_temp_tempstate_temp_tempstate_temp_temp\")\n",
        "                    # print(self.state_storg_arr[counter_2 : counter_3,:], \"3333333333333333333\")\n",
        "                    if np.sum(np.prod(self.state_storg_arr[counter_2 : counter_3,:] == state_temp_temp, axis = 1)) == 0:\n",
        "                      self.state_storg_arr[counter_3] = state_temp_temp\n",
        "                      self.state_lst_storg[i].append(counter_3)\n",
        "                      if self.is_use_dyn_prog:\n",
        "                        self.prob_vec[j, k, counter_3] = 1\n",
        "\n",
        "                      counter_3 = counter_3 + 1\n",
        "                else:\n",
        "                  state_temp_temp = np.concatenate((self.state_storg_arr[j, 0 : self.no_aa * self.no_materials] - np.sum(np.reshape(self.alot_loc_dcl_actions_full_array[k, self.no_dcl_loc_aval:], (self.no_aa * self.no_materials, self.no_dcl_loc_aval)), axis = 1) + self.demand_aa_mat[i], self.loc_state, np.array([i + 1])), axis = 0)\n",
        "                  # print(state_temp_temp, \"state_temp_tempstate_temp_tempstate_temp_temp\")\n",
        "                  # print(self.state_storg_arr[counter_2 : counter_3,:], \"3333333333333333333\")\n",
        "                  if np.sum(np.prod(self.state_storg_arr[counter_2 : counter_3,:] == state_temp_temp, axis = 1)) == 0:\n",
        "                    self.state_storg_arr[counter_3] = state_temp_temp\n",
        "                    self.state_lst_storg[i].append(counter_3)\n",
        "                    if self.is_use_dyn_prog:\n",
        "                      self.prob_vec[j, k, counter_3] = 1\n",
        "\n",
        "                    counter_3 = counter_3 + 1\n",
        "\n",
        "              else:\n",
        "                state_temp_temp = np.concatenate((self.state_storg_arr[j, 0 : self.no_aa * self.no_materials] - np.sum(np.reshape(self.alot_loc_dcl_actions_full_array[k], (self.no_aa * self.no_materials, self.no_dcl_loc_aval)), axis = 1) + self.demand_aa_mat[i], np.array([i + 1])), axis = 0)\n",
        "                if np.sum(np.prod(self.state_storg_arr[counter_2 : counter_3,:] == state_temp_temp, axis = 1)) == 0:\n",
        "                  self.state_storg_arr[counter_3] = state_temp_temp\n",
        "                  self.state_lst_storg[i].append(counter_3)\n",
        "                  if self.is_use_dyn_prog:\n",
        "                    self.prob_vec[j, k, counter_3] = 1\n",
        "\n",
        "                  counter_3 = counter_3 + 1\n",
        "                  # self.state_storg_arr[: counter_3 - 1, :] = np.unique(self.state_storg_arr[: counter_3 - 1, :], axis = 0)\n",
        "                  # print(counter_3, \"counter_2counter_2counter_2\")\n",
        "          counter_1 = counter_2 + 1\n",
        "          counter_2 = counter_3 - 1\n",
        "\n",
        "        self.counter_1 = counter_1\n",
        "        self.state_storg_arr = self.state_storg_arr[: counter_3]      # removing all the rows in state storage array which are not used after all the possible states have been enumerated\n",
        "        # self.state_storg_arr = np.unique(self.state_storg_arr, axis = 0)    # removing all the rows which are not unique (untile they are unique)\n",
        "\n",
        "        self.Ns = self.state_storg_arr.shape[0]\n",
        "        self.Na = self.alot_loc_dcl_actions_full_array.shape[0]\n",
        "\n",
        "        if self.is_use_dyn_prog:\n",
        "          self.prob_vec = self.prob_vec[:self.counter_1, :, :self.Ns]\n",
        "\n",
        "  def step(self, act):                                                            # The agent just gives the index of the action which is taken by the RL agent, as the actions are discrete.\n",
        "\n",
        "        self.sstep = self.sstep + 1                                                   # The above statement is there as a counter. It countes the number of steps taken in an episode. (updating of the step counter.)\n",
        "\n",
        "        self.act_raw = self.alot_loc_dcl_actions_full_array[act]              # Taking the action vector from the agent and storing it in the \"step\" class attribute \"act\".                                                    # Step counter, which counts the number of steps in an episodes.\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.act_loc = self.act_raw[:self.no_dcl_loc_aval]              # location allocation decision for local response centers (LRCs)\n",
        "\n",
        "          self.act_partial_raw = self.act_raw[self.no_dcl_loc_aval:]           # This statement extracts only the resoruce allocation portion of the action taken by the agent.\n",
        "\n",
        "        else:\n",
        "          self.act_partial_raw = self.act_raw\n",
        "\n",
        "        # the following conditional state will be executed only when the agent cannot make decisions on locations of LRCs. Thus, we must make the resoruce allocation actions feasible with the already made location allocation decisons in the previous time period (modified by disruption)\n",
        "        # making resource allocation action feasible to what LRCs locations are currently operational (for location allocation decisions less time periods)\n",
        "        if self.is_loc_dec and ((self.sstep - 1) % self.reallocate_t != 0):\n",
        "          # modification of \"self.act_partial_raw\"\n",
        "          self.act_partial_raw = np.multiply(np.tile(self.loc_state_temp, self.no_aa * self.no_materials).reshape(self.no_aa * self.no_materials, self.no_dcl_loc_aval), np.reshape(self.act_raw[self.no_dcl_loc_aval:], (self.no_aa * self.no_materials, self.no_dcl_loc_aval))).reshape(-1,)\n",
        "\n",
        "        # the following code will be executed when the trained feedback policy is being evaluated\n",
        "        if self.is_eval:\n",
        "          print(\"resource_allocation_decisions: \", self.act_partial_raw)\n",
        "\n",
        "        self.act_partial_raw_reshape = np.reshape(self.act_partial_raw, (self.no_aa, self.no_materials * self.no_dcl_loc_aval))\n",
        "        self.act_partial_raw_reshape_sum = np.sum(self.act_partial_raw_reshape, axis = 0)\n",
        "\n",
        "        # transition function\n",
        "        self.state = self.state - np.sum(np.reshape(self.act_partial_raw, (self.no_aa * self.no_materials, self.no_dcl_loc_aval)), axis = 1) + self.demand_aa_mat[self.sstep - 1]                  # The computation of the next state vector.\n",
        "\n",
        "        # local response centers' location update (part of transition function)\n",
        "        if self.is_loc_dec:\n",
        "          if (self.sstep - 1) % self.reallocate_t == 0:\n",
        "\n",
        "            # the following code will be executed if the the feedback policy is being evaluated\n",
        "            if self.is_eval:\n",
        "              print(\"LRCs'_location_allocation_and_relocation_decisions: \", self.act_loc)\n",
        "\n",
        "            self.loc_state = self.act_loc\n",
        "            print(self.act_loc, \"llllllllllllll\")\n",
        "          else:\n",
        "            self.loc_state = self.loc_state_temp\n",
        "\n",
        "          # self.loc_state = np.array([1 for i in range(0, self.no_dcl_loc_aval)])       # this statement will run only when location allocation decisons are not to be included\n",
        "\n",
        "        i = 0\n",
        "        self.act = []\n",
        "        self.accumulate = 0\n",
        "\n",
        "        self.cap_dist_matrix = self.cap_dist.reshape(-1, self.no_dcl_loc_aval)\n",
        "\n",
        "        # fixied cost for establishment of local response centers (LRCs) (installation or deinstallation cost)\n",
        "        if self.is_loc_dec:\n",
        "          self.fixed_cost_of_dist_centers = np.dot(np.abs(self.loc_state - self.loc_state_temp), self.fixed_cost)\n",
        "          if self.sstep == self.steps_per_episode:              # at the end of planning horizon, all the established location response centers (LRC) (as they are the temprary strcutures)\n",
        "            self.fixed_cost_of_dist_centers = self.fixed_cost_of_dist_centers + np.dot(self.loc_state, self.fixed_cost)            # at the of planning horizon (episode) the removal cost for all established local response centers (LRCs) must be included in the incurred fixed cost\n",
        "\n",
        "          self.loc_state_temp = copy.deepcopy(self.loc_state)\n",
        "\n",
        "        # before the system can move two the next time step (and after the complete execution of current time period's decisions) LRCs are exposed to all or nothing disruption\n",
        "        if self.is_loc_dec:\n",
        "          for i in range(0, self.no_dcl_loc_aval):\n",
        "            self.loc_disrup_temp[i] = np.random.choice(np.array([0, 1]), size=(1,), replace=True, p=self.disrup_prob[i])\n",
        "          self.loc_state_temp = np.multiply(self.loc_state_temp, self.loc_disrup_temp).reshape(-1,)\n",
        "\n",
        "          # the following code will be executed if the the feedback policy is being evaluated\n",
        "          if self.is_eval:\n",
        "            print(\"which_potential_LRCs'_locations_have_disruption_occured_on_them_in_the_current_time_period_which_will_impact_subsequent_time_period?: \", self.loc_disrup_temp)\n",
        "\n",
        "        counter = 0\n",
        "\n",
        "        self.accessibility_based_delivery_cost = np.dot(self.act_partial_raw, self.cost_path[self.sstep - 1])               # \"Accesibility Based Delivery Cost\" is calculated by computing the dot product of allocation action vector, and the path cost vector.\n",
        "\n",
        "        self.dep_cost_sum = 0                                            # Before calculating the total deprivation cost, the class attribute \"deprivation cost\" is set to \"0\".\n",
        "        if self.sstep != self.steps_per_episode:\n",
        "          for s in self.state:                                               # This for loop loops through all the affected areas and calculate the total deprivation cost.\n",
        "            if s >= 0:                                                                   # It the state value of the affected area is greater then or equal to \"0\" then the formula below is used.\n",
        "              self.dep = exp(self.aaa[counter]) * (exp(self.bbb[counter] * self.Len) - 1) * (exp(self.bbb[counter] * self.Len) ** s)                        # This formula is used to calculate the deprivation cost for a particular Affected Area, given a particular state value of that Affected Area.\n",
        "            else:\n",
        "              self.dep = 0                                                                 # If the state value of the Affected Area is less then \"0\" then the deprivation cost of that affected area will be set to \"0\".\n",
        "            if self.sstep == 1:                                                              # If it is the first time step of an episode then there is an additional step is calculating the deprivation cost for the time period.\n",
        "              self.dep = self.dep + np.multiply(np.exp(self.aaa[counter]), (np.exp(self.bbb[counter] * self.Len) - 1))               # This is the additional step is calculating he deprivation cost for the first time period of the episode.\n",
        "\n",
        "            counter = counter + 1\n",
        "\n",
        "            self.dep_cost_sum = self.dep_cost_sum + self.dep\n",
        "\n",
        "\n",
        "        else:                                     # If the time step of and episode is the last time step then there is an additional cost called the \"penelty cost\".\n",
        "          self.terminal_panelty_cost_sum = 0                                                # Before calculating the terminal panelty cost, the class attribute \"terminal_penelty_cost\" is set to zero.\n",
        "          counter = 0\n",
        "          for s in self.state:                                                         # This for loop loop through all the Affected Areas to calculate the terminal penelty cost of each Affected Area and total terminal penelty cost.\n",
        "            if s >= 0:                                                                   # If the state value of the Affected Area is greater then or equal to \"0\" then the formula is the next statement is used to calcualte the terminal penelty cost.\n",
        "              self.term = exp(self.aaa[counter]) * (exp(self.bbb[counter] * self.Len)-1) * (exp(self.bbb[counter] * self.Len) ** s)              # The formula used for calculating the terminal penelty cost for a particular Affected Area.\n",
        "            else:\n",
        "              self.term = 0                                                              # If state value of the affected area is less then \"0\" then the terminal panelty cost is set to zero.\n",
        "\n",
        "            counter = counter + 1\n",
        "\n",
        "            self.terminal_panelty_cost_sum = self.terminal_panelty_cost_sum + self.term\n",
        "\n",
        "        # The following if-elif-else computes the reward which will be delivered to the agent.\n",
        "        if self.is_loc_dec:\n",
        "          if self.sstep != self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.dep_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost - self.accessibality_based_delivery_cost_weight * self.fixed_cost_of_dist_centers                 # If the Markovian time step is the first time step, then the reward is calculated by summing the negative of total deprivations cost and total accessibility based delivery cost.\n",
        "          elif self.sstep == self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.terminal_panelty_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost - self.accessibality_based_delivery_cost_weight * self.fixed_cost_of_dist_centers             # If the Markovian time step is the last time step then the reward for the RL agent is calculated by summing the negative of accessibility based delivery cost and terminal penality cost.\n",
        "\n",
        "          del self.fixed_cost_of_dist_centers\n",
        "\n",
        "        else:\n",
        "          if self.sstep != self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.dep_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost                 # If the Markovian time step is the first time step, then the reward is calculated by summing the negative of total deprivations cost and total accessibility based delivery cost.\n",
        "          elif self.sstep == self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.terminal_panelty_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost             # If the Markovian time step is the last time step then the reward for the RL agent is calculated by summing the negative of accessibility based delivery cost and terminal penality cost.\n",
        "\n",
        "        del self.accessibility_based_delivery_cost\n",
        "\n",
        "        if self.sstep == self.steps_per_episode:\n",
        "          self.done = True                                                                 # If the time period is the last time period of the episode then set the class attribute \"done\" to True.\n",
        "        else:\n",
        "          self.done = False                                                                # If the time period is not the last time period of the episode then set the class attribute \"done\" to False.\n",
        "        self.info = {}                                                                     # Class attribute \"info\" is an empty python dictionary which is returned to the RL agent at every time step.\n",
        "\n",
        "        # the following code will be executed if the the feedback policy is being evaluated\n",
        "        if self.is_eval:\n",
        "          print(\"reward_or_stagewise_objective_function_value: \", self.reward)\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.state_indx_curr = np.where(np.all(self.state_storg_arr == np.concatenate((self.state.reshape(self.no_aa * self.no_materials,), self.loc_state_temp, np.array([self.sstep])), axis = 0), axis = 1))[0][0]\n",
        "          return self.state_indx_curr, float(self.reward), self.done, self.info           # The step method will return the state of the time period, the reward, \"done\" attribute (which shows weather the episode has terminated or not) and the empty python dictionary \"info\".\n",
        "        else:\n",
        "          self.state_indx_curr = np.where(np.all(self.state_storg_arr == np.concatenate((self.state.reshape(self.no_aa * self.no_materials,), np.array([self.sstep])), axis = 0), axis = 1))[0][0]\n",
        "          return self.state_indx_curr, float(self.reward), self.done, self.info           # The step method will return the state of the time period, the reward, \"done\" attribute (which shows weather the episode has terminated or not) and the empty python dictionary \"info\".\n",
        "\n",
        "\n",
        "  def render(self):                                                               # The \"render\" method is used to render the environment or to visually simulate the RL environment. It is computationally costly to render the environment and also it is not necessary to render this environment. This method is mostly used for debug the environment.\n",
        "        pass                                                                      # Here, \"pass\" keyword is used to just pass the method, this method () is not being used (Environment is not rendered).\n",
        "\n",
        "  def reset(self):                                                                # The \"reset\" method, which is called when a new episode is started. It gives the initial state to the RL agent.\n",
        "\n",
        "        self.sstep = 0                                                            # The step counter initialized at zero.\n",
        "        self.state = np.array([0 for i in range(0, self.no_aa * self.no_materials)])                                               # Initial state, which is zero for every affected area and material, Arrangement is affected areas and then number of materials.\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.loc_state_temp = np.array([0 for i in range(0, self.no_dcl_loc_aval)])        # assuming no local response centers are pre-eracted\n",
        "          self.state_indx_curr = np.where(np.all(self.state_storg_arr == np.concatenate((self.state, self.loc_state_temp, np.array([self.sstep])), axis = 0), axis=1))[0][0]\n",
        "          return self.state_indx_curr\n",
        "        else:\n",
        "          self.state_indx_curr = np.where(np.all(self.state_storg_arr == np.concatenate((self.state, np.array([self.sstep])), axis = 0), axis=1))[0][0]\n",
        "          return self.state_indx_curr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhnKb-Onalbm"
      },
      "source": [
        "# **Dynamic Programming (exploiting the fact that our MDP is of finite horizon)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mkMT6c2VIPx3"
      },
      "outputs": [],
      "source": [
        "# creating te excel sheet for insertion of data using \"openpyxl\"\n",
        "\n",
        "wb = openpyxl.Workbook()     # creating a new workbook\n",
        "ws = wb.active   # activating the workbook for reading and writing of data into and from the workbook respectively\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ng-Jok0c5Ny"
      },
      "outputs": [],
      "source": [
        "is_loc_dec = False         # we are not using location allocation decisions in this research\n",
        "\n",
        "gamma = 0.99999           # discount factor\n",
        "\n",
        "no_dcl_loc_aval = 1        # number of local response centers (we are considering just one)\n",
        "cap_consum_mat = np.array([1]).reshape(no_materials, no_dcl_loc_aval)                   # capacity consumption by a particular relief item in a particular local response center\n",
        "Len = 4       # length (in terms of hours) of one time period\n",
        "planning_horizon = Len * number_of_allocation_time_periods\n",
        "\n",
        "episode_counter = 0\n",
        "\n",
        "\n",
        "# enumeration of resource allocation decision from relief material allocation from local response centers to affected areas\n",
        "counter = 0                                                                  # Its value will increase until all the combinations of the affected areas and distrubuiton centers are exhausted.\n",
        "\n",
        "res_alocat_dc = []\n",
        "m = 0\n",
        "while m < no_aa:                                             # These nested while loops have the task to generate the numpy array list of all possible allocation aciton, which the agent can do. In it, there may be some actions which are infeasible.\n",
        "  i = 0\n",
        "  while i < no_materials:\n",
        "    j = 0\n",
        "    while j < no_dcl_loc_aval:\n",
        "      k = 0\n",
        "      res_alocat_dc.append([])\n",
        "      while k <= np.floor(cap_dist[j] / cap_consum_mat[i, j]):\n",
        "        res_alocat_dc[counter].append(k)\n",
        "        k = k + 1\n",
        "      counter = counter + 1\n",
        "      j = j + 1\n",
        "    i = i + 1\n",
        "  m = m + 1\n",
        "res_alocat_dc = np.array(res_alocat_dc)\n",
        "alot_dcl_actions = np.array(np.meshgrid(*res_alocat_dc)).T.reshape(-1, no_dcl_loc_aval * no_aa * no_materials)                           # This statement creates the real numpy array list of all the possible resource allocation decisions. \"alot_dcl_actions\" contains all the possible actions related to resource allocation. The arrangement in \"alot_dcl_actions\" is no_materials by no_dcl_loc_aval.\n",
        "\n",
        "# determination of action validaty (are all constraints satisfied or not?) and the elimination of invalid actions\n",
        "del_el = []                  # array elements to be deleated\n",
        "# if the allocation capacity of any distribution center location is exceeded by the total allocation made from that distribution ceneter location then the action is eliminated in the following code.\n",
        "for j in range(0, alot_dcl_actions.shape[0]):          # Action shaping implementation. (eliminating some obvious infeasible actions) Ref: A. Kanervisto, C. Scheller and V. Hautamäki, \"Action Space Shaping in Deep Reinforcement Learning,\" 2020 IEEE Conference on Games (CoG), 2020, pp. 479-486, doi: 10.1109/CoG47356.2020.9231687.\n",
        "  compare_for_action_shaping = np.sum(np.multiply(np.reshape(np.sum(np.reshape(alot_dcl_actions[j], (no_aa, no_materials * no_dcl_loc_aval)), axis = 0), (no_materials, no_dcl_loc_aval)), cap_consum_mat), axis = 0)               # As capacity constraint is the hard constraint so we use action shaping to eliminate the actions which violate this constraint to ensure that the capacity constraints must be satisfied, otherwise it makes no sense that the allocation exceeds the capacity of distribution centers.\n",
        "  for k in range(0, compare_for_action_shaping.shape[0]):\n",
        "    if compare_for_action_shaping[k] > cap_dist[k]:\n",
        "      del_el.append(j)\n",
        "      break\n",
        "\n",
        "alot_dcl_actions = np.delete(alot_dcl_actions, del_el, axis = 0)\n",
        "\n",
        "del counter\n",
        "del m\n",
        "del i\n",
        "del j\n",
        "del k\n",
        "del res_alocat_dc                                                          # Deleting the variables array as it is no longer required, to free up memory. (Make the code memory efficient)\n",
        "\n",
        "alot_loc_dcl_actions_full_array = alot_dcl_actions   # if location allocation and relocation decisions are not included, then all the possible actions are only valid resource allocation actions\n",
        "\n",
        "# enumeration of environment's state\n",
        "\n",
        "# upper bound on the number of possible state\n",
        "upper_bound_state_no = np.prod(number_of_allocation_time_periods * np.zeros_like(demand_aa_mat[0]) + 1 + np.abs(number_of_allocation_time_periods * np.zeros_like(demand_aa_mat[0]) - np.floor((np.max(cap_dist) / np.min(cap_consum_mat))) * number_of_allocation_time_periods)) * number_of_allocation_time_periods\n",
        "state_storg_arr = np.zeros((int(upper_bound_state_no) , no_aa * no_materials + 1))\n",
        "\n",
        "state_lst_storg = [[] for i in range(0, number_of_allocation_time_periods + 1)]     # this list of list will store only in the indices of the state arrays in \"state_storg_arr\" with respect to each time period\n",
        "init_state = np.array([0 for i in range(0, no_aa * no_materials)])\n",
        "\n",
        "counter_1 = 0       # this counter tracks the index in \"state_storg_arr\" where the states from the previous time period (which will be used along with actions to enumerate states for the current time period in our finite horizon MDP) starts\n",
        "counter_2 = 0       # this counter tracks the index in \"state_storg_arr\"  where state from the previous the previous time period ends\n",
        "counter_3 = 1       # this counter starts at the index where the next state (of the current time period) is to entered in \"state_storg_arr\"\n",
        "for i in range(0, number_of_allocation_time_periods):           # loop over all the time periods\n",
        "  for j in range(counter_1, counter_2 + 1):      # loop over all the states of the previous time period (i.e., between the index \"counter_1\" and \"counter_2\")\n",
        "    for k in range(0, alot_loc_dcl_actions_full_array.shape[0]):       # loop over all the actions which the agent can take\n",
        "      state_temp_temp = np.concatenate((state_storg_arr[j, 0 : no_aa * no_materials] - np.sum(np.reshape(alot_loc_dcl_actions_full_array[k], (no_aa * no_materials, no_dcl_loc_aval)), axis = 1) + demand_aa_mat[i], np.array([i + 1])), axis = 0)\n",
        "      if np.sum(np.prod(state_storg_arr[counter_2 : counter_3,:] == state_temp_temp, axis = 1)) == 0:\n",
        "        state_storg_arr[counter_3] = state_temp_temp\n",
        "        state_lst_storg[i].append(counter_3)\n",
        "\n",
        "        counter_3 = counter_3 + 1\n",
        "        # state_storg_arr[: counter_3 - 1, :] = np.unique(state_storg_arr[: counter_3 - 1, :], axis = 0)\n",
        "        # print(counter_3, \"counter_2counter_2counter_2\")\n",
        "  counter_1 = counter_2 + 1\n",
        "  counter_2 = counter_3 - 1\n",
        "\n",
        "counter_1 = counter_1\n",
        "state_storg_arr = state_storg_arr[: counter_3]      # removing all the rows in state storage array which are not used after all the possible states have been enumerated\n",
        "# state_storg_arr = np.unique(state_storg_arr, axis = 0)    # removing all the rows which are not unique (untile they are unique)\n",
        "\n",
        "state_lst_storg.insert(0, [0])\n",
        "\n",
        "Ns = state_storg_arr.shape[0]\n",
        "Na = alot_loc_dcl_actions_full_array.shape[0]\n",
        "\n",
        "planning_horizon = Len * number_of_allocation_time_periods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l64WTXRhyM8"
      },
      "outputs": [],
      "source": [
        "# a separate reward function for the environment\n",
        "def reward_func(s, a, s_):\n",
        "  # accessibility based delivery cost\n",
        "  logistics_cost = np.dot(cost_path[int(state_storg_arr[s, -1]) - 1], alot_loc_dcl_actions_full_array[a])\n",
        "\n",
        "  # deprivation cost\n",
        "  deprivation_cost = 0\n",
        "  counter = 0\n",
        "  for st in state_storg_arr[s_, : no_aa * no_materials]:           # This for loop loops through all the affected areas and calculate the total deprivation cost.\n",
        "    if st >= 0:                                                                   # It the state value of the affected area is greater then or equal to \"0\" then the formula below is used.\n",
        "      deprivation_cost = deprivation_cost + exp(aaa[counter]) * (exp(bbb[counter] * Len) - 1) * (exp(bbb[counter] * Len) ** st)                        # This formula is used to calculate the deprivation cost for a particular Affected Area, given a particular state value of that Affected Area.\n",
        "    if state_storg_arr[s, -1] == 0:                                                              # If it is the first time step of an episode then there is an additional step is calculating the deprivation cost for the time period.\n",
        "      deprivation_cost = deprivation_cost + np.multiply(np.exp(aaa[counter]), (np.exp(bbb[counter] * Len) - 1))               # This is the additional step is calculating he deprivation cost for the first time period of the episode.\n",
        "    counter = counter + 1\n",
        "  return -((1 / 3) * deprivation_cost + (1 / 3) * logistics_cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NJU5dbtai6B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools as it\n",
        "\n",
        "st_time = time.time()\n",
        "\n",
        "# Calcuation of the value functions corresponding to each state, as well as storing action corresponding to the highest state value for each state at every time step.\n",
        "state_act_value_function = [np.zeros((len(state_lst_storg[i]), Na)) for i in range(0, number_of_allocation_time_periods + 1)]\n",
        "for i in range(number_of_allocation_time_periods - 1, -1, -1):     # looping over the episodes but from the end of the episodes to the start of the episode as for calculating state values in dynamic programming the state values are memoized to calculate the state values of the prvious time steps's state\n",
        "  # calculating the value function for all the states in all the steps in an episode\n",
        "  # storing the state value along with the optimal action from a particular state\n",
        "  for indx_j, j in enumerate(state_lst_storg[i]):               # loop over all the states in the current time period\n",
        "    for k in range(0, alot_loc_dcl_actions_full_array.shape[0]):    # loop over all the actions which can be taken in the current time period\n",
        "      value = 0\n",
        "      for indx_l, l in enumerate(state_lst_storg[i + 1]):\n",
        "        value = value + gamma * np.all(state_storg_arr[l, 0 : no_aa * no_materials] == (state_storg_arr[j, 0 : no_aa * no_materials] - alot_loc_dcl_actions_full_array[k] + demand_aa_mat[i])) * (reward_func(j, k, l) + np.max(state_act_value_function[i + 1][indx_l]))\n",
        "      state_act_value_function[i][indx_j, k] = value\n",
        "if i == 0:\n",
        "  print(value)\n",
        "\n",
        "for i in range(0, number_of_allocation_time_periods):\n",
        "  state_act_value_function[i] = np.argmax(state_act_value_function[i], axis = 1)\n",
        "temp_list = list(state_act_value_function[i] for i in range(0, number_of_allocation_time_periods))\n",
        "greedy_policy_dyn_prog = np.concatenate(temp_list, axis = 0)\n",
        "del state_act_value_function\n",
        "\n",
        "exce_time = time.time() - st_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYg9uVX6BJk6"
      },
      "outputs": [],
      "source": [
        "# number of time steps for which te algorithm is trained\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Execution time: \", exce_time))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1MmrhFBB4F0"
      },
      "source": [
        "## **Evaluation of Dynamic Programming Solution Policy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAyVjLBzDSqr"
      },
      "outputs": [],
      "source": [
        "env = Envir(accessibality_based_delivery_cost_weight = accessibality_based_delivery_cost_weight, deprivation_cost_weight = deprivation_cost_weight, gamma = gamma,\n",
        "            reallocate_t = 5, is_eval = False, is_loc_dec = False, is_use_dyn_prog = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVdXhm19B4F0"
      },
      "outputs": [],
      "source": [
        "n_eval_ep = 1\n",
        "\n",
        "obj_val_dyn_prog = 0\n",
        "\n",
        "# for storing action during evaluation simulation\n",
        "act_store = np.zeros((env.number_of_allocation_time_periods, env.no_aa * env.no_materials * env.no_dcl_loc_aval))\n",
        "\n",
        "# for storing states during evaluation simulation\n",
        "state_store = np.zeros((env.number_of_allocation_time_periods + 1, env.no_aa * env.no_materials))\n",
        "\n",
        "ws.append((\"Stagewise objective function value or reward:\",))\n",
        "\n",
        "for i in range(0, n_eval_ep):     # loop over the number of episodes\n",
        "  s = env.reset()     # initial state\n",
        "  state_store[0, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials]\n",
        "  for j in range(0, env.number_of_allocation_time_periods):     # loop over all the time peirods inside an episode\n",
        "    a = greedy_policy_dyn_prog[s]\n",
        "    act_store[j, :] = env.alot_loc_dcl_actions_full_array[a]\n",
        "    s, reward, done, info = env.step(a)\n",
        "\n",
        "    # storing all the stage wise objective values into an excel sheet\n",
        "    ws.append((f\"Stagewise objective function value at stage {j + 1}: \", reward))\n",
        "\n",
        "    obj_val_dyn_prog = obj_val_dyn_prog + (gamma ** i) * reward\n",
        "    state_store[j + 1, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV6Mgl5jB4F1"
      },
      "outputs": [],
      "source": [
        "act_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNjyYvn_B4F1"
      },
      "outputs": [],
      "source": [
        "# storing all the objective function valeu (sum of all the rewards) into an excel sheet\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Objective function value: \", obj_val_dyn_prog))\n",
        "obj_val_dyn_prog\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVjiIG27B4F1"
      },
      "outputs": [],
      "source": [
        "# storing all the state variables value to an excel sheet\n",
        "ws.append((\"State Variables\",))\n",
        "for i in state_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "# storing all the agent's actions into an excel sheet\n",
        "ws.append((\"Decision Variables\",))\n",
        "for i in act_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToSSo_l5B4F1"
      },
      "outputs": [],
      "source": [
        "# saving of the excel workbook at the end of the last experient of an instance, the workbook is saved with the abbreviaed notation of the problem instance as the name\n",
        "save_temp = sum(cap_dist) / max(cap_consum_mat)\n",
        "wb.save(f'/content/drive/MyDrive/Dynamic_Programming_({no_aa},_{list(save_temp)},_{number_of_allocation_time_periods})_tensorboard_convergence_curves.xlsx')      # this statement wil only be exceuted at the end of the last experieent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP-XzanuKxy0"
      },
      "source": [
        "# **Training of Value Iteration agent (by not exploiting the fact that our MDP is having finite horizon and therefore starting at an arbitrarily initialized Q-table while iteratively updating and improving it)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating te excel sheet for insertion of data using \"openpyxl\"\n",
        "\n",
        "wb = openpyxl.Workbook()     # creating a new workbook\n",
        "ws = wb.active   # activating the workbook for reading and writing of data into and from the workbook respectively\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ],
      "metadata": {
        "id": "QZ7HTguprptD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_loc_dec = False         # we are not using location allocation decisions in this research\n",
        "\n",
        "gamma = 0.99999           # discount factor\n",
        "\n",
        "no_dcl_loc_aval = 1        # number of local response centers (we are considering just one)\n",
        "cap_consum_mat = np.array([1]).reshape(no_materials, no_dcl_loc_aval)                   # capacity consumption by a particular relief item in a particular local response center\n",
        "Len = 4       # length (in terms of hours) of one time period\n",
        "planning_horizon = Len * number_of_allocation_time_periods\n",
        "\n",
        "episode_counter = 0\n",
        "\n",
        "# enumeration of resource allocation decision from relief material allocation from local response centers to affected areas\n",
        "counter = 0                                                                  # Its value will increase until all the combinations of the affected areas and distrubuiton centers are exhausted.\n",
        "\n",
        "res_alocat_dc = []\n",
        "m = 0\n",
        "while m < no_aa:                                             # These nested while loops have the task to generate the numpy array list of all possible allocation aciton, which the agent can do. In it, there may be some actions which are infeasible.\n",
        "  i = 0\n",
        "  while i < no_materials:\n",
        "    j = 0\n",
        "    while j < no_dcl_loc_aval:\n",
        "      k = 0\n",
        "      res_alocat_dc.append([])\n",
        "      while k <= np.floor(cap_dist[j] / cap_consum_mat[i, j]):\n",
        "        res_alocat_dc[counter].append(k)\n",
        "        k = k + 1\n",
        "      counter = counter + 1\n",
        "      j = j + 1\n",
        "    i = i + 1\n",
        "  m = m + 1\n",
        "res_alocat_dc = np.array(res_alocat_dc)\n",
        "alot_dcl_actions = np.array(np.meshgrid(*res_alocat_dc)).T.reshape(-1, no_dcl_loc_aval * no_aa * no_materials)                           # This statement creates the real numpy array list of all the possible resource allocation decisions. \"alot_dcl_actions\" contains all the possible actions related to resource allocation. The arrangement in \"alot_dcl_actions\" is no_materials by no_dcl_loc_aval.\n",
        "\n",
        "# determination of action validaty (are all constraints satisfied or not?) and the elimination of invalid actions\n",
        "del_el = []                  # array elements to be deleated\n",
        "# if the allocation capacity of any distribution center location is exceeded by the total allocation made from that distribution ceneter location then the action is eliminated in the following code.\n",
        "for j in range(0, alot_dcl_actions.shape[0]):          # Action shaping implementation. (eliminating some obvious infeasible actions) Ref: A. Kanervisto, C. Scheller and V. Hautamäki, \"Action Space Shaping in Deep Reinforcement Learning,\" 2020 IEEE Conference on Games (CoG), 2020, pp. 479-486, doi: 10.1109/CoG47356.2020.9231687.\n",
        "  compare_for_action_shaping = np.sum(np.multiply(np.reshape(np.sum(np.reshape(alot_dcl_actions[j], (no_aa, no_materials * no_dcl_loc_aval)), axis = 0), (no_materials, no_dcl_loc_aval)), cap_consum_mat), axis = 0)               # As capacity constraint is the hard constraint so we use action shaping to eliminate the actions which violate this constraint to ensure that the capacity constraints must be satisfied, otherwise it makes no sense that the allocation exceeds the capacity of distribution centers.\n",
        "  for k in range(0, compare_for_action_shaping.shape[0]):\n",
        "    if compare_for_action_shaping[k] > cap_dist[k]:\n",
        "      del_el.append(j)\n",
        "      break\n",
        "\n",
        "alot_dcl_actions = np.delete(alot_dcl_actions, del_el, axis = 0)\n",
        "\n",
        "del counter\n",
        "del m\n",
        "del i\n",
        "del j\n",
        "del k\n",
        "del res_alocat_dc                                                          # Deleting the variables array as it is no longer required, to free up memory. (Make the code memory efficient)\n",
        "\n",
        "alot_loc_dcl_actions_full_array = alot_dcl_actions   # if location allocation and relocation decisions are not included, then all the possible actions are only valid resource allocation actions\n",
        "\n",
        "# enumeration of environment's state\n",
        "\n",
        "# upper bound on the number of possible state\n",
        "upper_bound_state_no = np.prod(number_of_allocation_time_periods * np.zeros_like(demand_aa_mat[0]) + 1 + np.abs(number_of_allocation_time_periods * np.zeros_like(demand_aa_mat[0]) - np.floor((np.max(cap_dist) / np.min(cap_consum_mat))) * number_of_allocation_time_periods)) * number_of_allocation_time_periods\n",
        "state_storg_arr = np.zeros((int(upper_bound_state_no) , no_aa * no_materials + 1))\n",
        "\n",
        "state_lst_storg = [[] for i in range(0, number_of_allocation_time_periods + 1)]     # this list of list will store only in the indices of the state arrays in \"state_storg_arr\" with respect to each time period\n",
        "init_state = np.array([0 for i in range(0, no_aa * no_materials)])\n",
        "\n",
        "counter_1 = 0       # this counter tracks the index in \"state_storg_arr\" where the states from the previous time period (which will be used along with actions to enumerate states for the current time period in our finite horizon MDP) starts\n",
        "counter_2 = 0       # this counter tracks the index in \"state_storg_arr\"  where state from the previous the previous time period ends\n",
        "counter_3 = 1       # this counter starts at the index where the next state (of the current time period) is to entered in \"state_storg_arr\"\n",
        "for i in range(0, number_of_allocation_time_periods):           # loop over all the time periods\n",
        "  for j in range(counter_1, counter_2 + 1):      # loop over all the states of the previous time period (i.e., between the index \"counter_1\" and \"counter_2\")\n",
        "    for k in range(0, alot_loc_dcl_actions_full_array.shape[0]):       # loop over all the actions which the agent can take\n",
        "      state_temp_temp = np.concatenate((state_storg_arr[j, 0 : no_aa * no_materials] - np.sum(np.reshape(alot_loc_dcl_actions_full_array[k], (no_aa * no_materials, no_dcl_loc_aval)), axis = 1) + demand_aa_mat[i], np.array([i + 1])), axis = 0)\n",
        "      if np.sum(np.prod(state_storg_arr[counter_2 : counter_3,:] == state_temp_temp, axis = 1)) == 0:\n",
        "        state_storg_arr[counter_3] = state_temp_temp\n",
        "        state_lst_storg[i].append(counter_3)\n",
        "\n",
        "        counter_3 = counter_3 + 1\n",
        "        # state_storg_arr[: counter_3 - 1, :] = np.unique(state_storg_arr[: counter_3 - 1, :], axis = 0)\n",
        "        # print(counter_3, \"counter_2counter_2counter_2\")\n",
        "  counter_1 = counter_2 + 1\n",
        "  counter_2 = counter_3 - 1\n",
        "\n",
        "counter_1 = counter_1\n",
        "state_storg_arr = state_storg_arr[: counter_3]      # removing all the rows in state storage array which are not used after all the possible states have been enumerated\n",
        "# state_storg_arr = np.unique(state_storg_arr, axis = 0)    # removing all the rows which are not unique (untile they are unique)\n",
        "\n",
        "state_lst_storg.insert(0, [0])\n",
        "\n",
        "Ns = state_storg_arr.shape[0]\n",
        "Na = alot_loc_dcl_actions_full_array.shape[0]\n",
        "\n",
        "planning_horizon = Len * number_of_allocation_time_periods"
      ],
      "metadata": {
        "id": "alSzco3AruNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiOirHNAnT-o"
      },
      "outputs": [],
      "source": [
        "# a separate reward function for the environment\n",
        "def reward_func(s, a, s_):\n",
        "  # accessibility based delivery cost\n",
        "  logistics_cost = np.dot(cost_path[int(state_storg_arr[s, -1]) - 1], alot_loc_dcl_actions_full_array[a])\n",
        "\n",
        "  # deprivation cost\n",
        "  deprivation_cost = 0\n",
        "  counter = 0\n",
        "  for st in state_storg_arr[s_, : no_aa * no_materials]:           # This for loop loops through all the affected areas and calculate the total deprivation cost.\n",
        "    if st >= 0:                                                                   # It the state value of the affected area is greater then or equal to \"0\" then the formula below is used.\n",
        "      deprivation_cost = deprivation_cost + exp(aaa[counter]) * (exp(bbb[counter] * Len) - 1) * (exp(bbb[counter] * Len) ** st)                        # This formula is used to calculate the deprivation cost for a particular Affected Area, given a particular state value of that Affected Area.\n",
        "    if state_storg_arr[s, -1] == 0:                                                              # If it is the first time step of an episode then there is an additional step is calculating the deprivation cost for the time period.\n",
        "      deprivation_cost = deprivation_cost + np.multiply(np.exp(aaa[counter]), (np.exp(bbb[counter] * Len) - 1))               # This is the additional step is calculating he deprivation cost for the first time period of the episode.\n",
        "    counter = counter + 1\n",
        "  return -(deprivation_cost_weight * deprivation_cost + accessibality_based_delivery_cost_weight * logistics_cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj3TdzN7MlJt"
      },
      "outputs": [],
      "source": [
        "# this will be used in value iteration algorithm\n",
        "def bellman_operator(Q, greedy_policy):\n",
        "  for s in range(0, state_storg_arr.shape[0]):              # loop over all the environment's states\n",
        "    collection_next_state = np.array(state_lst_storg[int(state_storg_arr[s, -1] + 1)])\n",
        "    if state_storg_arr[s, -1] != number_of_allocation_time_periods:\n",
        "      for a in range(0, alot_loc_dcl_actions_full_array.shape[0]):              # loop over all the environment's actions\n",
        "        value = 0\n",
        "        for s_ in collection_next_state:\n",
        "          if state_storg_arr[s, -1] != number_of_allocation_time_periods - 1:\n",
        "            value = value + np.all(state_storg_arr[s_, 0 : no_aa * no_materials] == (state_storg_arr[s, 0 : no_aa * no_materials] - alot_loc_dcl_actions_full_array[a] + demand_aa_mat[int(state_storg_arr[s, -1])])) * (reward_func(s, a, s_) + gamma * np.max(Q[s_, :]))     # one step bootstrapping to approximate reward-to-go (in case of maximization) (cost-to-go in case of minimization, as is the case in our  code) function\n",
        "          else:           # if the time period is the second last one in our MDP then no bootstraping is done as last states have zero reward and reward-to-go function\n",
        "            value = value + np.all(state_storg_arr[s_, 0 : no_aa * no_materials] == (state_storg_arr[s, 0 : no_aa * no_materials] - alot_loc_dcl_actions_full_array[a] + demand_aa_mat[int(state_storg_arr[s, -1])])) * reward_func(s, a, s_)     # one step bootstrapping to approximate reward-to-go (in case of maximization) (cost-to-go in case of minimization, as is the case in our  code) function\n",
        "          Q[s,a]  = value\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  greedy_policy = np.argmax(Q, axis = 1)           # output action (which can be taken by the agent) based on greedy policy\n",
        "\n",
        "  return Q, greedy_policy          # updated q-table and greedy policy action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuiSg2oIQjSB"
      },
      "outputs": [],
      "source": [
        "def value_iteration(Q, greedy_policy):\n",
        "  global term_n_steps\n",
        "  counter = 0\n",
        "  while True:\n",
        "    Q, greedy_policy = bellman_operator(Q, greedy_policy)\n",
        "\n",
        "    counter = counter + 1\n",
        "\n",
        "    obj_val_val_iter = 0\n",
        "    s = env.reset()     # initial state\n",
        "    for j in range(0, number_of_allocation_time_periods):     # loop over all the time peirods inside an episode\n",
        "      a = greedy_policy[s]\n",
        "      s, reward, done, info = env.step(a)\n",
        "\n",
        "      obj_val_val_iter = obj_val_val_iter + (gamma ** i) * reward\n",
        "\n",
        "    # for te evaluatoin of the agent after predetermined number of steps\n",
        "    ws.append((time.time(), counter, obj_val_val_iter))\n",
        "\n",
        "    if counter > initial_steps_episod_stall:\n",
        "      np_arr_eval[int(counter)] = obj_val_val_iter\n",
        "\n",
        "      if (counter >= n_episod_stall):\n",
        "        y = np.sort(np.abs(np_arr_eval[int(counter - n_episod_stall) : int(counter)] - np_arr_eval[int(counter)]))[0 : n_episod_stall - 100]\n",
        "        if ((time.time() - st_time) >= 18000) or (np.max(y) <= delta_episod_stall):\n",
        "          term_n_steps = counter\n",
        "          return Q, greedy_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoOLmKHbQjSb"
      },
      "outputs": [],
      "source": [
        "st_time = time.time()\n",
        "\n",
        "env = Envir(accessibality_based_delivery_cost_weight = accessibality_based_delivery_cost_weight, deprivation_cost_weight = deprivation_cost_weight, gamma = gamma,\n",
        "            reallocate_t = 5, is_eval = False, is_loc_dec = False, is_use_dyn_prog = False)\n",
        "\n",
        "np_arr_eval = np.zeros((int(10 ** 5),))\n",
        "initial_steps_episod_stall = 1\n",
        "delta_episod_stall = 50\n",
        "n_episod_stall = 200\n",
        "counter = 0\n",
        "\n",
        "Q = np.zeros((Ns, Na))    # we are using \"counter_1\" instead of \"env.Ns\" because we cannot take any actions at the terminal states and thus their cost to go value must be zero (as there is no state after them)\n",
        "greedy_policy = np.zeros(Ns)\n",
        "\n",
        "ws.append((\"Data for convergence curves: \",))\n",
        "ws.append((\"Wall Time\", \"Iteration\", \"Value\"))   # for the creation of header in te excel sheet\n",
        "\n",
        "Q, greedy_policy_val_iter = value_iteration(Q, greedy_policy)\n",
        "\n",
        "exce_time = time.time() - st_time\n",
        "print(exce_time)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of time steps for which te algorithm is trained\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Training time: \", exce_time))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Number of iterations for training: \", term_n_steps))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "print(term_n_steps)"
      ],
      "metadata": {
        "id": "xRuj92dxmtr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKvP6ScLtS_X"
      },
      "source": [
        "## **Evaluation of Value Iteration Solution Policy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6Uh854XtS_Y"
      },
      "outputs": [],
      "source": [
        "n_eval_ep = 1\n",
        "\n",
        "obj_val_val_iter = 0\n",
        "\n",
        "# for storing action during evaluation simulation\n",
        "act_store = np.zeros((env.number_of_allocation_time_periods, env.no_aa * env.no_materials * env.no_dcl_loc_aval))\n",
        "\n",
        "# for storing states during evaluation simulation\n",
        "state_store = np.zeros((env.number_of_allocation_time_periods + 1, env.no_aa * env.no_materials))\n",
        "\n",
        "ws.append((\"Stagewise objective function value or reward:\",))\n",
        "\n",
        "for i in range(0, n_eval_ep):     # loop over the number of episodes\n",
        "  s = env.reset()     # initial state\n",
        "  state_store[0, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials]\n",
        "  for j in range(0, env.number_of_allocation_time_periods):     # loop over all the time peirods inside an episode\n",
        "    a = greedy_policy_val_iter[s]\n",
        "    act_store[j, :] = env.alot_loc_dcl_actions_full_array[a]\n",
        "    s, reward, done, info = env.step(a)\n",
        "\n",
        "    # storing all the stage wise objective values into an excel sheet\n",
        "    ws.append((f\"Stagewise objective function value at stage {j + 1}: \", reward))\n",
        "\n",
        "    obj_val_val_iter = obj_val_val_iter + (gamma ** i) * reward\n",
        "    state_store[j + 1, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI1vjRy7tS_Y"
      },
      "outputs": [],
      "source": [
        "act_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9WO3npFtS_Z"
      },
      "outputs": [],
      "source": [
        "# storing all the objective function valeu (sum of all the rewards) into an excel sheet\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Objective function value: \", obj_val_val_iter))\n",
        "obj_val_val_iter\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpwAGpmztS_Z"
      },
      "outputs": [],
      "source": [
        "# storing all the state variables value to an excel sheet\n",
        "ws.append((\"State Variables\",))\n",
        "for i in state_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "# storing all the agent's actions into an excel sheet\n",
        "ws.append((\"Decision Variables\",))\n",
        "for i in act_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obj_val_val_iter"
      ],
      "metadata": {
        "id": "qqTQGeUwtyVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeV2_fS_tS_Z"
      },
      "outputs": [],
      "source": [
        "# saving of the excel workbook at the end of the last experient of an instance, the workbook is saved with the abbreviaed notation of the problem instance as the name\n",
        "save_temp = sum(cap_dist) / max(cap_consum_mat)\n",
        "wb.save(f'/content/drive/MyDrive/Value_Iteration_({no_aa},_{list(save_temp)},_{number_of_allocation_time_periods})_tensorboard_convergence_curves.xlsx')      # this statement wil only be exceuted at the end of the last experieent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow0Jh2rfM-_2"
      },
      "source": [
        "# **Training of SARSA agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U0L61QaSEiW7"
      },
      "outputs": [],
      "source": [
        "# creating te excel sheet for insertion of data using \"openpyxl\"\n",
        "\n",
        "wb = openpyxl.Workbook()     # creating a new workbook\n",
        "ws = wb.active   # activating the workbook for reading and writing of data into and from the workbook respectively\n",
        "ws.append((\"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", f\"Random seed: {seed}\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "duv55SsBQjTJ"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "class Sarsa:\n",
        "    \"\"\"\n",
        "    Implements SARSA algorithm.\n",
        "\n",
        "    If learning_rate is None; alpha(x,a) = 1/max(1, N(s,a))**alpha\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, alpha=0.6, learning_rate=None, min_learning_rate=0.01, tau=1.0, tau_decay=0.9995,\n",
        "                 tau_min=0.25, seed=42):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_learning_rate = min_learning_rate\n",
        "        self.tau = tau\n",
        "        self.tau_decay = tau_decay\n",
        "        self.tau_min = tau_min\n",
        "        self.Q = np.zeros((env.Ns, env.Na))\n",
        "        self.Nsa = np.zeros((env.Ns, env.Na))\n",
        "        self.state = env.reset()\n",
        "        self.RS = np.random.RandomState(seed)\n",
        "\n",
        "    def get_delta(self, r, x, a, y, next_a, done):\n",
        "        \"\"\"\n",
        "        :param r: reward\n",
        "        :param x: current state\n",
        "        :param a: current action\n",
        "        :param y: next state\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        q_y_a = self.Q[y, next_a]\n",
        "        q_x_a = self.Q[x, a]\n",
        "\n",
        "        return r + self.gamma*q_y_a - q_x_a\n",
        "\n",
        "    def get_learning_rate(self, s, a):\n",
        "        if self.learning_rate is None:\n",
        "            return max(1.0/max(1.0, self.Nsa[s, a])**self.alpha, self.min_learning_rate)\n",
        "        else:\n",
        "            return max(self.learning_rate, self.min_learning_rate)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        q = self.Q[state, :]\n",
        "        prob = softmax(q/self.tau)\n",
        "        a = np.random.choice(np.arange(0, self.env.alot_loc_dcl_actions_full_array.shape[0]))\n",
        "        return a\n",
        "\n",
        "    def step(self):\n",
        "        # Current state\n",
        "        x = self.env.state_indx_curr\n",
        "\n",
        "        # Choose action\n",
        "        a = self.get_action(x)\n",
        "\n",
        "        # Learning rate\n",
        "        alpha = self.get_learning_rate(x, a)\n",
        "\n",
        "        # Take step\n",
        "        observation, reward, done, info = self.env.step(a)\n",
        "        y = observation\n",
        "        r = reward\n",
        "        next_a = self.get_action(y)\n",
        "        delta = self.get_delta(r, x, a, y, next_a, done)\n",
        "\n",
        "        # Update\n",
        "        self.Q[x, a] = self.Q[x, a] + alpha*delta\n",
        "\n",
        "        self.Nsa[x, a] += 1\n",
        "\n",
        "        if done:\n",
        "          # print(x, observation, reward)\n",
        "          self.tau = max(self.tau*self.tau_decay, self.tau_min)\n",
        "          self.env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MdBRPazcjBgu"
      },
      "outputs": [],
      "source": [
        "# function for the evaluation of the policy\n",
        "\n",
        "def evaluate_policy(gamma = 0.99999):\n",
        "  global envv\n",
        "  reward_sum = 0\n",
        "  for i in range(0, number_of_allocation_time_periods):\n",
        "    if i == 0:\n",
        "      state = envv.reset()\n",
        "    a = sarsa.Q[state, :].argmax()\n",
        "    state, reward, done, info = envv.step(a)\n",
        "    reward_sum = reward_sum + (gamma ** i) * reward\n",
        "  return reward_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1bZtS5lQjTR"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Convergence of SARSA\n",
        "# ---------------------------\n",
        "\n",
        "gamma = 0.99999\n",
        "np_arr_eval = np.zeros((int(10 ** 5),))\n",
        "save_freq = number_of_allocation_time_periods * 100\n",
        "initial_steps_episod_stall = 10 ** 6\n",
        "delta_episod_stall = 50\n",
        "n_episod_stall = 200\n",
        "count_eval = 0\n",
        "term_n_steps = 0\n",
        "st_time = time.time()\n",
        "\n",
        "# Iterate\n",
        "n_calls = 0\n",
        "count_eval = 0\n",
        "\n",
        "# crating environment object\n",
        "env = Envir(accessibality_based_delivery_cost_weight = 1 / 3, deprivation_cost_weight = 1 / 3, gamma = gamma,\n",
        "            reallocate_t = 5, is_eval = False, is_loc_dec = False, is_use_dyn_prog = False)\n",
        "\n",
        "# crating environment object\n",
        "envv = Envir(accessibality_based_delivery_cost_weight = 1 / 3, deprivation_cost_weight = 1 / 3, gamma = gamma,     # for evaluation only\n",
        "            reallocate_t = 5, is_eval = False, is_loc_dec = False, is_use_dyn_prog = False)\n",
        "\n",
        "# Get optimal value function and its greedy policy\n",
        "Q0 = np.zeros((env.counter_1, env.Na))\n",
        "\n",
        "# Create sarsa object\n",
        "sarsa = Sarsa(env, gamma=env.gamma)\n",
        "\n",
        "ws.append((\"Data for convergence curves: \",))\n",
        "ws.append((\"Wall Time\", \"Step\", \"Value\"))   # for the creation of header in te excel sheet\n",
        "\n",
        "# Q_est = np.zeros((n_steps, env.counter_1, env.Na))\n",
        "while True:\n",
        "\n",
        "    if (n_calls > initial_steps_episod_stall) and (n_calls % save_freq == 0):\n",
        "        np_arr_eval[int(count_eval)] = evaluate_policy()\n",
        "\n",
        "        if (count_eval >= n_episod_stall):\n",
        "          y = np.sort(np.abs(np_arr_eval[int(count_eval - n_episod_stall) : int(count_eval)] - np_arr_eval[int(count_eval)]))[0 : n_episod_stall - 100]\n",
        "          if ((time.time() - st_time) >= 18000) or (np.max(y) <= delta_episod_stall):\n",
        "            # print(((np.max(np.abs(np_arr_eval[int(count_eval - n_episod_stall) : int(count_eval)] - int(np_arr_eval[int(self.count_eval)]))) / int(np_arr_eval[int(self.count_eval)])) * 100), \"qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq\")\n",
        "            # print(int(np_arr_eval[int(count_eval)]), \"dddddddddddddddddddddddddddddddddddddddddd\")\n",
        "            # print(np.max(np.abs(np_arr_eval[int(count_eval - n_episod_stall) : int(count_eval)] - int(np_arr_eval[int(count_eval)]))), \"uuuuuuuuuuuuuuuuuuuuuu\")\n",
        "            term_n_steps = n_calls\n",
        "            exce_time = time.time() - st_time\n",
        "            print(exce_time)\n",
        "            break\n",
        "        count_eval = count_eval + 1\n",
        "\n",
        "    sarsa.step()\n",
        "\n",
        "    # Store estimate of Q*\n",
        "    # Q_est[tt, :, :] = sarsa.Q\n",
        "    n_calls +=1\n",
        "\n",
        "    # for te evaluatoin of the agent after predetermined number of steps\n",
        "    if n_calls % (1000 * number_of_allocation_time_periods) == 0:\n",
        "      ws.append((time.time(), n_calls, evaluate_policy()))\n",
        "\n",
        "# Compute greedy policy (with estimated Q)\n",
        "greedy_policy_sarsa = np.argmax(sarsa.Q, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGjNrHucnls4"
      },
      "outputs": [],
      "source": [
        "# number of time steps for which te algorithm is trained\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Training time: \", exce_time))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Number of steps for training: \", n_calls))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "print(n_calls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxeueiHp7zIX"
      },
      "source": [
        "## **Evaluation of SARSA agnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqv8X2DXjYjd"
      },
      "outputs": [],
      "source": [
        "n_eval_ep = 1\n",
        "\n",
        "obj_val_sarsa = 0\n",
        "\n",
        "# for storing action during evaluation simulation\n",
        "if env.is_loc_dec:\n",
        "  act_store = np.zeros((env.number_of_allocation_time_periods, env.no_aa * env.no_materials * env.no_dcl_loc_aval + env.no_dcl_loc_aval))\n",
        "else:\n",
        "  act_store = np.zeros((env.number_of_allocation_time_periods, env.no_aa * env.no_materials * env.no_dcl_loc_aval))\n",
        "\n",
        "# for storing states during evaluation simulation\n",
        "if env.is_loc_dec:\n",
        "  state_store = np.zeros((env.number_of_allocation_time_periods + 1, env.no_aa * env.no_materials + env.no_dcl_loc_aval))\n",
        "else:\n",
        "  state_store = np.zeros((env.number_of_allocation_time_periods + 1, env.no_aa * env.no_materials))\n",
        "\n",
        "ws.append((\"Stagewise objective function value or reward:\",))\n",
        "\n",
        "for i in range(0, n_eval_ep):     # loop over the number of episodes\n",
        "  s = env.reset()     # initial state\n",
        "  if env.is_loc_dec:\n",
        "    state_store[0, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials + env.no_dcl_loc_aval]\n",
        "  else:\n",
        "    state_store[0, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials]\n",
        "  for j in range(0, env.number_of_allocation_time_periods):     # loop over all the time peirods inside an episode\n",
        "    a = greedy_policy_sarsa[s]\n",
        "    act_store[j, :] = env.alot_loc_dcl_actions_full_array[a]\n",
        "    s, reward, done, info = env.step(a)\n",
        "\n",
        "    # storing all the stage wise objective values into an excel sheet\n",
        "    ws.append((f\"Stagewise objective function value at stage {j + 1}: \", reward))\n",
        "\n",
        "    obj_val_sarsa = obj_val_sarsa + (gamma ** i) * reward\n",
        "    if env.is_loc_dec:\n",
        "      state_store[j + 1, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials + env.no_dcl_loc_aval]\n",
        "    else:\n",
        "      state_store[j + 1, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyTb9gH3dSDv"
      },
      "outputs": [],
      "source": [
        "act_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJQ8785JdSEF"
      },
      "outputs": [],
      "source": [
        "# storing all the objective function valeu (sum of all the rewards) into an excel sheet\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Objective function value: \", obj_val_sarsa))\n",
        "obj_val_sarsa\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1GpWx0MdSEG"
      },
      "outputs": [],
      "source": [
        "# storing all the state variables value to an excel sheet\n",
        "ws.append((\"State Variables\",))\n",
        "for i in state_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "# storing all the agent's actions into an excel sheet\n",
        "ws.append((\"Decision Variables\",))\n",
        "for i in act_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-hT6XJsdSEG"
      },
      "outputs": [],
      "source": [
        "# saving of the excel workbook at the end of the last experient of an instance, the workbook is saved with the abbreviaed notation of the problem instance as the name\n",
        "save_temp = sum(cap_dist) / max(cap_consum_mat)\n",
        "wb.save(f'/content/drive/MyDrive/SARSA_agent_({no_aa},_{list(save_temp)},_{number_of_allocation_time_periods})_{seed}_tensorboard_convergence_curves.xlsx')      # this statement wil only be exceuted at the end of the last experieent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogIF8HFcEv9S"
      },
      "source": [
        "# **Training of Q-Learning agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zmauc_i1Eknh"
      },
      "outputs": [],
      "source": [
        "# creating te excel sheet for insertion of data using \"openpyxl\"\n",
        "\n",
        "wb = openpyxl.Workbook()     # creating a new workbook\n",
        "ws = wb.active   # activating the workbook for reading and writing of data into and from the workbook respectively\n",
        "ws.append((\"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", f\"Random seed: {seed}\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Gq_LD5WZQjSs"
      },
      "outputs": [],
      "source": [
        "#-------------------------------\n",
        "# Q-Learning implementation\n",
        "# ------------------------------\n",
        "\n",
        "class QLearning:\n",
        "    \"\"\"\n",
        "    Implements Q-learning algorithm with epsilon-greedy exploration\n",
        "\n",
        "    If learning_rate is None; alpha(x,a) = 1/max(1, N(s,a))**alpha\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, alpha=0.6, learning_rate=None, min_learning_rate=0.01, epsilon=1.0, epsilon_decay=0.9995,\n",
        "                 epsilon_min=0.25, seed=42):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_learning_rate = min_learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.Q = np.zeros((env.Ns, env.Na))\n",
        "        self.Nsa = np.zeros((env.Ns, env.Na))\n",
        "        self.state = env.reset()\n",
        "        self.RS = np.random.RandomState(seed)\n",
        "\n",
        "    def get_delta(self, r, x, a, y, done):\n",
        "        \"\"\"\n",
        "        :param r: reward\n",
        "        :param x: current state\n",
        "        :param a: current action\n",
        "        :param y: next state\n",
        "        :param done:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        max_q_y_a = self.Q[y, :].max()\n",
        "        q_x_a = self.Q[x, a]\n",
        "\n",
        "        return r + self.gamma*max_q_y_a - q_x_a\n",
        "\n",
        "    def get_learning_rate(self, s, a):\n",
        "        if self.learning_rate is None:\n",
        "            return max(1.0/max(1.0, self.Nsa[s, a])**self.alpha, self.min_learning_rate)\n",
        "        else:\n",
        "            return max(self.learning_rate, self.min_learning_rate)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if self.RS.uniform(0, 1) < self.epsilon:\n",
        "            # explore\n",
        "            return np.random.choice(np.arange(0, self.env.alot_loc_dcl_actions_full_array.shape[0]))\n",
        "        else:\n",
        "            # exploit\n",
        "            a = self.Q[state, :].argmax()\n",
        "            return a\n",
        "\n",
        "    def step(self):\n",
        "        # Current state\n",
        "        x = self.env.state_indx_curr\n",
        "\n",
        "        # Choose action\n",
        "        a = self.get_action(x)\n",
        "\n",
        "        # Learning rate\n",
        "        alpha = self.get_learning_rate(x, a)\n",
        "\n",
        "        # Take step\n",
        "        observation, reward, done, info = self.env.step(a)\n",
        "        y = observation\n",
        "        r = reward\n",
        "        delta = self.get_delta(r, x, a, y, done)\n",
        "\n",
        "        # Update\n",
        "        self.Q[x, a] = self.Q[x, a] + alpha*delta\n",
        "\n",
        "        self.Nsa[x, a] += 1\n",
        "\n",
        "        if done:\n",
        "            # print(x, observation, reward)\n",
        "            self.epsilon = max(self.epsilon*self.epsilon_decay, self.epsilon_min)\n",
        "            self.env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Wn2kMkgTn3uK"
      },
      "outputs": [],
      "source": [
        "# function for the evaluation of the policy\n",
        "\n",
        "def evaluate_policy(gamma = 0.99999):\n",
        "  global envv\n",
        "  reward_sum = 0\n",
        "  for i in range(0, number_of_allocation_time_periods):\n",
        "    if i == 0:\n",
        "      state = envv.reset()\n",
        "    a = qlearning.Q[state, :].argmax()\n",
        "    state, reward, done, info = envv.step(a)\n",
        "    reward_sum = reward_sum + (gamma ** i) * reward\n",
        "  return reward_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOmzktfHQjS6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Convergence of Q-Learning\n",
        "# ---------------------------\n",
        "\n",
        "gamma = 0.99999\n",
        "np_arr_eval = np.zeros((int(10 ** 5),))\n",
        "save_freq = number_of_allocation_time_periods * 100\n",
        "initial_steps_episod_stall = 10 ** 6\n",
        "delta_episod_stall = 50\n",
        "n_episod_stall = 200\n",
        "count_eval = 0\n",
        "term_n_steps = 0\n",
        "st_time = time.time()\n",
        "\n",
        "# Iterate\n",
        "n_calls = 0\n",
        "count_eval = 0\n",
        "\n",
        "# crating environment object\n",
        "env = Envir(accessibality_based_delivery_cost_weight = 1 / 3, deprivation_cost_weight = 1 / 3, gamma = gamma,\n",
        "            reallocate_t = 5, is_eval = False, is_loc_dec = False, is_use_dyn_prog = False)\n",
        "\n",
        "# crating environment object\n",
        "envv = Envir(accessibality_based_delivery_cost_weight = 1 / 3, deprivation_cost_weight = 1 / 3, gamma = gamma,     # for evaluation only\n",
        "            reallocate_t = 5, is_eval = False, is_loc_dec = False, is_use_dyn_prog = False)\n",
        "\n",
        "# Get optimal value function and its greedy policy\n",
        "Q0 = np.zeros((env.counter_1, env.Na))\n",
        "\n",
        "# Create qlearning object\n",
        "qlearning = QLearning(env, gamma=env.gamma)\n",
        "\n",
        "ws.append((\"Data for convergence curves: \",))\n",
        "ws.append((\"Wall Time\", \"Step\", \"Value\"))   # for the creation of header in te excel sheet\n",
        "\n",
        "# Q_est = np.zeros((n_steps, env.counter_1, env.Na))\n",
        "while True:\n",
        "\n",
        "    if (n_calls > initial_steps_episod_stall) and (n_calls % save_freq == 0):\n",
        "        np_arr_eval[int(count_eval)] = evaluate_policy()\n",
        "\n",
        "        if (count_eval >= n_episod_stall):\n",
        "          y = np.sort(np.abs(np_arr_eval[int(count_eval - n_episod_stall) : int(count_eval)] - np_arr_eval[int(count_eval)]))[0 : n_episod_stall - 100]\n",
        "          if ((time.time() - st_time) >= 18000) or (np.max(y) <= delta_episod_stall):\n",
        "            # print(((np.max(np.abs(np_arr_eval[int(count_eval - n_episod_stall) : int(count_eval)] - int(np_arr_eval[int(self.count_eval)]))) / int(np_arr_eval[int(self.count_eval)])) * 100), \"qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq\")\n",
        "            # print(int(np_arr_eval[int(count_eval)]), \"dddddddddddddddddddddddddddddddddddddddddd\")\n",
        "            # print(np.max(np.abs(np_arr_eval[int(count_eval - n_episod_stall) : int(count_eval)] - int(np_arr_eval[int(count_eval)]))), \"uuuuuuuuuuuuuuuuuuuuuu\")\n",
        "            term_n_steps = n_calls\n",
        "            exce_time = time.time() - st_time\n",
        "            print(exce_time)\n",
        "            break\n",
        "        count_eval = count_eval + 1\n",
        "\n",
        "    qlearning.step()\n",
        "\n",
        "    # Store estimate of Q*\n",
        "    # Q_est[tt, :, :] = qlearning.Q\n",
        "    n_calls +=1\n",
        "\n",
        "    # for te evaluatoin of the agent after predetermined number of steps\n",
        "    if n_calls % (1000 * number_of_allocation_time_periods) == 0:\n",
        "      ws.append((time.time(), n_calls, evaluate_policy()))\n",
        "\n",
        "# Compute greedy policy (with estimated Q)\n",
        "greedy_policy_q_learning = np.argmax(qlearning.Q, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymPveQIrmhbP"
      },
      "outputs": [],
      "source": [
        "# number of time steps for which te algorithm is trained\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Training time: \", exce_time))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Number of steps for training: \", n_calls))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "print(n_calls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et0q3EqYK1IX"
      },
      "source": [
        "## **Evaluation of Q-Learning agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwGkwUlMHDZr"
      },
      "outputs": [],
      "source": [
        "n_eval_ep = 1\n",
        "\n",
        "obj_val_q_learning = 0\n",
        "\n",
        "# for storing action during evaluation simulation\n",
        "if env.is_loc_dec:\n",
        "  act_store = np.zeros((env.number_of_allocation_time_periods, env.no_aa * env.no_materials * env.no_dcl_loc_aval + env.no_dcl_loc_aval))\n",
        "else:\n",
        "  act_store = np.zeros((env.number_of_allocation_time_periods, env.no_aa * env.no_materials * env.no_dcl_loc_aval))\n",
        "\n",
        "# for storing states during evaluation simulation\n",
        "if env.is_loc_dec:\n",
        "  state_store = np.zeros((env.number_of_allocation_time_periods + 1, env.no_aa * env.no_materials + env.no_dcl_loc_aval))\n",
        "else:\n",
        "  state_store = np.zeros((env.number_of_allocation_time_periods + 1, env.no_aa * env.no_materials))\n",
        "\n",
        "ws.append((\"Stagewise objective function value or reward:\",))\n",
        "\n",
        "for i in range(0, n_eval_ep):     # loop over the number of episodes\n",
        "  s = env.reset()     # initial state\n",
        "  if env.is_loc_dec:\n",
        "    state_store[0, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials + env.no_dcl_loc_aval]\n",
        "  else:\n",
        "    state_store[0, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials]\n",
        "  for j in range(0, env.number_of_allocation_time_periods):     # loop over all the time peirods inside an episode\n",
        "    a = greedy_policy_q_learning[s]\n",
        "    act_store[j, :] = env.alot_loc_dcl_actions_full_array[a]\n",
        "    s, reward, done, info = env.step(a)\n",
        "\n",
        "    # storing all the stage wise objective values into an excel sheet\n",
        "    ws.append((f\"Stagewise objective function value at stage {j + 1}: \", reward))\n",
        "\n",
        "    obj_val_q_learning = obj_val_q_learning + (gamma ** i) * reward\n",
        "    if env.is_loc_dec:\n",
        "      state_store[j + 1, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials + env.no_dcl_loc_aval]\n",
        "    else:\n",
        "      state_store[j + 1, :] = env.state_storg_arr[s, :env.no_aa * env.no_materials]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEFYddrwl2Ey"
      },
      "outputs": [],
      "source": [
        "act_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SClWyGNWHDZv"
      },
      "outputs": [],
      "source": [
        "# storing all the objective function valeu (sum of all the rewards) into an excel sheet\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Objective function value: \", obj_val_q_learning))\n",
        "obj_val_q_learning\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp7G-AUwUWwt"
      },
      "outputs": [],
      "source": [
        "# storing all the state variables value to an excel sheet\n",
        "ws.append((\"State Variables\",))\n",
        "for i in state_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "# storing all the agent's actions into an excel sheet\n",
        "ws.append((\"Decision Variables\",))\n",
        "for i in act_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDkcQ2D4cAHL"
      },
      "outputs": [],
      "source": [
        "# saving of the excel workbook at the end of the last experient of an instance, the workbook is saved with the abbreviaed notation of the problem instance as the name\n",
        "save_temp = sum(cap_dist) / max(cap_consum_mat)\n",
        "wb.save(f'/content/drive/MyDrive/Q_learning_agent_({no_aa},_{list(save_temp)},_{number_of_allocation_time_periods})_{seed}_tensorboard_convergence_curves.xlsx')      # this statement wil only be exceuted at the end of the last experieent"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EehMAfXJR7Vd",
        "p-f5X-C-wLdm",
        "fJkFfpGrm0Qv",
        "vhnKb-Onalbm",
        "vP-XzanuKxy0",
        "ow0Jh2rfM-_2",
        "ogIF8HFcEv9S",
        "Et0q3EqYK1IX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}