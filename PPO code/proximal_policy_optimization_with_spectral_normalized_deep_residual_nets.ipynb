{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZsilbzh33bT"
      },
      "outputs": [],
      "source": [
        "# making the experiments as much reproducible as possible (due to the usage of GPU and cuda, all the experiments may not be fully reproducible)\n",
        "seed = 10\n",
        "from numpy.random import seed as sd\n",
        "sd(seed)\n",
        "import random\n",
        "random.seed(seed)\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.random.set_random_seed(seed)\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['CHAINER_SEED'] = str(seed)\n",
        "# import chainer\n",
        "# chainer.cuda.cupy.random.seed(seed)\n",
        "# chainer.backends.cuda.get_device_from_id(gpu).use()\n",
        "# chainer.backends.cuda.cupy.random.seed(seed)\n",
        "# pyrandom.seed(seed)\n",
        "# with chainer.cuda.get_device_from_id(gpu):\n",
        "#   chainer.cuda.cupy.random.seed(seed)\n",
        "\n",
        "# importing of required libraries\n",
        "import openpyxl\n",
        "\n",
        "# Python 3.6 or 3.7 is required for running this notebook (run on GPU runtime)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating te excel sheet for insertion of data using \"openpyxl\"\n",
        "\n",
        "wb = openpyxl.Workbook()     # creating a new workbook\n",
        "ws = wb.active   # activating the workbook for reading and writing of data into and from the workbook respectively\n",
        "ws.append((\"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", f\"Random seed: {seed}\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ],
      "metadata": {
        "id": "j0Ewgn--C2n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5NnNlzxGhxJ"
      },
      "source": [
        "# **Installation of old version of python (3.7.16) in Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eNrhqaWfeZk"
      },
      "outputs": [],
      "source": [
        "#install python 3.7\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.7\n",
        "\n",
        "#change alternatives\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 2\n",
        "\n",
        "#check python version\n",
        "!python --version\n",
        "#3.7.16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW6zSWkWiKpa"
      },
      "outputs": [],
      "source": [
        "# install pip for new python\n",
        "!sudo apt-get install python3.7-distutils\n",
        "!wget https://bootstrap.pypa.io/get-pip.py\n",
        "!python get-pip.py\n",
        "\n",
        "# credit of these last two commands blongs to @Erik\n",
        "# install colab's dependencies\n",
        "!python -m pip install ipython ipython_genutils ipykernel jupyter_console prompt_toolkit httplib2 astor\n",
        "\n",
        "# link to the old google package\n",
        "!ln -s /usr/local/lib/python3.10/dist-packages/google \\\n",
        "       /usr/local/lib/python3.7/dist-packages/google\n",
        "\n",
        "# due to continuous changes in colab environment itself, this installation of Python 3.7 may not work so please find an approproate which can install the appropriate Python version and then run the rest of the notebook as usual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqdj2CUSUSu"
      },
      "source": [
        "# **Installation of all the packages into the newly installed old version of python**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# writing the \"requirements.txt\" file\n",
        "\n",
        "f = open(\"requirements.txt\", \"w\")\n",
        "f.write(\"\"\"# the requirments are at \"https://github.com/RussTedrake/manipulation/blob/master/colab-requirements.txt\"\n",
        "#\n",
        "# This file is autogenerated by pip-compile with python 3.7\n",
        "# To update, run:\n",
        "#\n",
        "#    pip-compile --output-file=colab-requirements.txt colab-requirements.in setup.cfg\n",
        "#\n",
        "absl-py==0.12.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "bayesian-optimization==1.2.0\n",
        "    # via nevergrad\n",
        "bleach==4.1.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "cachetools==4.2.4\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   google-auth\n",
        "certifi==2021.10.8\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests\n",
        "chardet==3.0.4\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests\n",
        "cloudpickle==1.6.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   gym\n",
        "    #   manipulation (setup.cfg)\n",
        "    #   stable-baselines3\n",
        "cma==3.1.0\n",
        "    # via nevergrad\n",
        "cycler==0.11.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   matplotlib\n",
        "decorator==4.4.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "defusedxml==0.7.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "easyprocess==0.3\n",
        "    # via pyvirtualdisplay\n",
        "entrypoints==0.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "future==0.16.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   pyglet\n",
        "google-auth==1.35.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   google-auth-oauthlib\n",
        "    #   tensorboard\n",
        "google-auth-oauthlib==0.4.6\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "gradescope-utils==0.4.0\n",
        "    # via manipulation (setup.cfg)\n",
        "grpcio==1.42.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "gym==0.17.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   stable-baselines3\n",
        "idna==2.10\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests\n",
        "imageio==2.4.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-image\n",
        "imgaug==0.2.6\n",
        "    # via -r colab-requirements.in\n",
        "importlib-metadata==4.8.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   markdown\n",
        "ipykernel==4.10.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipywidgets\n",
        "    #   notebook\n",
        "ipython==5.5.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipykernel\n",
        "    #   ipywidgets\n",
        "    #   meshcat\n",
        "ipython-genutils==0.2.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipywidgets\n",
        "    #   nbformat\n",
        "    #   notebook\n",
        "ipywidgets==7.6.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   open3d\n",
        "jinja2==2.11.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   mpld3\n",
        "    #   nbconvert\n",
        "    #   notebook\n",
        "joblib==1.1.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-learn\n",
        "jsonschema==2.6.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbformat\n",
        "jupyter-client==5.3.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipykernel\n",
        "    #   notebook\n",
        "jupyter-core==4.9.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   jupyter-client\n",
        "    #   nbconvert\n",
        "    #   nbformat\n",
        "    #   notebook\n",
        "jupyter-console==5.0.0\n",
        "jupyterlab-widgets==1.0.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipywidgets\n",
        "kiwisolver==1.3.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   matplotlib\n",
        "markdown==3.3.6\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "markupsafe==2.0.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   jinja2\n",
        "matplotlib==3.2.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   drake\n",
        "    #   mpld3\n",
        "    #   open3d\n",
        "    #   scikit-image\n",
        "    #   stable-baselines3\n",
        "meshcat==0.3.2\n",
        "    # via drake\n",
        "mistune==0.8.4\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "mpld3==0.5.1\n",
        "    # via manipulation (setup.cfg)\n",
        "nbconvert==5.6.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   notebook\n",
        "nbformat==5.1.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipywidgets\n",
        "    #   nbconvert\n",
        "    #   notebook\n",
        "networkx==2.6.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-image\n",
        "nevergrad==0.4.3.post9\n",
        "    # via manipulation (setup.cfg)\n",
        "notebook==5.3.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   open3d\n",
        "    #   widgetsnbextension\n",
        "numpy==1.19.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   bayesian-optimization\n",
        "    #   drake\n",
        "    #   gym\n",
        "    #   imageio\n",
        "    #   imgaug\n",
        "    #   matplotlib\n",
        "    #   meshcat\n",
        "    #   nevergrad\n",
        "    #   open3d\n",
        "    #   pandas\n",
        "    #   pywavelets\n",
        "    #   scikit-image\n",
        "    #   scikit-learn\n",
        "    #   scipy\n",
        "    #   stable-baselines3\n",
        "    #   tensorboard\n",
        "    #   tifffile\n",
        "    #   torchvision\n",
        "oauthlib==3.1.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests-oauthlib\n",
        "open3d==0.10.0.0\n",
        "    # via manipulation (setup.cfg)\n",
        "packaging==21.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   bleach\n",
        "pandas==1.1.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   stable-baselines3\n",
        "pandocfilters==1.5.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "pexpect==4.8.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "pickleshare==0.7.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "pillow==7.1.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   imageio\n",
        "    #   meshcat\n",
        "    #   scikit-image\n",
        "    #   torchvision\n",
        "prompt-toolkit==1.0.18\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "protobuf==3.17.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "ptyprocess==0.7.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   pexpect\n",
        "    #   terminado\n",
        "pyasn1==0.4.8\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   pyasn1-modules\n",
        "    #   rsa\n",
        "pyasn1-modules==0.2.8\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   google-auth\n",
        "pydot==1.3.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   drake\n",
        "pygame==2.1.2\n",
        "    # via drake\n",
        "pyglet==1.5.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   gym\n",
        "pygments==2.6.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "    #   nbconvert\n",
        "pymcubes==0.1.2\n",
        "    # via manipulation (setup.cfg)\n",
        "pyngrok==5.0.2\n",
        "    # via\n",
        "    #   -r colab-requirements.in\n",
        "    #   meshcat\n",
        "pyparsing==3.0.6\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   matplotlib\n",
        "    #   packaging\n",
        "    #   pydot\n",
        "python-dateutil==2.8.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   jupyter-client\n",
        "    #   matplotlib\n",
        "    #   pandas\n",
        "pytz==2018.9\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   pandas\n",
        "pyvirtualdisplay==1.3.2\n",
        "    # via -r colab-requirements.in\n",
        "pywavelets==1.2.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-image\n",
        "pyyaml==3.13\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   drake\n",
        "    #   pyngrok\n",
        "pyzmq==22.3.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   jupyter-client\n",
        "    #   meshcat\n",
        "requests==2.23.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests-oauthlib\n",
        "    #   tensorboard\n",
        "requests-oauthlib==1.3.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   google-auth-oauthlib\n",
        "rsa==4.8\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   google-auth\n",
        "scikit-image==0.18.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   imgaug\n",
        "scikit-learn==1.0.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   bayesian-optimization\n",
        "scipy==1.5.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   bayesian-optimization\n",
        "    #   drake\n",
        "    #   gym\n",
        "    #   imgaug\n",
        "    #   scikit-image\n",
        "    #   scikit-learn\n",
        "send2trash==1.8.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   notebook\n",
        "simplegeneric==0.8.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "six==1.15.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   absl-py\n",
        "    #   bleach\n",
        "    #   google-auth\n",
        "    #   grpcio\n",
        "    #   imgaug\n",
        "    #   prompt-toolkit\n",
        "    #   protobuf\n",
        "    #   python-dateutil\n",
        "tensorboard==1.15.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   manipulation (setup.cfg)\n",
        "tensorboard-data-server==0.6.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "tensorboard-plugin-wit==1.8.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "terminado==0.12.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   notebook\n",
        "testpath==0.5.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "threadpoolctl==3.0.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-learn\n",
        "tifffile==2021.11.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-image\n",
        "timeout-decorator==0.5.0\n",
        "    # via manipulation (setup.cfg)\n",
        "tornado==5.1.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipykernel\n",
        "    #   jupyter-client\n",
        "    #   meshcat\n",
        "    #   notebook\n",
        "    #   terminado\n",
        "traitlets==5.1.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipykernel\n",
        "    #   ipython\n",
        "    #   ipywidgets\n",
        "    #   jupyter-client\n",
        "    #   jupyter-core\n",
        "    #   nbconvert\n",
        "    #   nbformat\n",
        "    #   notebook\n",
        "typing-extensions==3.10.0.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   importlib-metadata\n",
        "    #   nevergrad\n",
        "    #   torch\n",
        "u-msgpack-python==2.7.1\n",
        "    # via meshcat\n",
        "urllib3==1.24.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests\n",
        "wcwidth==0.2.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   prompt-toolkit\n",
        "webencodings==0.5.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   bleach\n",
        "werkzeug==1.0.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "wheel==0.37.0\n",
        "    # via tensorboard\n",
        "widgetsnbextension==3.5.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipywidgets\n",
        "    #   open3d\n",
        "zipp==3.6.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   importlib-metadata\n",
        "\n",
        "# The following packages are considered to be unsafe in a requirements file:\n",
        "# setuptools\n",
        "tensorflow==1.15.0\n",
        "optuna==2.10.0                        # Optuna is a python library for hyperparameter optimization in machine learning. Optuna python library is framework agnostic.\n",
        "plotly==5.6.0\n",
        "pyflann-py3==0.1.0\n",
        "codecarbon==2.1.4\"\"\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "W3UqKwaF5B6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0xM-DZYUpP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5465b6fc-43fe-4713-fbdc-481f70c1248d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==0.12.0 (from -r requirements.txt (line 8))\n",
            "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bayesian-optimization==1.2.0 (from -r requirements.txt (line 12))\n",
            "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bleach==4.1.0 (from -r requirements.txt (line 14))\n",
            "  Downloading bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.9/157.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools==4.2.4 (from -r requirements.txt (line 18))\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting certifi==2021.10.8 (from -r requirements.txt (line 22))\n",
            "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==3.0.4 (from -r requirements.txt (line 26))\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle==1.6.0 (from -r requirements.txt (line 30))\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting cma==3.1.0 (from -r requirements.txt (line 36))\n",
            "  Downloading cma-3.1.0-py2.py3-none-any.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.5/269.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler==0.11.0 (from -r requirements.txt (line 38))\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting decorator==4.4.2 (from -r requirements.txt (line 42))\n",
            "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting defusedxml==0.7.1 (from -r requirements.txt (line 46))\n",
            "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting easyprocess==0.3 (from -r requirements.txt (line 50))\n",
            "  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting entrypoints==0.3 (from -r requirements.txt (line 52))\n",
            "  Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
            "Collecting future==0.16.0 (from -r requirements.txt (line 56))\n",
            "  Downloading future-0.16.0.tar.gz (824 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m824.5/824.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-auth==1.35.0 (from -r requirements.txt (line 60))\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-oauthlib==0.4.6 (from -r requirements.txt (line 65))\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting gradescope-utils==0.4.0 (from -r requirements.txt (line 69))\n",
            "  Downloading gradescope_utils-0.4.0-py2.py3-none-any.whl (5.6 kB)\n",
            "Collecting grpcio==1.42.0 (from -r requirements.txt (line 71))\n",
            "  Downloading grpcio-1.42.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym==0.17.3 (from -r requirements.txt (line 75))\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting idna==2.10 (from -r requirements.txt (line 79))\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio==2.4.1 (from -r requirements.txt (line 83))\n",
            "  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting imgaug==0.2.6 (from -r requirements.txt (line 87))\n",
            "  Downloading imgaug-0.2.6.tar.gz (631 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.4/631.4 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata==4.8.2 (from -r requirements.txt (line 89))\n",
            "  Downloading importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting ipykernel==4.10.1 (from -r requirements.txt (line 93))\n",
            "  Downloading ipykernel-4.10.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.9/109.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython==5.5.0 (from -r requirements.txt (line 98))\n",
            "  Downloading ipython-5.5.0-py3-none-any.whl (758 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m758.9/758.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 104)) (0.2.0)\n",
            "Collecting ipywidgets==7.6.5 (from -r requirements.txt (line 110))\n",
            "  Downloading ipywidgets-7.6.5-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.8/121.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2==2.11.3 (from -r requirements.txt (line 114))\n",
            "  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib==1.1.0 (from -r requirements.txt (line 120))\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonschema==2.6.0 (from -r requirements.txt (line 124))\n",
            "  Downloading jsonschema-2.6.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting jupyter-client==5.3.5 (from -r requirements.txt (line 128))\n",
            "  Downloading jupyter_client-5.3.5-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.4/92.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-core==4.9.1 (from -r requirements.txt (line 133))\n",
            "  Downloading jupyter_core-4.9.1-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-console==5.0.0 (from -r requirements.txt (line 140))\n",
            "  Downloading jupyter_console-5.0.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting jupyterlab-widgets==1.0.2 (from -r requirements.txt (line 141))\n",
            "  Downloading jupyterlab_widgets-1.0.2-py3-none-any.whl (243 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver==1.3.2 (from -r requirements.txt (line 145))\n",
            "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown==3.3.6 (from -r requirements.txt (line 149))\n",
            "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markupsafe==2.0.1 (from -r requirements.txt (line 153))\n",
            "  Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (31 kB)\n",
            "Collecting matplotlib==3.2.2 (from -r requirements.txt (line 157))\n",
            "  Downloading matplotlib-3.2.2-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting meshcat==0.3.2 (from -r requirements.txt (line 165))\n",
            "  Downloading meshcat-0.3.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mistune==0.8.4 (from -r requirements.txt (line 167))\n",
            "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting mpld3==0.5.1 (from -r requirements.txt (line 171))\n",
            "  Downloading mpld3-0.5.1.tar.gz (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nbconvert==5.6.1 (from -r requirements.txt (line 173))\n",
            "  Downloading nbconvert-5.6.1-py2.py3-none-any.whl (455 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.1/455.1 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbformat==5.1.3 (from -r requirements.txt (line 177))\n",
            "  Downloading nbformat-5.1.3-py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx==2.6.3 (from -r requirements.txt (line 183))\n",
            "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nevergrad==0.4.3.post9 (from -r requirements.txt (line 187))\n",
            "  Downloading nevergrad-0.4.3.post9-py3-none-any.whl (411 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.3/411.3 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting notebook==5.3.1 (from -r requirements.txt (line 189))\n",
            "  Downloading notebook-5.3.1-py2.py3-none-any.whl (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.19.5 (from -r requirements.txt (line 194))\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oauthlib==3.1.1 (from -r requirements.txt (line 215))\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.2/146.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting open3d==0.10.0.0 (from -r requirements.txt (line 219))\n",
            "  Downloading open3d-0.10.0.0-cp37-cp37m-manylinux1_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==21.3 (from -r requirements.txt (line 221))\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.1.5 (from -r requirements.txt (line 225))\n",
            "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandocfilters==1.5.0 (from -r requirements.txt (line 229))\n",
            "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 233)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 237)) (0.7.5)\n",
            "Collecting pillow==7.1.2 (from -r requirements.txt (line 241))\n",
            "  Downloading Pillow-7.1.2-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prompt-toolkit==1.0.18 (from -r requirements.txt (line 248))\n",
            "  Downloading prompt_toolkit-1.0.18-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.4/245.4 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.17.3 (from -r requirements.txt (line 252))\n",
            "  Downloading protobuf-3.17.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 256)) (0.7.0)\n",
            "Collecting pyasn1==0.4.8 (from -r requirements.txt (line 261))\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyasn1-modules==0.2.8 (from -r requirements.txt (line 266))\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydot==1.3.0 (from -r requirements.txt (line 270))\n",
            "  Downloading pydot-1.3.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting pygame==2.1.2 (from -r requirements.txt (line 274))\n",
            "  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyglet==1.5.0 (from -r requirements.txt (line 276))\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments==2.6.1 (from -r requirements.txt (line 280))\n",
            "  Downloading Pygments-2.6.1-py3-none-any.whl (914 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.4/914.4 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymcubes==0.1.2 (from -r requirements.txt (line 285))\n",
            "  Downloading PyMCubes-0.1.2-cp37-cp37m-manylinux2010_x86_64.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok==5.0.2 (from -r requirements.txt (line 287))\n",
            "  Downloading pyngrok-5.0.2.tar.gz (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.4/731.4 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyparsing==3.0.6 (from -r requirements.txt (line 291))\n",
            "  Downloading pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 297)) (2.8.2)\n",
            "Collecting pytz==2018.9 (from -r requirements.txt (line 303))\n",
            "  Downloading pytz-2018.9-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.7/510.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyvirtualdisplay==1.3.2 (from -r requirements.txt (line 307))\n",
            "  Downloading PyVirtualDisplay-1.3.2-py2.py3-none-any.whl (14 kB)\n",
            "Collecting pywavelets==1.2.0 (from -r requirements.txt (line 309))\n",
            "  Downloading PyWavelets-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml==3.13 (from -r requirements.txt (line 313))\n",
            "  Downloading PyYAML-3.13.tar.gz (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.6/270.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyzmq==22.3.0 (from -r requirements.txt (line 318))\n",
            "  Downloading pyzmq-22.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests==2.23.0 (from -r requirements.txt (line 323))\n",
            "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-oauthlib==1.3.0 (from -r requirements.txt (line 328))\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting rsa==4.8 (from -r requirements.txt (line 332))\n",
            "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting scikit-image==0.18.3 (from -r requirements.txt (line 336))\n",
            "  Downloading scikit_image-0.18.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (29.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.2/29.2 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.0.1 (from -r requirements.txt (line 340))\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.5.3 (from -r requirements.txt (line 344))\n",
            "  Downloading scipy-1.5.3-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.9/25.9 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting send2trash==1.8.0 (from -r requirements.txt (line 353))\n",
            "  Downloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
            "Collecting simplegeneric==0.8.1 (from -r requirements.txt (line 357))\n",
            "  Downloading simplegeneric-0.8.1.zip (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting six==1.15.0 (from -r requirements.txt (line 361))\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting tensorboard==1.15.0 (from -r requirements.txt (line 372))\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-data-server==0.6.1 (from -r requirements.txt (line 376))\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit==1.8.0 (from -r requirements.txt (line 380))\n",
            "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.2/781.2 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting terminado==0.12.1 (from -r requirements.txt (line 384))\n",
            "  Downloading terminado-0.12.1-py3-none-any.whl (15 kB)\n",
            "Collecting testpath==0.5.0 (from -r requirements.txt (line 388))\n",
            "  Downloading testpath-0.5.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl==3.0.0 (from -r requirements.txt (line 392))\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting tifffile==2021.11.2 (from -r requirements.txt (line 396))\n",
            "  Downloading tifffile-2021.11.2-py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timeout-decorator==0.5.0 (from -r requirements.txt (line 400))\n",
            "  Downloading timeout-decorator-0.5.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tornado==5.1.1 (from -r requirements.txt (line 402))\n",
            "  Downloading tornado-5.1.1.tar.gz (516 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.8/516.8 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting traitlets==5.1.1 (from -r requirements.txt (line 410))\n",
            "  Downloading traitlets-5.1.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions==3.10.0.2 (from -r requirements.txt (line 421))\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting u-msgpack-python==2.7.1 (from -r requirements.txt (line 427))\n",
            "  Downloading u_msgpack_python-2.7.1-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting urllib3==1.24.3 (from -r requirements.txt (line 429))\n",
            "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.8/118.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wcwidth==0.2.5 (from -r requirements.txt (line 433))\n",
            "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting webencodings==0.5.1 (from -r requirements.txt (line 437))\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting werkzeug==1.0.1 (from -r requirements.txt (line 441))\n",
            "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wheel==0.37.0 (from -r requirements.txt (line 445))\n",
            "  Downloading wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
            "Collecting widgetsnbextension==3.5.2 (from -r requirements.txt (line 447))\n",
            "  Downloading widgetsnbextension-3.5.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zipp==3.6.0 (from -r requirements.txt (line 452))\n",
            "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting tensorflow==1.15.0 (from -r requirements.txt (line 459))\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optuna==2.10.0 (from -r requirements.txt (line 460))\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.2/308.2 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plotly==5.6.0 (from -r requirements.txt (line 461))\n",
            "  Downloading plotly-5.6.0-py2.py3-none-any.whl (27.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.7/27.7 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyflann-py3==0.1.0 (from -r requirements.txt (line 462))\n",
            "  Downloading pyflann-py3-0.1.0.tar.gz (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting codecarbon==2.1.4 (from -r requirements.txt (line 463))\n",
            "  Downloading codecarbon-2.1.4-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.9/174.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.35.0->-r requirements.txt (line 60)) (68.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 459)) (0.8.1)\n",
            "Collecting gast==0.2.2 (from tensorflow==1.15.0->-r requirements.txt (line 459))\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-pasta>=0.1.6 (from tensorflow==1.15.0->-r requirements.txt (line 459))\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-applications>=1.0.8 (from tensorflow==1.15.0->-r requirements.txt (line 459))\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow==1.15.0->-r requirements.txt (line 459))\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2 (from tensorflow==1.15.0->-r requirements.txt (line 459))\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1 (from tensorflow==1.15.0->-r requirements.txt (line 459))\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.4/503.4 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow==1.15.0->-r requirements.txt (line 459))\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting wrapt>=1.11.1 (from tensorflow==1.15.0->-r requirements.txt (line 459))\n",
            "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic (from optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Obtaining dependency information for alembic from https://files.pythonhosted.org/packages/34/fe/eebb260c86c71d9ed861aa1434fc50601df657425b18329994af8c0bd789/alembic-1.11.2-py3-none-any.whl.metadata\n",
            "  Downloading alembic-1.11.2-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting cliff (from optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.8.2 (from optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Obtaining dependency information for cmaes>=0.8.2 from https://files.pythonhosted.org/packages/f7/46/7d9544d453346f6c0c405916c95fdb653491ea2e9976cabb810ba2fe8cd4/cmaes-0.10.0-py3-none-any.whl.metadata\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting colorlog (from optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting sqlalchemy>=1.1.0 (from optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Obtaining dependency information for sqlalchemy>=1.1.0 from https://files.pythonhosted.org/packages/3f/9e/4848abbf34d8a1e328f38ed67bb824b33c279b2fc41636df94bad758d968/SQLAlchemy-2.0.19-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading SQLAlchemy-2.0.19-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.4 kB)\n",
            "Collecting tqdm (from optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tenacity>=6.2.0 (from plotly==5.6.0->-r requirements.txt (line 461))\n",
            "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
            "Collecting arrow (from codecarbon==2.1.4->-r requirements.txt (line 463))\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynvml (from codecarbon==2.1.4->-r requirements.txt (line 463))\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from codecarbon==2.1.4->-r requirements.txt (line 463)) (5.9.5)\n",
            "Collecting py-cpuinfo (from codecarbon==2.1.4->-r requirements.txt (line 463))\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting fuzzywuzzy (from codecarbon==2.1.4->-r requirements.txt (line 463))\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting click (from codecarbon==2.1.4->-r requirements.txt (line 463))\n",
            "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/1a/70/e63223f8116931d365993d4a6b7ef653a4d920b41d03de7c59499962821f/click-8.1.6-py3-none-any.whl.metadata\n",
            "  Downloading click-8.1.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting h5py (from keras-applications>=1.0.8->tensorflow==1.15.0->-r requirements.txt (line 459))\n",
            "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of sqlalchemy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sqlalchemy>=1.1.0 (from optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Obtaining dependency information for sqlalchemy>=1.1.0 from https://files.pythonhosted.org/packages/9f/87/fbfa01c280833a20010dc70ac753d7ccf5debcd116a0f55961b0e03f0915/SQLAlchemy-2.0.18-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading SQLAlchemy-2.0.18-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.4 kB)\n",
            "  Obtaining dependency information for sqlalchemy>=1.1.0 from https://files.pythonhosted.org/packages/02/bd/02387ae8021175f3b6cfe9ab65c82ac40d8d82805740c8e71ee4fcf673e4/SQLAlchemy-2.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading SQLAlchemy-2.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.4 kB)\n",
            "  Obtaining dependency information for sqlalchemy>=1.1.0 from https://files.pythonhosted.org/packages/25/49/a7f72f347a1a6ba1f5bec4f29bfc5f98745f94b6e9692da60f39cf84b7a6/SQLAlchemy-2.0.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading SQLAlchemy-2.0.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.4 kB)\n",
            "  Obtaining dependency information for sqlalchemy>=1.1.0 from https://files.pythonhosted.org/packages/c2/e7/e1ff8d966e5102d4e66b11caba85725ca0f96a34588131a8fc288e7906ad/SQLAlchemy-2.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading SQLAlchemy-2.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "  Obtaining dependency information for sqlalchemy>=1.1.0 from https://files.pythonhosted.org/packages/93/24/5a1308f5e259e3b8b409d703b86efcb33e16dcaeb7f97c8a5bb06669d22c/SQLAlchemy-2.0.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading SQLAlchemy-2.0.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "  Downloading SQLAlchemy-2.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of sqlalchemy to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading SQLAlchemy-2.0.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading SQLAlchemy-2.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.5.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading SQLAlchemy-2.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Obtaining dependency information for sqlalchemy>=1.1.0 from https://files.pythonhosted.org/packages/b4/3a/78aefb3a2ab4181401339116bcdcf442a5c33f8c0df51a53dd6d45d88b86/SQLAlchemy-1.4.49-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading SQLAlchemy-1.4.49-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.1.0->optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading greenlet-2.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (566 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Mako (from alembic->optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of alembic to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting alembic (from optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Obtaining dependency information for alembic from https://files.pythonhosted.org/packages/11/00/46a4f66ad54c661350a1cd5daae4b4ab2232486c55635ee12ff12958b03f/alembic-1.11.1-py3-none-any.whl.metadata\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Obtaining dependency information for alembic from https://files.pythonhosted.org/packages/59/4d/28b13ff3a9c26988f8c32f460cc34ee806ac46038232ee493b9baaaf6164/alembic-1.11.0-py3-none-any.whl.metadata\n",
            "  Downloading alembic-1.11.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading alembic-1.10.4-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading alembic-1.10.3-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading alembic-1.10.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading alembic-1.10.0-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.9/211.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of alembic to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-resources (from alembic->optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
            "Collecting PrettyTable>=0.7.2 (from cliff->optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading prettytable-3.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting autopage>=0.4.0 (from cliff->optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting cmd2>=1.0.0 (from cliff->optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading cmd2-2.4.3-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.2/147.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0 (from cliff->optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stevedore>=2.0.1 (from cliff->optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading stevedore-3.5.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=16.3.0 (from cmd2>=1.0.0->cliff->optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyperclip>=1.6 (from cmd2>=1.0.0->cliff->optuna==2.10.0->-r requirements.txt (line 460))\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Downloading SQLAlchemy-1.4.49-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.6-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: bayesian-optimization, future, gym, imageio, imgaug, mpld3, pyngrok, pyyaml, simplegeneric, timeout-decorator, tornado, pyflann-py3, gast, pyperclip\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11660 sha256=9c7c22801c59f362d9e4c7abca8353f68e58b9242d612a63e048a96f45910c75\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.16.0-py3-none-any.whl size=487794 sha256=2f9c50f7278fed9c57d9dbdf0dfbfc76c7acdcf29ea4d3c9b5602fb6dbbec959\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/09/6f/dfd31fb2ebe951aefbee461a0dfce1bdc2e4b2c1016e85d233\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=937544c301bc2b721ee0c8ed86640ef898a66bf023527696da2c0f4345ef639d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/81/4b/dd9c029691022cb957398d1f015e66b75e37637dda61abdf58\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303886 sha256=0e73a175bd183f47527648e13230256c29b6f7493a1e6b7a15eeb1c355a49b6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/20/07/7bb9c8c44e6ec2efa60fd0e6280094f53f65f41767ef69a5ee\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654003 sha256=e1a51a80ea28f650a23fc8e07c4d41aa023159e3bf4e852752d039bfca3563ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/72/98/3ebfdba1069a9a8eaaa7ae7265cfd67d63ef0197aaee2e5f9c\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.5.1-py3-none-any.whl size=364069 sha256=20e46115569e9654193df58d4a04b3641ebba29285a6647aab829bdc00dd8649\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/36/27/d61b9f1327012961fa31f05a20b190f836dd3fcb1c0264177b\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.0.2-py3-none-any.whl size=18965 sha256=d08dd989b549958aa3c70affc0e072624a58720e8f3d17c4584a0ac15962c714\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/fe/90/615cd2d859560bd8eae2865f7539ea958b03c12238eb5c72b1\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-3.13-cp37-cp37m-linux_x86_64.whl size=43102 sha256=5a317d40ca2e9d8a53b69dc8abba6e0157f71658f3ba8ac299e44e7156467ce6\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/cd/14/899edaa9cdb9a65aa7224539f6e0ad488e9a7b202bb48f6ae6\n",
            "  Building wheel for simplegeneric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplegeneric: filename=simplegeneric-0.8.1-py3-none-any.whl size=5059 sha256=6e9989d56c58cdda49cfb5fd2e6184e552d6233e6a25040b02983357ca5c6f99\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/de/df/f1324f4dd966636ab877b82d467acad38a0998b1c42ddb7288\n",
            "  Building wheel for timeout-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timeout-decorator: filename=timeout_decorator-0.5.0-py3-none-any.whl size=5006 sha256=6f108abbbc1c392b9508be83f3036508a811ab7b97ad33edeb85462aeea9ef92\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/64/ac/de1dd54f9a6e48b846e9cb5e4176d6f063380e7f83d69807ad\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-5.1.1-cp37-cp37m-linux_x86_64.whl size=449838 sha256=7fcd4c7a3b4fbfeb044c1e0f35a3c555231bea372c82647c228195274e1bf218\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/91/4b/ee8ffb993d3372fc4129c52c9140792c118dda0373b41e7a8f\n",
            "  Building wheel for pyflann-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyflann-py3: filename=pyflann_py3-0.1.0-py3-none-any.whl size=7186971 sha256=6bbf7838765fdbc6a540a6d53f327ebbac6514f0822f318e01c02dd0ee81aed9\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/1a/4d/6e86c7ecda5792c327e686176dff982eab8c35be47951bd854\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=bd3c74cde9fcc25122c4cfed70bd5a667f3986da58d57fa5e972e148b10cf776\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11122 sha256=0ae4ff6bab4f27d01740e1a3db815e092ea7f57f02f19cc2e8eef191c63a26f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built bayesian-optimization future gym imageio imgaug mpld3 pyngrok pyyaml simplegeneric timeout-decorator tornado pyflann-py3 gast pyperclip\n",
            "Installing collected packages: webencodings, wcwidth, u-msgpack-python, typing-extensions, timeout-decorator, tensorflow-estimator, tensorboard-plugin-wit, simplegeneric, send2trash, pyyaml, pytz, pyperclip, pymcubes, pyflann-py3, pyasn1, py-cpuinfo, mistune, jsonschema, gradescope-utils, fuzzywuzzy, future, easyprocess, cma, chardet, certifi, zipp, wrapt, wheel, werkzeug, urllib3, traitlets, tqdm, tornado, threadpoolctl, testpath, termcolor, tensorboard-data-server, tenacity, six, rsa, pyzmq, pyvirtualdisplay, pyparsing, pynvml, pyngrok, pygments, pyglet, pygame, pyasn1-modules, pillow, pbr, pandocfilters, oauthlib, numpy, networkx, markupsafe, kiwisolver, jupyterlab-widgets, joblib, idna, greenlet, gast, entrypoints, defusedxml, decorator, cycler, colorlog, cloudpickle, cachetools, autopage, tifffile, terminado, scipy, requests, pywavelets, pydot, protobuf, prompt-toolkit, plotly, packaging, opt-einsum, keras-preprocessing, jupyter-core, jinja2, importlib-resources, importlib-metadata, imageio, h5py, grpcio, google-pasta, google-auth, cmaes, absl-py, stevedore, sqlalchemy, scikit-learn, requests-oauthlib, PrettyTable, pandas, nbformat, matplotlib, markdown, Mako, keras-applications, jupyter-client, ipython, gym, click, bleach, attrs, arrow, tensorboard, scikit-image, nbconvert, mpld3, meshcat, ipykernel, google-auth-oauthlib, codecarbon, cmd2, bayesian-optimization, alembic, tensorflow, notebook, nevergrad, jupyter-console, imgaug, cliff, widgetsnbextension, optuna, ipywidgets, open3d\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.6\n",
            "    Uninstalling wcwidth-0.2.6:\n",
            "      Successfully uninstalled wcwidth-0.2.6\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.41.1\n",
            "    Uninstalling wheel-0.41.1:\n",
            "      Successfully uninstalled wheel-0.41.1\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.9.0\n",
            "    Uninstalling traitlets-5.9.0:\n",
            "      Successfully uninstalled traitlets-5.9.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.2\n",
            "    Uninstalling tornado-6.2:\n",
            "      Successfully uninstalled tornado-6.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 25.1.0\n",
            "    Uninstalling pyzmq-25.1.0:\n",
            "      Successfully uninstalled pyzmq-25.1.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.16.1\n",
            "    Uninstalling Pygments-2.16.1:\n",
            "      Successfully uninstalled Pygments-2.16.1\n",
            "  Attempting uninstall: entrypoints\n",
            "    Found existing installation: entrypoints 0.4\n",
            "    Uninstalling entrypoints-0.4:\n",
            "      Successfully uninstalled entrypoints-0.4\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.1.1\n",
            "    Uninstalling decorator-5.1.1:\n",
            "      Successfully uninstalled decorator-5.1.1\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 3.0.39\n",
            "    Uninstalling prompt-toolkit-3.0.39:\n",
            "      Successfully uninstalled prompt-toolkit-3.0.39\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.1\n",
            "    Uninstalling packaging-23.1:\n",
            "      Successfully uninstalled packaging-23.1\n",
            "  Attempting uninstall: jupyter-core\n",
            "    Found existing installation: jupyter_core 4.12.0\n",
            "    Uninstalling jupyter_core-4.12.0:\n",
            "      Successfully uninstalled jupyter_core-4.12.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.6.4\n",
            "    Uninstalling importlib-metadata-4.6.4:\n",
            "      Successfully uninstalled importlib-metadata-4.6.4\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter_client 7.4.9\n",
            "    Uninstalling jupyter_client-7.4.9:\n",
            "      Successfully uninstalled jupyter_client-7.4.9\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.34.0\n",
            "    Uninstalling ipython-7.34.0:\n",
            "      Successfully uninstalled ipython-7.34.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 6.16.2\n",
            "    Uninstalling ipykernel-6.16.2:\n",
            "      Successfully uninstalled ipykernel-6.16.2\n",
            "  Attempting uninstall: jupyter-console\n",
            "    Found existing installation: jupyter-console 6.6.3\n",
            "    Uninstalling jupyter-console-6.6.3:\n",
            "      Successfully uninstalled jupyter-console-6.6.3\n",
            "Successfully installed Mako-1.2.4 PrettyTable-3.7.0 absl-py-0.12.0 alembic-1.9.4 arrow-1.2.3 attrs-23.1.0 autopage-0.5.1 bayesian-optimization-1.2.0 bleach-4.1.0 cachetools-4.2.4 certifi-2021.10.8 chardet-3.0.4 click-8.1.6 cliff-3.10.1 cloudpickle-1.6.0 cma-3.1.0 cmaes-0.10.0 cmd2-2.4.3 codecarbon-2.1.4 colorlog-6.7.0 cycler-0.11.0 decorator-4.4.2 defusedxml-0.7.1 easyprocess-0.3 entrypoints-0.3 future-0.16.0 fuzzywuzzy-0.18.0 gast-0.2.2 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 gradescope-utils-0.4.0 greenlet-2.0.2 grpcio-1.42.0 gym-0.17.3 h5py-3.8.0 idna-2.10 imageio-2.4.1 imgaug-0.2.6 importlib-metadata-4.8.2 importlib-resources-5.12.0 ipykernel-4.10.1 ipython-5.5.0 ipywidgets-7.6.5 jinja2-2.11.3 joblib-1.1.0 jsonschema-2.6.0 jupyter-client-5.3.5 jupyter-console-5.0.0 jupyter-core-4.9.1 jupyterlab-widgets-1.0.2 keras-applications-1.0.8 keras-preprocessing-1.1.2 kiwisolver-1.3.2 markdown-3.3.6 markupsafe-2.0.1 matplotlib-3.2.2 meshcat-0.3.2 mistune-0.8.4 mpld3-0.5.1 nbconvert-5.6.1 nbformat-5.1.3 networkx-2.6.3 nevergrad-0.4.3.post9 notebook-5.3.1 numpy-1.19.5 oauthlib-3.1.1 open3d-0.10.0.0 opt-einsum-3.3.0 optuna-2.10.0 packaging-21.3 pandas-1.1.5 pandocfilters-1.5.0 pbr-5.11.1 pillow-7.1.2 plotly-5.6.0 prompt-toolkit-1.0.18 protobuf-3.17.3 py-cpuinfo-9.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.3.0 pyflann-py3-0.1.0 pygame-2.1.2 pyglet-1.5.0 pygments-2.6.1 pymcubes-0.1.2 pyngrok-5.0.2 pynvml-11.5.0 pyparsing-3.0.6 pyperclip-1.8.2 pytz-2018.9 pyvirtualdisplay-1.3.2 pywavelets-1.2.0 pyyaml-3.13 pyzmq-22.3.0 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.8 scikit-image-0.18.3 scikit-learn-1.0.1 scipy-1.5.3 send2trash-1.8.0 simplegeneric-0.8.1 six-1.15.0 sqlalchemy-1.4.49 stevedore-3.5.2 tenacity-8.2.2 tensorboard-1.15.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 termcolor-2.3.0 terminado-0.12.1 testpath-0.5.0 threadpoolctl-3.0.0 tifffile-2021.11.2 timeout-decorator-0.5.0 tornado-5.1.1 tqdm-4.65.0 traitlets-5.1.1 typing-extensions-3.10.0.2 u-msgpack-python-2.7.1 urllib3-1.24.3 wcwidth-0.2.5 webencodings-0.5.1 werkzeug-1.0.1 wheel-0.37.0 widgetsnbextension-3.5.2 wrapt-1.15.0 zipp-3.6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "decorator",
                  "defusedxml"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines==2.10.1\n",
            "  Downloading stable_baselines-2.10.1-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/240.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/240.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.1/240.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines==2.10.1) (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines==2.10.1) (1.5.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines==2.10.1) (1.1.0)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines==2.10.1) (1.6.0)\n",
            "Collecting opencv-python (from stable-baselines==2.10.1)\n",
            "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/34/7c/8a5043f362b0a55f07812a0db3f86092cdbd0fe41b933d7bc6fce3ab6c15/opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines==2.10.1) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines==2.10.1) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines==2.10.1) (3.2.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines==2.10.1) (1.5.0)\n",
            "Collecting atari-py~=0.2.0 (from gym[atari,classic_control]>=0.11->stable-baselines==2.10.1)\n",
            "  Downloading atari_py-0.2.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines==2.10.1) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines==2.10.1) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines==2.10.1) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines==2.10.1) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines==2.10.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines==2.10.1) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines==2.10.1) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines==2.10.1) (0.16.0)\n",
            "Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python, atari-py, stable-baselines\n",
            "Successfully installed atari-py-0.2.9 opencv-python-4.8.0.74 stable-baselines-2.10.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting openpyxl==3.1.2\n",
            "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting et-xmlfile (from openpyxl==3.1.2)\n",
            "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt\n",
        "! pip install stable-baselines==2.10.1\n",
        "! pip install openpyxl==3.1.2\n",
        "# a hack to restart runtime\n",
        "import os\n",
        "os._exit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71f30fOmImQE"
      },
      "source": [
        "# **Mounting Google Drive (for saving files fastly)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhtICyv6IunU"
      },
      "outputs": [],
      "source": [
        "# mount your google drive so that experiment logs can be saved\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Parameters**"
      ],
      "metadata": {
        "id": "CNmQOPyhbOvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inorder to run different problem instances, change these parameters accordingly\n",
        "\n",
        "no_aa = 3   # number of affected areas\n",
        "no_materials = 1                 # number of relief items, considered one in all of my experiments\n",
        "number_of_allocation_time_periods = 6    # number of time periods\n",
        "cap_dist = np.array([1])          # amount of relief in local resposnse center in one time period\n",
        "\n",
        "aaa = np.array([2.04 for i in range(0, no_aa * no_materials)])                       # first deprivation parameter used in the deprivation cost function\n",
        "bbb = np.array([0.24 for i in range(0, no_aa * no_materials)])                       # Second deprivation parameter used in the deprivation cost function\n",
        "\n",
        "cost_path = np.array([np.array([200 + 50 * i for i in range(0, no_aa * no_materials)]) for i in range(0, number_of_allocation_time_periods)]).reshape(number_of_allocation_time_periods, no_aa * no_materials)      # unit capacity's accessibilities for each affected area and local response center\n",
        "\n",
        "demand_aa_mat = 1.5 * np.array([np.array([1 for i in range(0, no_aa * no_materials)]) for i in range(0, number_of_allocation_time_periods)]).reshape(number_of_allocation_time_periods, no_aa * no_materials)       # robustified demand generated by an affected area for a relief item in one time period"
      ],
      "metadata": {
        "id": "HH5vEy54Z73V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJkFfpGrm0Qv"
      },
      "source": [
        "# **Markov Decision Process (MDP) Environment for Reinforcement Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLinbrobXU2b"
      },
      "outputs": [],
      "source": [
        "# definintion for te function which calculates total objective functions value given atates and resource allocation decisions in all the time periods (this function will be used for te calculation of the scaling factor fof the reward as well as for the evaluation of the solution from julia language)\n",
        "\n",
        "def eval_non_convex_obj_fn(dep_lvl_state_var_dict, no_time_periods, no_lrc, no_aa, no_m, aaa, bbb, Len,\n",
        "                               cost_path, logistics_cost_weight, absolute_deprivation_cost_weight, cap):\n",
        "\n",
        "  # calculation of deprivation cost\n",
        "  dep_cost_sum = 0                                            # Before calculating the total deprivation cost, the class attribute \"deprivation cost\" is set to \"0\".\n",
        "  for state_dict_temp in dep_lvl_state_var_dict:\n",
        "    for i in range(0, no_aa):                                               # This for loop loops through all the affected areas and calculate the total deprivation cost\n",
        "      for j in range(0, no_m):\n",
        "        if (state_dict_temp[i, j] >= 0):                                                                   # It the state value of the affected area is greater then or equal to \"0\" then the formula below is used.\n",
        "          dep_cost_sum = dep_cost_sum + exp(aaa[i, j]) * (exp(bbb[i, j] * Len) - 1) * (exp(bbb[i, j] * Len) ** state_dict_temp[i, j])                        # This formula is used to calculate the deprivation cost for a particular Affected Area, given a particular state value of that Affected Area.\n",
        "        else:\n",
        "          dep_cost_sum = dep_cost_sum                                                                 # If the state value of the Affected Area is less then \"0\" then the deprivation cost of that affected area will be set to \"0\".\n",
        "\n",
        "  # logistics cost\n",
        "  log_cost = 0\n",
        "  for j in range(0, no_aa):\n",
        "    for k in range(0, no_m):\n",
        "      for l in range(0, no_lrc):\n",
        "        log_cost = log_cost + cost_path * cap\n",
        "\n",
        "  return absolute_deprivation_cost_weight * dep_cost_sum + logistics_cost_weight * log_cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdVTtCfi6Eb3"
      },
      "outputs": [],
      "source": [
        "import gym                                                                        # This library is used to make the custom RL environment. It is the framework for making custom RL environment.\n",
        "import numpy as np\n",
        "from math import floor\n",
        "from math import exp                                                              # Exponential function from the \"math\"library is used to calculate the exponential in the non-linear equations for calculating the deprivation cost and terminal panelty cost.\n",
        "import itertools as it\n",
        "import scipy.stats                                                      # This library is used to generate the demand values using the truncated normal distrubution for a particular episode.\n",
        "import copy\n",
        "\n",
        "is_loc_dec = False\\\n",
        "\n",
        "no_dcl_loc_aval = 1\n",
        "\n",
        "cap_consum_mat = np.array([1]).reshape(no_materials, no_dcl_loc_aval)                   # capacity consumption by a particular relief item in a particular local response center\n",
        "Len = 4       # length (in terms of hours) of one time period\n",
        "planning_horizon = Len * number_of_allocation_time_periods\n",
        "gamma = 0.99999       # discount factor\n",
        "\n",
        "if is_loc_dec:\n",
        "  erect_cost = np.array([0])                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "  removal_cost = np.array([0])                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "  # a way to simplify the model and avoid unecessary complications (non-convexities)\n",
        "  fixed_cost = (erect_cost + removal_cost) / 2        # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "\n",
        "episode_counter = 0\n",
        "\n",
        "scaling_reward_arr = np.sum(demand_aa_mat, axis = 0).reshape(1, -1)\n",
        "reward_scaling = eval_non_convex_obj_fn(scaling_reward_arr.reshape(1, no_aa, no_materials), number_of_allocation_time_periods, no_dcl_loc_aval, no_aa, no_materials, aaa.reshape(no_aa, no_materials), bbb.reshape(no_aa, no_materials), Len,\n",
        "                                                                    np.max(cost_path), 1 / 3, 1 / 3, np.max(cap_dist) / np.max(cap_consum_mat))\n",
        "\n",
        "# the following variables are used to store states and action from trained jpolicy during evaluation\n",
        "# for storing states during evaluation simulation\n",
        "if is_loc_dec:\n",
        "  state_store = np.zeros((number_of_allocation_time_periods + 1, no_aa * no_materials + no_dcl_loc_aval))\n",
        "else:\n",
        "  state_store = np.zeros((number_of_allocation_time_periods + 1, no_aa * no_materials))\n",
        "\n",
        "# for storing action during evaluation simulation\n",
        "if is_loc_dec:\n",
        "  act_store = np.zeros((number_of_allocation_time_periods, no_aa * no_materials * no_dcl_loc_aval + no_dcl_loc_aval))\n",
        "else:\n",
        "  act_store = np.zeros((number_of_allocation_time_periods, no_aa * no_materials * no_dcl_loc_aval))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV2VwhOdmv9q"
      },
      "outputs": [],
      "source": [
        "class Envir(gym.Env):\n",
        "  def __init__(self, accessibality_based_delivery_cost_weight = 1 / 2, deprivation_cost_weight = 1 / 2, reward_scaling = 10 ** 10,\n",
        "               reallocate_t = 5, no_time_dim_state = 2, no_loc_dim_state = 2, is_eval = False, is_loc_dec = True):\n",
        "\n",
        "        self.is_eval = is_eval\n",
        "        self.is_loc_dec = is_loc_dec                 # it is a boolean variable which is \"True\" if location allocation decsions are included otherwise it will be \"False\"\n",
        "\n",
        "        self.accessibality_based_delivery_cost_weight = accessibality_based_delivery_cost_weight\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.fixed_cost_of_dist_centers_weight = accessibality_based_delivery_cost_weight\n",
        "\n",
        "        self.deprivation_cost_weight = deprivation_cost_weight\n",
        "        self.terminal_penality_cost_weight = deprivation_cost_weight\n",
        "        self.reward_scaling = reward_scaling\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.reallocate_t = reallocate_t                               # number of time periods after which location realloction decision for local response centers needs to be taken\n",
        "\n",
        "        self.no_time_dim_state = no_time_dim_state\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.no_loc_dim_state = no_loc_dim_state\n",
        "\n",
        "        global no_aa\n",
        "        global no_materials\n",
        "        global number_of_allocation_time_periods\n",
        "        global no_dcl_loc_aval\n",
        "        global cap_dist\n",
        "        global cap_consum_mat\n",
        "        global Len\n",
        "        global planning_horizon\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          global erect_cost\n",
        "          global removal_cost\n",
        "          # a way to simplify the model and avoid unecessary complications (non-convexities)\n",
        "          global fixed_cost\n",
        "\n",
        "        global aaa\n",
        "        global bbb\n",
        "        global cost_path\n",
        "\n",
        "        global demand_aa_mat\n",
        "\n",
        "        self.no_aa = no_aa                                                           # Number of Affected Areas (AAs), to be entered for every instance of the problem.\n",
        "        self.no_dcl_loc_aval = no_dcl_loc_aval                                                          # number of destrubution centers (DCs) or local response centers, to be entered for every instance of the problem.\n",
        "        self.no_materials = no_materials\n",
        "        self.number_of_allocation_time_periods = number_of_allocation_time_periods                                # The number of discrete time periods into which the whole planning time period is to be divided.\n",
        "        self.steps_per_episode = self.number_of_allocation_time_periods           # Steps per Episodes. (same as above)\n",
        "        self.cap_dist = cap_dist                            # Capacities of the Distribution Centers or local response center (Simulated by an RL agent). The size of this array is equal to \"self.no_materials\" times \"self.no_dcl\".\n",
        "        self.cap_consum_mat = cap_consum_mat                   # capacity consumption by a particular relief item in a particular local response center\n",
        "        self.Len = Len       # length (in terms of hours) of one time period\n",
        "        self.planning_horizon = planning_horizon\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.erect_cost =  erect_cost                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "          self.removal_cost = removal_cost                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "          # a way to simplify the model and avoid unecessary complications (non-convexities)\n",
        "          self.fixed_cost = fixed_cost        # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "\n",
        "        self.episode_counter = 0\n",
        "        self.aaa = aaa                       # The shape is of the form self.no_aa by self.no_materials.                        # In the numpy array the cofficient values for different materials for the same affected area are grouped together. The size of this array is equal to \"self.no_materials\" times \"self.no_dcl_loc_aval\".                                       # A constant used in calculating reward for the Agent (Depreivation cost and Terminal Penality cost).\n",
        "        self.bbb = bbb                       # The shape is of the form self.no_aa by self.no_materials.                        # In the numpy array the cofficient values for different materials for the same affected area are grouped together. The size of this array is equal to \"self.no_materials\" times \"self.no_dcl_loc_aval\".                                      # A constant used in calculating reward for the Agent (Depreivation cost and Terminal Penality cost).\n",
        "        # 200, 250, 300, 350, 400, 450, 500, 550, 600, 650\n",
        "        self.cost_path = cost_path\n",
        "\n",
        "        self.demand_aa_mat = demand_aa_mat                 # In each time period, there is demand associated with each affected area and material. For each time period, the demand is arranged by affected areas and material.\n",
        "\n",
        "        self.state_temp = np.array([-1 for i in range(0, self.steps_per_episode)])\n",
        "\n",
        "        # all or nothing disruption probabilities for potential LRC location\n",
        "        if self.is_loc_dec:\n",
        "          self.disrup_prob = np.array([[0, 1]])         # this must be two dimensional array with size equal to number of local response centers time \"2\" (\"2\" because there can be only two possible scenarios for an LRC, either it is disrupted or it is not disrupted)\n",
        "          self.loc_disrup_temp = np.zeros((self.no_dcl_loc_aval,))          # this array will be used in disruption calculations\n",
        "\n",
        "        # enumeration of location decisions for all local response centers or distribution centers\n",
        "        if self.is_loc_dec:\n",
        "          self.loc_dcl = []\n",
        "          a = 0\n",
        "          while a < self.no_dcl_loc_aval:\n",
        "            self.loc_dcl.append([0, 1])\n",
        "            a = a + 1\n",
        "          self.loc_dcl = np.array(self.loc_dcl)\n",
        "          self.loc_dcl_actions = np.array(np.meshgrid(*list(self.loc_dcl))).T.reshape(-1, self.no_dcl_loc_aval)      # len([0, 1]) = 2                     # All the possible actions related to loction are listed in \"self.loc_dcl_actions\".\n",
        "          del self.loc_dcl                                     # Deleting the 'self.loc_dcl' array as it is no longer required, to free up memory. (Make the code memory efficient)\n",
        "          del a\n",
        "\n",
        "          self.dc_loc_alr_const = np.array([1])                             # The length of the array is equal to the number of distrubution centers, \"self.no_dcl_loc_aval\".                   # The array representing the agent's decision for establishing distribution center at a particular avalilable location (to be taken from agent's action array).\n",
        "\n",
        "        # enumeration of resource allocation decision from relief material allocation from local response centers to affected areas\n",
        "        self.counter = 0                                                                  # Its value will increase until all the combinations of the affected areas and distrubuiton centers are exhausted.\n",
        "\n",
        "        self.res_alocat_dc = []\n",
        "        m = 0\n",
        "        while m < self.no_aa:                                             # These nested while loops have the task to generate the numpy array list of all possible allocation aciton, which the agent can do. In it, there may be some actions which are infeasible.\n",
        "          i = 0\n",
        "          while i < self.no_materials:\n",
        "            j = 0\n",
        "            while j < self.no_dcl_loc_aval:\n",
        "              k = 0\n",
        "              self.res_alocat_dc.append([])\n",
        "              while k <= self.cap_dist[j] / self.cap_consum_mat[i, j]:\n",
        "                self.res_alocat_dc[self.counter].append(k)\n",
        "                k = k + 1\n",
        "              self.counter = self.counter + 1\n",
        "              j = j + 1\n",
        "            i = i + 1\n",
        "          m = m + 1\n",
        "        self.res_alocat_dc = np.array(self.res_alocat_dc)\n",
        "        self.alot_dcl_actions = np.array(np.meshgrid(*self.res_alocat_dc)).T.reshape(-1, self.no_dcl_loc_aval * self.no_aa * self.no_materials)                           # This statement creates the real numpy array list of all the possible resource allocation decisions. \"self.alot_dcl_actions\" contains all the possible actions related to resource allocation. The arrangement in \"self.alot_dcl_actions\" is self.no_materials by self.no_dcl_loc_aval.\n",
        "\n",
        "        # determination of action validaty (are all constraints satisfied or not?) and the elimination of invalid actions\n",
        "        self.del_el = []                  # array elements to be deleated\n",
        "        # if the allocation capacity of any distribution center location is exceeded by the total allocation made from that distribution ceneter location then the action is eliminated in the following code.\n",
        "        for j in range(0, self.alot_dcl_actions.shape[0]):          # Action shaping implementation. (eliminating some obvious infeasible actions) Ref: A. Kanervisto, C. Scheller and V. Hautamäki, \"Action Space Shaping in Deep Reinforcement Learning,\" 2020 IEEE Conference on Games (CoG), 2020, pp. 479-486, doi: 10.1109/CoG47356.2020.9231687.\n",
        "          self.compare_for_action_shaping = np.sum(np.multiply(np.reshape(np.sum(np.reshape(self.alot_dcl_actions[j], (self.no_aa, self.no_materials * self.no_dcl_loc_aval)), axis = 0), (self.no_materials, self.no_dcl_loc_aval)), self.cap_consum_mat), axis = 0)               # As capacity constraint is the hard constraint so we use action shaping to eliminate the actions which violate this constraint to ensure that the capacity constraints must be satisfied, otherwise it makes no sense that the allocation exceeds the capacity of distribution centers.\n",
        "          for k in range(0, self.compare_for_action_shaping.shape[0]):\n",
        "            if self.compare_for_action_shaping[k] > self.cap_dist[k]:\n",
        "              self.del_el.append(j)\n",
        "              break\n",
        "\n",
        "        self.alot_dcl_actions = np.delete(self.alot_dcl_actions, self.del_el, axis = 0)\n",
        "\n",
        "        del self.counter\n",
        "        del m\n",
        "        del i\n",
        "        del j\n",
        "        del k\n",
        "        del self.res_alocat_dc                                                          # Deleting the variables array as it is no longer required, to free up memory. (Make the code memory efficient)\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          i = 0\n",
        "          while i < 2 ** self.no_dcl_loc_aval:                                          # Number of possible location allocation decisions are equal to 2 raise to power the total number of distrubution centers.\n",
        "            self.temp_arr_loc = np.repeat(self.loc_dcl_actions[i].reshape(1, -1), self.alot_dcl_actions.shape[0], axis = 0)\n",
        "            self.alot_loc_dcl_actions_full = np.concatenate((self.temp_arr_loc, np.copy(self.alot_dcl_actions)), axis = 1)\n",
        "            if i == 0:\n",
        "              self.alot_loc_dcl_actions_full_array = self.alot_loc_dcl_actions_full\n",
        "            else:\n",
        "              self.alot_loc_dcl_actions_full_array = np.concatenate((self.alot_loc_dcl_actions_full_array, self.alot_loc_dcl_actions_full), axis = 0)                              # \"self.alot_loc_dcl_actions_full_array\" numpy array contains all the possible action (location and resoruce allocation actions combined)\n",
        "            i = i + 1\n",
        "\n",
        "          del self.alot_loc_dcl_actions_full\n",
        "\n",
        "          # filteration based on location allocation and reallocation deicsions\n",
        "          # self.del_el_loc_constr = []                  # array elements to be deleated\n",
        "          # i = 0\n",
        "          # # if any allocation is made through the distribution center location such that this distribution center location is not used in a particular action then these actions are eliminated (deleted).\n",
        "          # while i < self.alot_loc_dcl_actions_full_array.shape[0]:                                              # Action shaping implementation. (eliminating some obvious infeasible actions) Ref: A. Kanervisto, C. Scheller and V. Hautamäki, \"Action Space Shaping in Deep Reinforcement Learning,\" 2020 IEEE Conference on Games (CoG), 2020, pp. 479-486, doi: 10.1109/CoG47356.2020.9231687.\n",
        "          #   self.alot_loc_dcl_actions_full_array_loc = self.alot_loc_dcl_actions_full_array[i, 0 : self.no_dcl_loc_aval]\n",
        "          #   self.alot_loc_dcl_actions_full_array_res = self.alot_loc_dcl_actions_full_array[i, self.no_dcl_loc_aval:].reshape(-1, self.no_dcl_loc_aval)\n",
        "          #   self.alot_loc_dcl_actions_full_array_res = np.sum(self.alot_loc_dcl_actions_full_array_res, axis = 0)\n",
        "          #   for j in range(0, self.no_dcl_loc_aval):\n",
        "          #     if self.alot_loc_dcl_actions_full_array_loc[j] == 0 and self.alot_loc_dcl_actions_full_array_res[j] != 0:\n",
        "          #       self.del_el_loc_constr.append(i)\n",
        "          #       break\n",
        "          #   i = i + 1\n",
        "\n",
        "          # self.alot_loc_dcl_actions_full_array = np.delete(self.alot_loc_dcl_actions_full_array, self.del_el_loc_constr, axis = 0)\n",
        "        else:\n",
        "         self.alot_loc_dcl_actions_full_array = self.alot_dcl_actions   # if location allocation and relocation decisions are not included, then all the possible actions are only valid resource allocation actions\n",
        "\n",
        "        self.actions_loc_alloct_dist = self.alot_loc_dcl_actions_full_array\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(self.alot_loc_dcl_actions_full_array.shape[0])                             # The action space of this custom environment, \"Discrete\" is used. As it the optimization problem which we are solving is combinatorial in nature.\n",
        "        if self.is_loc_dec:\n",
        "          self.observation_space = gym.spaces.Box(np.array([-np.inf for i in range(0, self.no_aa * self.no_materials + self.no_loc_dim_state * self.no_dcl_loc_aval + self.no_time_dim_state * (self.steps_per_episode))]), np.array([np.inf for i in range(0, self.no_aa * self.no_materials + self.no_loc_dim_state * self.no_dcl_loc_aval + self.no_time_dim_state * (self.steps_per_episode))]))\n",
        "        else:\n",
        "          self.observation_space = gym.spaces.Box(np.array([-np.inf for i in range(0, self.no_aa * self.no_materials + self.no_time_dim_state * (self.steps_per_episode))]), np.array([np.inf for i in range(0, self.no_aa * self.no_materials + self.no_time_dim_state * (self.steps_per_episode))]))\n",
        "\n",
        "\n",
        "  def step(self, act):                                                            # The agent just gives the index of the action which is taken by the RL agent, as the actions are discrete.\n",
        "\n",
        "        global state_store\n",
        "        global act_store\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.loc_state_temp[self.loc_state_temp == -1] = 0              # this modification is done for the ease of computation (so that we do not have to modify it repeatedly)\n",
        "\n",
        "        self.sstep = self.sstep + 1                                                   # The above statement is there as a counter. It countes the number of steps taken in an episode. (updating of the step counter.)\n",
        "\n",
        "        self.act_raw = self.actions_loc_alloct_dist[act]              # Taking the action vector from the agent and storing it in the \"step\" class attribute \"act\".                                                    # Step counter, which counts the number of steps in an episodes.\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          if ((self.sstep - 1) % self.reallocate_t != 0) and (self.sstep != self.steps_per_episode):    # location allocation and relocation action can only be taken in time periods whose index is divisiable by \"self.reallocate_t\"\n",
        "            self.act_loc = self.act_raw[:self.no_dcl_loc_aval]              # location allocation decision for local response centers (LRCs)\n",
        "\n",
        "          self.act_partial_raw = self.act_raw[self.no_dcl_loc_aval:]           # This statement extracts only the resoruce allocation portion of the action taken by the agent.\n",
        "\n",
        "        else:\n",
        "          self.act_partial_raw = self.act_raw\n",
        "\n",
        "        # the following conditional state will be executed only when the agent cannot make decisions on locations of LRCs. Thus, we must make the resoruce allocation actions feasible with the already made location allocation decisons in the previous time period (modified by disruption)\n",
        "        # making resource allocation action feasible to what LRCs locations are currently operational (for location allocation decisions less time periods)\n",
        "        if self.is_loc_dec:\n",
        "          # modification of \"self.act_partial_raw\"\n",
        "          self.act_partial_raw = np.multiply(np.tile(self.loc_state_temp, self.no_aa * self.no_materials).reshape(self.no_aa * self.no_materials, self.no_dcl_loc_aval),\n",
        "                                             np.reshape(self.act_partial_raw, (self.no_aa * self.no_materials, self.no_dcl_loc_aval))).reshape(-1,)\n",
        "\n",
        "        # the following code will be executed when the trained feedback policy is being evaluated\n",
        "        if self.is_eval:\n",
        "          act_store[self.sstep - 1, :] = self.act_partial_raw\n",
        "          print(\"resource_allocation_decisions: \", self.act_partial_raw)\n",
        "\n",
        "        self.act_partial_raw_reshape = np.reshape(self.act_partial_raw, (self.no_aa, self.no_materials * self.no_dcl_loc_aval))\n",
        "        self.act_partial_raw_reshape_sum = np.sum(self.act_partial_raw_reshape, axis = 0)\n",
        "\n",
        "        # transition function\n",
        "        self.state = self.state - np.sum(np.reshape(self.act_partial_raw, (self.no_aa * self.no_materials, self.no_dcl_loc_aval)), axis = 1) + self.demand_aa_mat[self.sstep - 1]                                         # The computation of the next state vector.\n",
        "\n",
        "        if self.is_eval:\n",
        "          state_store[self.sstep, :] = self.state\n",
        "\n",
        "        # local response centers' location update (part of transition function)\n",
        "        if self.is_loc_dec:\n",
        "          if ((self.sstep - 1) % self.reallocate_t == 0) and (self.sstep != self.steps_per_episode):      # there is no any reason to take location allocation decisons if you facilities are only for the response phase and cannot be used subsequently\n",
        "            # the following code will be executed if the the feedback policy is being evaluated\n",
        "            if self.is_eval:\n",
        "              print(\"LRCs'_location_allocation_and_relocation_decisions: \", self.act_loc)\n",
        "\n",
        "            self.loc_state = self.act_loc              # outgoing state variable\n",
        "            print(self.act_loc, \"llllllllllllll\")\n",
        "          else:\n",
        "            self.loc_state = self.loc_state_temp       # outgoing state variable\n",
        "\n",
        "          # self.loc_state = np.array([1 for i in range(0, self.no_dcl_loc_aval)])       # this statement will run only when location allocation decisons are not to be included\n",
        "\n",
        "        self.fixed_cost_of_dist_centers = 0\n",
        "        i = 0\n",
        "        self.act = []\n",
        "        self.accumulate = 0\n",
        "\n",
        "        self.cap_dist_matrix = self.cap_dist.reshape(-1, self.no_dcl_loc_aval)\n",
        "\n",
        "        # fixied cost for establishment of local response centers (LRCs) (installation or deinstallation cost)\n",
        "        if self.is_loc_dec and ((self.sstep - 1) % self.reallocate_t == 0) and (self.sstep != self.steps_per_episode):\n",
        "          self.fixed_cost_of_dist_centers = np.dot(np.abs(self.act_loc - self.loc_state_temp), self.fixed_cost)\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.loc_state_temp = copy.deepcopy(self.loc_state)\n",
        "\n",
        "        # before the system can move two the next time step (and after the complete execution of current time period's decisions) LRCs are exposed to all or nothing disruption\n",
        "        if self.is_loc_dec:\n",
        "          for i in range(0, self.no_dcl_loc_aval):\n",
        "            self.loc_disrup_temp[i] = np.random.choice(np.array([0, 1]), size=(1,), replace=True, p=self.disrup_prob[i])\n",
        "          self.loc_state_temp = np.multiply(self.loc_state_temp, self.loc_disrup_temp).reshape(-1,)\n",
        "\n",
        "          # the following code will be executed if the the feedback policy is being evaluated\n",
        "          if self.is_eval:\n",
        "            print(\"which_potential_LRCs'_locations_have_disruption_occured_on_them_in_the_current_time_period_which_will_impact_subsequent_time_period?: \", self.loc_disrup_temp)\n",
        "\n",
        "        if self.is_loc_dec and self.sstep == self.steps_per_episode:              # at the end of planning horizon, all the established location response centers (LRC) (as they are the temprary strcutures)\n",
        "          self.fixed_cost_of_dist_centers = self.fixed_cost_of_dist_centers + np.dot(self.loc_state_temp, self.fixed_cost)            # at the of planning horizon (episode) the removal cost for all established local response centers (LRCs) must be included in the incurred fixed cost\n",
        "\n",
        "        counter = 0\n",
        "\n",
        "        self.accessibility_based_delivery_cost = np.dot(self.act_partial_raw, self.cost_path[self.sstep - 1])               # \"Accesibility Based Delivery Cost\" is calculated by computing the dot product of allocation action vector, and the path cost vector.\n",
        "\n",
        "        self.dep_cost_sum = 0                                            # Before calculating the total deprivation cost, the class attribute \"deprivation cost\" is set to \"0\".\n",
        "        if self.sstep != self.steps_per_episode:\n",
        "          for s in self.state:                                               # This for loop loops through all the affected areas and calculate the total deprivation cost.\n",
        "            if s >= 0:                                                                   # It the state value of the affected area is greater then or equal to \"0\" then the formula below is used.\n",
        "              self.dep = exp(self.aaa[counter]) * (exp(self.bbb[counter] * self.Len) - 1) * (exp(self.bbb[counter] * self.Len) ** s)                        # This formula is used to calculate the deprivation cost for a particular Affected Area, given a particular state value of that Affected Area.\n",
        "            else:\n",
        "              self.dep = 0                                                                 # If the state value of the Affected Area is less then \"0\" then the deprivation cost of that affected area will be set to \"0\".\n",
        "            if self.sstep == 1:                                                              # If it is the first time step of an episode then there is an additional step is calculating the deprivation cost for the time period.\n",
        "              self.dep = self.dep + np.multiply(np.exp(self.aaa[counter]), (np.exp(self.bbb[counter] * self.Len) - 1))               # This is the additional step is calculating he deprivation cost for the first time period of the episode.\n",
        "\n",
        "            counter = counter + 1\n",
        "\n",
        "            self.dep_cost_sum = self.dep_cost_sum + self.dep\n",
        "\n",
        "\n",
        "        else:                                     # If the time step of and episode is the last time step then there is an additional cost called the \"penelty cost\".\n",
        "          self.terminal_panelty_cost_sum = 0                                                # Before calculating the terminal panelty cost, the class attribute \"terminal_penelty_cost\" is set to zero.\n",
        "          counter = 0\n",
        "          for s in self.state:                                                         # This for loop loop through all the Affected Areas to calculate the terminal penelty cost of each Affected Area and total terminal penelty cost.\n",
        "            if s >= 0:                                                                   # If the state value of the Affected Area is greater then or equal to \"0\" then the formula is the next statement is used to calcualte the terminal penelty cost.\n",
        "              self.term = exp(self.aaa[counter]) * (exp(self.bbb[counter] * self.Len)-1) * (exp(self.bbb[counter] * self.Len) ** s)              # The formula used for calculating the terminal penelty cost for a particular Affected Area.\n",
        "            else:\n",
        "              self.term = 0                                                              # If state value of the affected area is less then \"0\" then the terminal panelty cost is set to zero.\n",
        "\n",
        "            counter = counter + 1\n",
        "\n",
        "            self.terminal_panelty_cost_sum = self.terminal_panelty_cost_sum + self.term\n",
        "\n",
        "        # The following if-elif-else computes the reward which will be delivered to the agent.\n",
        "        if self.is_loc_dec:\n",
        "          if self.sstep != self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.dep_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost - self.accessibality_based_delivery_cost_weight * self.fixed_cost_of_dist_centers                 # If the Markovian time step is the first time step, then the reward is calculated by summing the negative of total deprivations cost and total accessibility based delivery cost.\n",
        "          elif self.sstep == self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.terminal_panelty_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost - self.accessibality_based_delivery_cost_weight * self.fixed_cost_of_dist_centers             # If the Markovian time step is the last time step then the reward for the RL agent is calculated by summing the negative of accessibility based delivery cost and terminal penality cost.\n",
        "\n",
        "          del self.fixed_cost_of_dist_centers\n",
        "\n",
        "        else:\n",
        "          if self.sstep != self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.dep_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost                 # If the Markovian time step is the first time step, then the reward is calculated by summing the negative of total deprivations cost and total accessibility based delivery cost.\n",
        "          elif self.sstep == self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.terminal_panelty_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost             # If the Markovian time step is the last time step then the reward for the RL agent is calculated by summing the negative of accessibility based delivery cost and terminal penality cost.\n",
        "\n",
        "        del self.accessibility_based_delivery_cost\n",
        "\n",
        "        if self.sstep == self.steps_per_episode:\n",
        "          self.done = True                                                                 # If the time period is the last time period of the episode then set the class attribute \"done\" to True.\n",
        "        else:\n",
        "          self.done = False                                                                # If the time period is not the last time period of the episode then set the class attribute \"done\" to False.\n",
        "        self.info = {}                                                                     # Class attribute \"info\" is an empty python dictionary which is returned to the RL agent at every time step.\n",
        "        self.state_t = copy.deepcopy(self.state_temp)\n",
        "\n",
        "        # the following code will be executed if the the feedback policy is being evaluated\n",
        "        if self.is_eval:\n",
        "          print(\"reward_or_stagewise_objective_function_value: \", self.reward)\n",
        "\n",
        "        # print(self.loc_state, \"self.loc_stateself.loc_stateself.loc_state\")\n",
        "        self.state_t[self.sstep - 1] = 1\n",
        "        # print(self.state_t, \"vself.state_tself.state_t\")\n",
        "        # NewValue = (((OldValue - OldMin) * (NewMax - NewMin)) / (OldMax - OldMin)) + NewMin\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.loc_state_temp[self.loc_state_temp == 0] = -1\n",
        "          return np.concatenate(((((self.state.reshape(self.no_aa * self.no_materials,) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist) / np.min(self.cap_consum_mat)), repeats = self.no_aa * self.no_materials))) / (np.sum(self.demand_aa_mat, axis = 0) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist)), repeats = self.no_aa * self.no_materials)))) * (2 * np.ones((self.no_aa * self.no_materials,))) - 1), np.repeat(self.loc_state_temp, self.no_loc_dim_state), np.repeat(self.state_t, self.no_time_dim_state))),\\\n",
        "                        float(self.reward / self.reward_scaling), self.done, self.info           # The step method will return the state of the time period, the reward, \"done\" attribute (which shows weather the episode has terminated or not) and the empty python dictionary \"info\".\n",
        "        else:\n",
        "          return np.concatenate(((((self.state.reshape(self.no_aa * self.no_materials,) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist) / np.min(self.cap_consum_mat)), repeats = self.no_aa * self.no_materials))) / (np.sum(self.demand_aa_mat, axis = 0) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist)), repeats = self.no_aa * self.no_materials)))) * (2 * np.ones((self.no_aa * self.no_materials,))) - 1), np.repeat(self.state_t, self.no_time_dim_state))),\\\n",
        "                        float(self.reward / self.reward_scaling), self.done, self.info           # The step method will return the state of the time period, the reward, \"done\" attribute (which shows weather the episode has terminated or not) and the empty python dictionary \"info\".\n",
        "\n",
        "\n",
        "  def render(self):                                                               # The \"render\" method is used to render the environment or to visually simulate the RL environment. It is computationally costly to render the environment and also it is not necessary to render this environment. This method is mostly used for debug the environment.\n",
        "        pass                                                                      # Here, \"pass\" keyword is used to just pass the method, this method () is not being used (Environment is not rendered).\n",
        "\n",
        "  def reset(self):                                                                # The \"reset\" method, which is called when a new episode is started. It gives the initial state to the RL agent.\n",
        "\n",
        "        global state_store\n",
        "\n",
        "        self.sstep = 0                                                            # The step counter initialized at zero.\n",
        "        self.state = np.array([0 for i in range(0, self.no_aa * self.no_materials)])                                               # Initial state, which is zero for every affected area and material, Arrangement is affected areas and then number of materials.\n",
        "        if self.is_eval:\n",
        "          state_store[0, :] = self.state\n",
        "        self.state_t = copy.deepcopy(self.state_temp)\n",
        "        # print(self.state_t, \"vself.state_tself.state_t\")\n",
        "        if self.is_loc_dec:\n",
        "\n",
        "          self.loc_state_temp = np.array([-1 for i in range(0, self.no_dcl_loc_aval)])        # assuming no local response centers are pre-eracted\n",
        "          print(self.loc_state_temp, \"self.loc_stateself.loc_stateself.loc_state\")\n",
        "          # self.loc_state_temp = np.array([1 for i in range(0, self.no_dcl_loc_aval)])        # if location allocation decisions are not to be included the this state will run\n",
        "\n",
        "          return np.concatenate(((((self.state.reshape(self.no_aa * self.no_materials,) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist)), repeats = self.no_aa * self.no_materials))) / (np.sum(self.demand_aa_mat, axis = 0) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist) / np.min(self.cap_consum_mat)), repeats = self.no_aa * self.no_materials)))) * (2 * np.ones((self.no_aa * self.no_materials,))) - 1), np.repeat(self.loc_state_temp, self.no_loc_dim_state), np.repeat(self.state_t, self.no_time_dim_state)))\n",
        "\n",
        "        else:\n",
        "\n",
        "          return np.concatenate(((((self.state.reshape(self.no_aa * self.no_materials,) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist)), repeats = self.no_aa * self.no_materials))) / (np.sum(self.demand_aa_mat, axis = 0) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist) / np.min(self.cap_consum_mat)), repeats = self.no_aa * self.no_materials)))) * (2 * np.ones((self.no_aa * self.no_materials,))) - 1), np.repeat(self.state_t, self.no_time_dim_state)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmPPanf1S7AY"
      },
      "source": [
        "# **Definition of all custom Neural Network layers in TensorFlow & Keras**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1GoGv4tGhxL"
      },
      "outputs": [],
      "source": [
        "# env = Envir()                 # creating object of the envirnment class which is in the python file where the envirnment is defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC9qVixkGhxM"
      },
      "outputs": [],
      "source": [
        "# ! rm -r /content/ppo_humanitarian_logistics_without_tensorboard                                  # For the deletion of the folder in which all the tensorboard logs are stored after each hyperparameter optimization run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3M9FI6gWTdt"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard.notebook\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "altemRFbxPUO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "# from tensorflow.keras.engine import *\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.python.keras import constraints\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
        "\n",
        "\n",
        "class DenseSN(Dense):\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        input_dim = input_shape[-1]\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
        "                                      initializer=initializers.RandomNormal(0, 1),\n",
        "                                      name='kernel',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint, trainable=True)\n",
        "        self.bias = self.add_weight(shape=(self.units,),\n",
        "                                      initializer=initializers.RandomNormal(0, 1),\n",
        "                                      name='bias',\n",
        "                                      regularizer=self.bias_regularizer,\n",
        "                                      constraint=self.bias_constraint, trainable=True)\n",
        "\n",
        "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                                 initializer=initializers.RandomNormal(0, 1),\n",
        "                                 name='sn',\n",
        "                                 trainable=False)\n",
        "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
        "        # self.input_spec = InputSpec(min_ndim=2)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        def _l2normalize(v, eps=1e-12):\n",
        "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "        def power_iteration(W, u):\n",
        "            _u = u\n",
        "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
        "            _u = _l2normalize(K.dot(_v, W))\n",
        "            return _u, _v\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        #Flatten the Tensor\n",
        "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
        "        _u, _v = power_iteration(W_reshaped, self.u)\n",
        "        #Calculate Sigma\n",
        "        sigma=K.dot(_v, W_reshaped)\n",
        "        sigma=K.dot(sigma, K.transpose(_u))\n",
        "        #normalize it\n",
        "        W_bar = W_reshaped / sigma\n",
        "        #reshape weight tensor\n",
        "        if training in {0, False}:\n",
        "            W_bar = K.reshape(W_bar, W_shape)\n",
        "        else:\n",
        "            with tf.control_dependencies([self.u.assign(_u)]):\n",
        "                 W_bar = K.reshape(W_bar, W_shape)\n",
        "        output = K.dot(inputs, W_bar)\n",
        "\n",
        "        output = K.bias_add(output, self.bias, data_format='channels_last')\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfO9UZ4HvVMK"
      },
      "outputs": [],
      "source": [
        "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================\n",
        "\n",
        "# The following implementation is done so as to use the spectral normalization for weight matrix for the affine layers.\n",
        "\n",
        "\n",
        "\"\"\"Contains the core layers: Dense, Dropout.\n",
        "\n",
        "Also contains their functional aliases.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "from tensorflow.python.layers import base\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.python.util import deprecation\n",
        "from tensorflow.python.util.tf_export import tf_export\n",
        "\n",
        "\n",
        "class SNDense(DenseSN, base.Layer):\n",
        "  \"\"\"Densely-connected layer class.\n",
        "\n",
        "  This layer implements the operation:\n",
        "  `outputs = activation(inputs * kernel + bias)`\n",
        "  Where `activation` is the activation function passed as the `activation`\n",
        "  argument (if not `None`), `kernel` is a weights matrix created by the layer,\n",
        "  and `bias` is a bias vector created by the layer\n",
        "  (only if `use_bias` is `True`).\n",
        "\n",
        "  Arguments:\n",
        "    units: Integer or Long, dimensionality of the output space.\n",
        "    activation: Activation function (callable). Set it to None to maintain a\n",
        "      linear activation.\n",
        "    use_bias: Boolean, whether the layer uses a bias.\n",
        "    kernel_initializer: Initializer function for the weight matrix.\n",
        "      If `None` (default), weights are initialized using the default\n",
        "      initializer used by `tf.compat.v1.get_variable`.\n",
        "    bias_initializer: Initializer function for the bias.\n",
        "    kernel_regularizer: Regularizer function for the weight matrix.\n",
        "    bias_regularizer: Regularizer function for the bias.\n",
        "    activity_regularizer: Regularizer function for the output.\n",
        "    kernel_constraint: An optional projection function to be applied to the\n",
        "        kernel after being updated by an `Optimizer` (e.g. used to implement\n",
        "        norm constraints or value constraints for layer weights). The function\n",
        "        must take as input the unprojected variable and must return the\n",
        "        projected variable (which must have the same shape). Constraints are\n",
        "        not safe to use when doing asynchronous distributed training.\n",
        "    bias_constraint: An optional projection function to be applied to the\n",
        "        bias after being updated by an `Optimizer`.\n",
        "    trainable: Boolean, if `True` also add variables to the graph collection\n",
        "      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
        "    name: String, the name of the layer. Layers with the same name will\n",
        "      share weights, but to avoid mistakes we require reuse=True in such cases.\n",
        "    _reuse: Boolean, whether to reuse the weights of a previous layer\n",
        "      by the same name.\n",
        "\n",
        "  Properties:\n",
        "    units: Python integer, dimensionality of the output space.\n",
        "    activation: Activation function (callable).\n",
        "    use_bias: Boolean, whether the layer uses a bias.\n",
        "    kernel_initializer: Initializer instance (or name) for the kernel matrix.\n",
        "    bias_initializer: Initializer instance (or name) for the bias.\n",
        "    kernel_regularizer: Regularizer instance for the kernel matrix (callable)\n",
        "    bias_regularizer: Regularizer instance for the bias (callable).\n",
        "    activity_regularizer: Regularizer instance for the output (callable)\n",
        "    kernel_constraint: Constraint function for the kernel matrix.\n",
        "    bias_constraint: Constraint function for the bias.\n",
        "    kernel: Weight matrix (TensorFlow variable or tensor).\n",
        "    bias: Bias vector, if applicable (TensorFlow variable or tensor).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, units,\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer=None,\n",
        "               bias_initializer=init_ops.random_normal_initializer(),\n",
        "               kernel_regularizer=init_ops.random_normal_initializer(),\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               trainable=True,\n",
        "               name=None,\n",
        "               **kwargs):\n",
        "    super(SNDense, self).__init__(units=units,\n",
        "                                activation=activation,\n",
        "                                use_bias=use_bias,\n",
        "                                kernel_initializer=kernel_initializer,\n",
        "                                bias_initializer=bias_initializer,\n",
        "                                kernel_regularizer=kernel_regularizer,\n",
        "                                bias_regularizer=bias_regularizer,\n",
        "                                activity_regularizer=activity_regularizer,\n",
        "                                kernel_constraint=kernel_constraint,\n",
        "                                bias_constraint=bias_constraint,\n",
        "                                trainable=trainable,\n",
        "                                name=name,\n",
        "                                **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def SNdense(\n",
        "    inputs, scope, units,\n",
        "    activation=None,\n",
        "    use_bias=True,\n",
        "    kernel_initializer=init_ops.random_normal_initializer(),\n",
        "    bias_initializer=init_ops.random_normal_initializer(),\n",
        "    kernel_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    trainable=True,\n",
        "    name=None,\n",
        "    reuse=None):\n",
        "  \"\"\"Functional interface for the densely-connected layer.\n",
        "\n",
        "  This layer implements the operation:\n",
        "  `outputs = activation(inputs * kernel + bias)`\n",
        "  where `activation` is the activation function passed as the `activation`\n",
        "  argument (if not `None`), `kernel` is a weights matrix created by the layer,\n",
        "  and `bias` is a bias vector created by the layer\n",
        "  (only if `use_bias` is `True`).\n",
        "\n",
        "  Arguments:\n",
        "    inputs: Tensor input.\n",
        "    units: Integer or Long, dimensionality of the output space.\n",
        "    activation: Activation function (callable). Set it to None to maintain a\n",
        "      linear activation.\n",
        "    use_bias: Boolean, whether the layer uses a bias.\n",
        "    kernel_initializer: Initializer function for the weight matrix.\n",
        "      If `None` (default), weights are initialized using the default\n",
        "      initializer used by `tf.compat.v1.get_variable`.\n",
        "    bias_initializer: Initializer function for the bias.\n",
        "    kernel_regularizer: Regularizer function for the weight matrix.\n",
        "    bias_regularizer: Regularizer function for the bias.\n",
        "    activity_regularizer: Regularizer function for the output.\n",
        "    kernel_constraint: An optional projection function to be applied to the\n",
        "        kernel after being updated by an `Optimizer` (e.g. used to implement\n",
        "        norm constraints or value constraints for layer weights). The function\n",
        "        must take as input the unprojected variable and must return the\n",
        "        projected variable (which must have the same shape). Constraints are\n",
        "        not safe to use when doing asynchronous distributed training.\n",
        "    bias_constraint: An optional projection function to be applied to the\n",
        "        bias after being updated by an `Optimizer`.\n",
        "    trainable: Boolean, if `True` also add variables to the graph collection\n",
        "      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
        "    name: String, the name of the layer.\n",
        "    reuse: Boolean, whether to reuse the weights of a previous layer\n",
        "      by the same name.\n",
        "\n",
        "  Returns:\n",
        "    Output tensor the same shape as `inputs` except the last dimension is of\n",
        "    size `units`.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if eager execution is enabled.\n",
        "  \"\"\"\n",
        "  layer = SNDense(units,\n",
        "                activation=None,\n",
        "                use_bias=True,\n",
        "                kernel_initializer=init_ops.random_normal_initializer(),\n",
        "                bias_initializer=init_ops.random_normal_initializer(),\n",
        "                kernel_regularizer=None,\n",
        "                bias_regularizer=None,\n",
        "                activity_regularizer=None,\n",
        "                kernel_constraint=None,\n",
        "                bias_constraint=None,\n",
        "                trainable=True,\n",
        "                name=None,\n",
        "                _scope=scope,\n",
        "                _reuse=None\n",
        "                )\n",
        "  return layer.apply(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfj7uWGHV1Yw"
      },
      "outputs": [],
      "source": [
        "# to be used for the calculation of q-values\n",
        "def linear(input_tensor, scope, n_hidden, *, init_scale=1.0, init_bias=0.0, reuse = False):\n",
        "    \"\"\"\n",
        "    Creates a fully connected layer for TensorFlow\n",
        "    :param input_tensor: (TensorFlow Tensor) The input tensor for the fully connected layer\n",
        "    :param scope: (str) The TensorFlow variable scope\n",
        "    :param n_hidden: (int) The number of hidden neurons\n",
        "    :param init_scale: (int) The initialization scale\n",
        "    :param init_bias: (int) The initialization offset bias\n",
        "    :return: (TensorFlow Tensor) fully connected layer\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope, reuse = reuse):\n",
        "        n_input = input_tensor.get_shape()[1].value\n",
        "        weight = tf.get_variable(\"w\", [n_input, n_hidden], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=seed))\n",
        "        bias = tf.get_variable(\"b\", [n_hidden], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=seed))\n",
        "        return tf.matmul(input_tensor, weight) + bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z2xA1zxNz4U"
      },
      "outputs": [],
      "source": [
        "# layer normalization implementation in tensorflow\n",
        "def _ln(input_tensor, scope, epsilon=1e-5, axis=None):\n",
        "    \"\"\"\n",
        "    Apply layer normalisation.\n",
        "    :param input_tensor: (TensorFlow Tensor) The input tensor for the Layer normalization\n",
        "    :param gain: (TensorFlow Tensor) The scale tensor for the Layer normalization\n",
        "    :param bias: (TensorFlow Tensor) The bias tensor for the Layer normalization\n",
        "    :param epsilon: (float) The epsilon value for floating point calculations\n",
        "    :param axes: (tuple, list or int) The axes to apply the mean and variance calculation\n",
        "    :return: (TensorFlow Tensor) a normalizing layer\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope):\n",
        "      gain = tf.get_variable(\"gain_layer_normalization\", input_tensor.get_shape()[-1],\n",
        "                                 dtype=input_tensor.dtype, initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=seed, dtype=tf.dtypes.float32))\n",
        "      bias = tf.get_variable(\"bias_layer_normalization\", input_tensor.get_shape()[-1],\n",
        "                                 dtype=input_tensor.dtype, initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=seed, dtype=tf.dtypes.float32))\n",
        "      if axis is None:\n",
        "          axis = [1]\n",
        "      mean, variance = tf.nn.moments(input_tensor, axes=axis, keep_dims=True)\n",
        "      input_tensor = (input_tensor - mean) / tf.sqrt(variance + epsilon)\n",
        "      input_tensor = input_tensor * gain + bias\n",
        "      return input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ykv1mP4EjqU"
      },
      "outputs": [],
      "source": [
        "# implementation of prelu activation function, as in tensorflow version 1.15, perelu is not implemented\n",
        "import tensorflow as tf\n",
        "def prelu(_x, scope=None):\n",
        "    \"\"\"parametric ReLU activation\"\"\"\n",
        "    with tf.variable_scope(name_or_scope = scope, default_name=\"prelu\"):\n",
        "        _alpha = tf.get_variable(\"prelu\", _x.get_shape()[-1],\n",
        "                                 dtype=_x.dtype, initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=seed, dtype=tf.dtypes.float32))\n",
        "        return tf.maximum(0.0, _x) + _alpha * tf.minimum(0.0, _x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2fCrRtQaiCy"
      },
      "outputs": [],
      "source": [
        "# # custom tensorboard scalers for constraint voilation values for each episode\n",
        "# import tensorflow as tf\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "# import os\n",
        "# import time\n",
        "\n",
        "# # now = time.localtime()\n",
        "# # subdir = time.strftime(\"%d-%b-%Y_%H.%M.%S\", now)\n",
        "\n",
        "# # dist = tf.Tensor(tf.float32, [100])\n",
        "\n",
        "# # dist = tf.convert_to_tensor(new_list)\n",
        "\n",
        "# dist = output_policy_output\n",
        "\n",
        "# tf.compat.v1.summary.histogram(name=\"distribution\", values=dist)\n",
        "\n",
        "# summary_dir_constrait_voilation = os.path.join(\"/content/tensorboard_constraint_voilation\")\n",
        "# summary_writer_constrait_voilation = tf.summary.FileWriter(summary_dir_constrait_voilation, sess.graph)\n",
        "# summaries_constraint_voilation = tf.compat.v1.summary.merge_all()\n",
        "\n",
        "# # for cont in range(200):\n",
        "# #     with summary_writer1.as_default():\n",
        "# #         tf.summary.scalar(name=\"unify/sin_x\", data=np.math.sin(cont) ,step=cont)\n",
        "# #         tf.summary.scalar(name=\"unify/sin_x_2\", data=np.math.sin(cont/2), step=cont)\n",
        "# #     summary_writer1.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVBMluMd_Cev"
      },
      "outputs": [],
      "source": [
        "# List of alblation studies; Initially all the new components which are incorporated our artifical intelligence system are used, while doing ablaiton studies, we will remove the components one by one (keeping rest of the components intact) and assess the performance in terms of three matrics, viz, computational time, solution quality and carbon emission involved, for all the reinforcement learning algorithms used.\n",
        "# with the combination of (dense connections or residual connections or not both) and (PPO2 or SAC) the following experiments in the ablation study will be performed\n",
        "## reward saltation, execution time aware hyperparameter optimization, prelu, spectral normalizaetion\n",
        "## reward saltation, without execution time aware hyperparameter optimization, prelu (leaky relu)\n",
        "## without reward saltation, execution time aware hyperparameter optimization, prelu (leaky relu)\n",
        "## reward saltation, execution time aware hyperparameter optimization, without prelu (leaky relu), spectral normalizaetion\n",
        "## reward saltation, execution time aware hyperparameter optimization, prelu, without spectral normalizaetion\n",
        "\n",
        "# Note: inside each ablation study, we are doing hyperparameter optimization over the hperparameters which are listed as follows:\n",
        "## number of layer units in the policy network\n",
        "## number of layers units in the value network\n",
        "## number of epochs\n",
        "## learning rate of the policy network\n",
        "## learning rate of the value network\n",
        "## discount factor\n",
        "## exploration term (for A2C, PPO and SAC)\n",
        "## Number of neurons in the first layer unit (dense layers) of the policy network\n",
        "## Number of neurons in the last layer unit (dense layers) of the policy network\n",
        "## Number of neurons in the first layer unit (dense layers) of the value network\n",
        "## Number of neurons in the last layer unit (dense layers) of the value network\n",
        "## Number of hidden neurons in each of the intermediate layer units' (dense layers) of the policy network\n",
        "## Number of hidden neurons in each of the intermediate layer units' (dense layers) of the value network\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# First ablation study is done and after that a comparative study of best performing DRL algorithm with ant colony and genetic algorithm will be done on large scale optimization problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agq82meQAzZ1"
      },
      "source": [
        "# **Custom Checkpoints Implementation for termination criterion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KFG3wrrytZG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from abc import ABC\n",
        "import warnings\n",
        "import typing\n",
        "from typing import Union, List, Dict, Any, Optional\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.common.vec_env import VecEnv, sync_envs_normalization, DummyVecEnv\n",
        "from stable_baselines.common.evaluation import evaluate_policy\n",
        "from stable_baselines import logger\n",
        "\n",
        "if typing.TYPE_CHECKING:\n",
        "    from stable_baselines.common.base_class import BaseRLModel  # pytype: disable=pyi-error\n",
        "\n",
        "from stable_baselines.common.callbacks import CheckpointCallback\n",
        "\n",
        "envv = Envir(accessibality_based_delivery_cost_weight = 1 / 3, deprivation_cost_weight = 1 / 3,\n",
        "            reward_scaling = 1, reallocate_t = 5, no_time_dim_state = 5, no_loc_dim_state = 5, is_eval = True,\n",
        "             is_loc_dec = False)\n",
        "\n",
        "# for polting the convergence curve\n",
        "ws.append((\"Data for convergence curves: \",))\n",
        "ws.append((\"Wall Time\", \"Step\", \"Value\"))   # for the creation of header in te excel sheet\n",
        "\n",
        "class CheckpointCallback(CheckpointCallback):\n",
        "\n",
        "  def __init__(self, save_freq, save_path, st_time, name_prefix = f'model_ppo_', verbose = 0):\n",
        "          super(CheckpointCallback, self).__init__(save_freq, save_path, name_prefix, verbose)\n",
        "\n",
        "          envv = Envir(accessibality_based_delivery_cost_weight = 1 / 3, deprivation_cost_weight = 1 / 3,\n",
        "          reward_scaling = 1, reallocate_t = 5, no_time_dim_state = 5, no_loc_dim_state = 5,\n",
        "          is_eval = False, is_loc_dec = False)\n",
        "\n",
        "          verbose = 0\n",
        "          self.env = envv\n",
        "          self.np_arr_eval = np.zeros((int(10 ** 5),))\n",
        "          self.save_freq = save_freq\n",
        "          self.initial_steps_episod_stall = 10 ** 6\n",
        "          self.delta_episod_stall = 50\n",
        "          self.n_episod_stall = 200\n",
        "\n",
        "          self.save_path = save_path\n",
        "          self.name_prefix = name_prefix\n",
        "          self.count_eval = 0\n",
        "          self.term_n_steps = 0\n",
        "          self.st_time = st_time\n",
        "\n",
        "\n",
        "  def _on_step(self) -> bool:\n",
        "\n",
        "          global envv\n",
        "          global model_ppo\n",
        "\n",
        "          # for te evaluatoin of the agent after predetermined number of steps\n",
        "          if self.n_calls % (1000 * number_of_allocation_time_periods) == 0:\n",
        "            eval_tuple = stable_baselines.common.evaluation.evaluate_policy(model = model_ppo, env = envv, n_eval_episodes= 1,\n",
        "                                                                          deterministic = True, return_episode_rewards = True)\n",
        "            ws.append((time.time(), self.n_calls, eval_tuple[0][0]))\n",
        "\n",
        "          if (self.n_calls > self.initial_steps_episod_stall) and (self.n_calls % self.save_freq == 0):\n",
        "              path = os.path.join(self.save_path, '{}_{}_steps'.format(self.name_prefix, self.num_timesteps))\n",
        "              # self.model.save(path)\n",
        "              self.np_arr_eval[int(self.count_eval)] = evaluate_policy(model = self.model, env = self.env, n_eval_episodes= 1,\n",
        "                              deterministic = True, return_episode_rewards = True)[0][0]\n",
        "\n",
        "              if (self.count_eval >= self.n_episod_stall):\n",
        "                y = np.sort(np.abs(self.np_arr_eval[int(self.count_eval - self.n_episod_stall) : int(self.count_eval)] - self.np_arr_eval[int(self.count_eval)]))[0 : self.n_episod_stall - 100]\n",
        "                if ((time.time() - self.st_time) >= 18000) or (np.max(y) <= self.delta_episod_stall):\n",
        "                  self.term_n_steps = self.n_calls\n",
        "                  return False\n",
        "              self.count_eval = self.count_eval + 1\n",
        "              if self.verbose > 1:\n",
        "                  print(\"Saving model checkpoint to {}\".format(path))\n",
        "          return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wwLiKo1TJfK"
      },
      "source": [
        "# **Training of PPO agent (on-policy) (discrete action space version)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Slv95IRzPtwJ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "maxSize = sys.float_info.max\n",
        "minSize = sys.float_info.min\n",
        "criteria_obj_hyperparameter_previous = sys.float_info.max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNpKKbLszWx3"
      },
      "outputs": [],
      "source": [
        "# While training, first modify the \"adam.py\" file at \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/adam.py\" by changing the implementation of \"adam\" optimizer so that it will execute \"amsgrad\" optimizer, and then also modify the file \"optimizers.py\" at \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizers.py\" by changing the variable \"amsgrad\" to \"True\" in the \"__init__\" method of \"adam\" optimizer.\n",
        "\n",
        "# although we are importing optuna (a hyperparameter optimization library), we are not using it for hyperparameter optimization or tuning\n",
        "\n",
        "# PPO MLP agent (Without quantization)\n",
        "import tensorflow as tf\n",
        "\n",
        "from stable_baselines import A2C\n",
        "# from stable_baselines import TRPO\n",
        "from stable_baselines import PPO2\n",
        "\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.common import make_vec_env\n",
        "\n",
        "from stable_baselines.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.common.policies import ActorCriticPolicy\n",
        "from stable_baselines.common.input import observation_input\n",
        "# from stable_baselines.common.callbacks import CheckpointCallback\n",
        "\n",
        "import time\n",
        "\n",
        "# env = make_vec_env.VecEnv(env, n_envs=4)\n",
        "# env = DummyVecEnv([lambda: env])\n",
        "\n",
        "previous_reward = -maxSize                 # Maximum size of the integer possible is used as the starting value for \"previous_reward\", so as to decide, wether to save the model or not.\n",
        "\n",
        "tiral_counter_saving_model = 0                  # Trial counter for determining weather a trial is the first trial or not for saving the model\n",
        "st = time.time()\n",
        "\n",
        "def objective(trial, fast_check=True, target_meter=0, return_info=False) -> float:\n",
        "\n",
        "  # Only one of \"dense_connections\", \"residual_connections\" and \"not_dense_connections_nor_residual_connections\" will be true\n",
        "  dense_connections = False\n",
        "  residual_connections = True\n",
        "  not_dense_connections_nor_residual_connections = False\n",
        "\n",
        "\n",
        "  # Only one of \"use_attention_layer\", \"use_transformer_layer\" and \"not_attention_layer_nor_transformer_layer\" will be true\n",
        "  use_attention_layer = False\n",
        "  use_transformer_layer = False\n",
        "  not_attention_layer_nor_transformer_layer = True\n",
        "\n",
        "\n",
        "  # if previous decison is to use attention layer, then only one of the following must be true\n",
        "  use_attention_Luong_style =  False                                       # for dot-product and scaled dot-product attention layer (we will do hyperparameter tuning on weather to do scaling or not)\n",
        "  use_attention_Bahdanau_style = False                                # for additive attention layer (we will do hyperparameter tuning on weather to do scaling or not)\n",
        "\n",
        "  global previous_reward\n",
        "  global tiral_counter_saving_model\n",
        "\n",
        "  # Int parameters\n",
        "  noptepochs = trial.suggest_int('noptepochs', 3, 3)            # number of epochs to use are to be optimized\n",
        "  number_layers_policy_net = trial.suggest_int('number_layers_policy_net', 4, 4)           # total nubmer of layers to be used in the policy deep neural network\n",
        "  number_layers_value_net = trial.suggest_int('number_layers_value_net', 5, 5)         # total nubmer of layers to be used in the value function deep neural network\n",
        "\n",
        "  # Uniform parameter\n",
        "  learning_rate = trial.suggest_uniform('learning_rate', 0.0002, 0.0003)               # learning rate to be used for both policy and value function deep neural network\n",
        "  gamma = trial.suggest_uniform('gamma', 1, 1)            # discound factor\n",
        "\n",
        "  global no_aa\n",
        "  global no_materials\n",
        "  global no_dcl_loc_aval\n",
        "  global number_of_allocation_time_periods\n",
        "  global reward_scaling\n",
        "\n",
        "  input_shape = no_aa * no_materials + 5                           # The shape of the observation at a single time step. It is also used to determine the reward scaling factor.\n",
        "\n",
        "  # here we are setting only the first and the last layers' number of neurons as the number of neurons in the hidden layers are set during the definition of the hidden layers\n",
        "  n_hiddens_first_policy = trial.suggest_int(\"n_hiddens_first_policy\", input_shape + 2, input_shape + 2)\n",
        "  n_hiddens_first_value = trial.suggest_int(\"n_hiddens_first_value\", input_shape + 2, input_shape + 2)\n",
        "\n",
        "  n_hiddens_second_last_policy = trial.suggest_int(\"n_hiddens_second_last_policy\", input_shape + 2, input_shape + 2)\n",
        "  n_hiddens_last_value = 1\n",
        "\n",
        "\n",
        "  class KerasPolicy(ActorCriticPolicy):\n",
        "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **kwargs):\n",
        "        super(KerasPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse, scale=False)\n",
        "\n",
        "        n_hiddens_last_policy = ac_space.n\n",
        "\n",
        "        obs_ph, processed_obs = observation_input(ob_space, n_batch, name='Ob', scale=False)               # As the observation space is box, so input is scaled between 0 and 1.\n",
        "\n",
        "        self._obs_ph = obs_ph\n",
        "\n",
        "        with tf.variable_scope(\"model\", reuse=reuse):\n",
        "            extracted_features = tf.layers.flatten(processed_obs)\n",
        "            pi_h = extracted_features\n",
        "\n",
        "            # definition of attention mechinism\n",
        "            if use_attention_Luong_style and use_attention_layer:\n",
        "              attention_input_stack_policy = tf.expand_dims(pi_h, 1)\n",
        "              output_policy_attention_1_attention_policy = tf.keras.layers.Attention(use_scale = False)([attention_input_stack_policy, attention_input_stack_policy, attention_input_stack_policy])              # for Luong style attention\n",
        "              output_policy_attention_1 = tf.squeeze(output_policy_attention_1_attention_policy, [1])\n",
        "            elif use_attention_Bahdanau_style and use_attention_layer:\n",
        "              attention_input_stack_policy = tf.expand_dims(pi_h, 1)\n",
        "              output_policy_attention_1_attention_policy = tf.keras.layers.AdditiveAttention(use_scale = False)([attention_input_stack_policy, attention_input_stack_policy, attention_input_stack_policy])              # for Luong style attention\n",
        "              output_policy_attention_1 = tf.squeeze(output_policy_attention_1_attention_policy, [1])\n",
        "            else:\n",
        "              output_policy_attention_1 = pi_h\n",
        "\n",
        "            # reshaping a tensor to be a 2-D tensor\n",
        "\n",
        "            # the start of the definition of the first layer\n",
        "\n",
        "            output_policy_layer_norm_1 = _ln(output_policy_attention_1, \"output_policy_layer_norm_1\")\n",
        "            output_policy_dense_1 = linear(output_policy_layer_norm_1, \"output_policy_dense_1\", n_hiddens_first_policy)\n",
        "            output_policy_prelu_1 = prelu(output_policy_dense_1)\n",
        "            output_policy_dense_1_2 = linear(output_policy_prelu_1, \"output_policy_dense_1_2\", n_hiddens_first_policy)\n",
        "            # the end of the definition of the first layer\n",
        "            if dense_connections:\n",
        "              output_policy_concatenate_1 = tf.concat([output_policy_dense_1_2, pi_h], axis = 1)                  # contatination is done so as to implement dense connections\n",
        "            elif residual_connections:\n",
        "              output_policy_concatenate_1 = tf.add(output_policy_dense_1_2, linear(output_policy_attention_1, \"output_policy_concatenate_1\", n_hiddens_first_policy))                  # contatination is done so as to implement dense connections\n",
        "            elif not_dense_connections_nor_residual_connections:\n",
        "              output_policy_concatenate_1 = output_policy_dense_1_2\n",
        "            # the following \"for\" loop will define the hidden layers\n",
        "            layers_policy = []\n",
        "            for i in range(2, number_layers_policy_net + 1):\n",
        "              layers_policy.append(trial.suggest_int(str(i) + \"_hidden_layer_policy_function_number_of_neurons\", n_hiddens_last_policy, n_hiddens_last_policy))           # number of neurons in the hidden layers of the policy network\n",
        "              locals()[f\"output_policy_layer_norm_{i}\"] = _ln(locals()[f\"output_policy_concatenate_{i - 1}\"], f\"output_policy_layer_norm_{i}\")\n",
        "              locals()[f\"output_policy_dense_{i}\"] = SNdense(locals()[f\"output_policy_layer_norm_{i}\"], f\"output_policy_dense_{i}\", layers_policy[i - 2])\n",
        "              locals()[f\"output_policy_prelu_{i}\"] = prelu(locals()[f\"output_policy_dense_{i}\"])\n",
        "              locals()[f\"output_policy_dense_{i}_2\"] = SNdense(locals()[f\"output_policy_prelu_{i}\"], f\"output_policy_dense_{i}_2\", layers_policy[i - 2])\n",
        "              if dense_connections:\n",
        "                locals()[f\"output_policy_concatenate_{i}\"] = tf.concat([locals()[f\"output_policy_dense_{i}_2\"], pi_h], axis = 1)                  # contatination is done so as to implement dense connections\n",
        "              elif residual_connections:\n",
        "                locals()[f\"output_policy_concatenate_{i}\"] = tf.add(locals()[f\"output_policy_dense_{i}_2\"], SNdense(locals()[f\"output_policy_concatenate_{i - 1}\"], f\"output_policy_concatenate_{i}\", layers_policy[i - 2]))                  # contatination is done so as to implement dense connections\n",
        "              elif not_dense_connections_nor_residual_connections:\n",
        "                locals()[f\"output_policy_concatenate_{i}\"] = locals()[f\"output_policy_dense_{i}_2\"]\n",
        "            # the start of the definition of the last layer\n",
        "            locals()[f\"output_policy_layer_norm_{number_layers_policy_net + 1}\"] = _ln(locals()[f\"output_policy_concatenate_{i}\"], f\"output_policy_layer_norm_{number_layers_policy_net + 1}\")\n",
        "            locals()[f\"output_policy_dense_{number_layers_policy_net + 1}\"] = linear(locals()[f\"output_policy_layer_norm_{number_layers_policy_net + 1}\"], f\"output_policy_dense_{number_layers_policy_net + 1}\", n_hiddens_last_policy)\n",
        "            locals()[f\"output_policy_prelu_{number_layers_policy_net + 1}\"] = prelu(locals()[f\"output_policy_dense_{number_layers_policy_net + 1}\"])\n",
        "            locals()[f\"output_policy_dense_{number_layers_policy_net + 1}_2\"] = linear(locals()[f\"output_policy_prelu_{number_layers_policy_net + 1}\"], f\"output_policy_dense_{number_layers_policy_net + 1}_2\", n_hiddens_last_policy)\n",
        "            if residual_connections:\n",
        "              output_policy_dense = tf.add(locals()[f\"output_policy_dense_{number_layers_policy_net + 1}_2\"], linear(locals()[f\"output_policy_concatenate_{i}\"], \"output_policy_dense\", n_hiddens_last_policy))                  # contatination is done so as to implement dense connections\n",
        "            elif not_dense_connections_nor_residual_connections:\n",
        "              output_policy_dense = locals()[f\"output_policy_dense_{number_layers_policy_net + 1}_2\"]\n",
        "            # the end of the definition of the last layer\n",
        "\n",
        "            output_policy_output = output_policy_dense\n",
        "            # the end of the definition of the last layer\n",
        "\n",
        "            pi_latent = output_policy_output\n",
        "\n",
        "\n",
        "            # Value Function Deep Neural Network.\n",
        "            vf_h = extracted_features\n",
        "            output_value_1 = vf_h                 # input to the value network\n",
        "            # the start of the definition of the first layer\n",
        "\n",
        "            # implementation of attention layer in state value network\n",
        "            if use_attention_Luong_style and use_attention_layer:\n",
        "              attention_input_stack_value = tf.expand_dims(vf_h, 1)\n",
        "              output_attention_1_attention_value = tf.keras.layers.Attention(use_scale = False)([attention_input_stack_value, attention_input_stack_value, attention_input_stack_value])              # for Luong style attention\n",
        "              output_attention_1 = tf.squeeze(output_attention_1_attention_value, [1])\n",
        "            elif use_attention_Bahdanau_style and use_attention_layer:\n",
        "              attention_input_stack_value = tf.expand_dims(vf_h, 1)\n",
        "              output_attention_1_attention_value = tf.keras.layers.AdditiveAttention(use_scale = False)([attention_input_stack_value, attention_input_stack_value, attention_input_stack_value])              # for Luong style attention\n",
        "              output_attention_1 = tf.squeeze(output_attention_1_attention_value, [1])\n",
        "            else:\n",
        "              output_attention_1 = vf_h\n",
        "\n",
        "            output_value_layer_norm_1 = _ln(output_attention_1, \"output_value_layer_norm_1\")\n",
        "            output_value_dense_1 = linear(output_value_layer_norm_1, \"output_value_dense_1\", n_hiddens_first_value)\n",
        "            output_value_prelu_1 = prelu(output_value_dense_1)\n",
        "            output_value_dense_1_2 = linear(output_value_prelu_1, \"output_value_dense_1_2\", n_hiddens_first_value)\n",
        "            # the end of the definition of the first layer\n",
        "            if dense_connections:\n",
        "              output_value_concatenate_1 = tf.concat([output_value_dense_1_2, vf_h], axis = 1)\n",
        "            elif residual_connections:\n",
        "              output_value_concatenate_1 = tf.add(output_value_dense_1_2, linear(output_attention_1, \"output_value_concatenate_1\", n_hiddens_first_value))\n",
        "            elif not_dense_connections_nor_residual_connections:\n",
        "              output_value_concatenate_1 = output_value_dense_1_2\n",
        "            # the following \"for\" loop will define the hidden layers in the value function\n",
        "\n",
        "            # assertion of some conditions related to number of hidden neurons in the value function network\n",
        "            assert (2 / 3) * input_shape + 1 > min(input_shape, 1), \"the number of hidden neurons should be greater then the minimum of input dimension and the output dimension\"\n",
        "            assert (2 / 3) * input_shape + 1 < max(input_shape, 1), \"the number of hidden neurons should be less then the maximum of input dimension and the output dimension\"\n",
        "            assert (2 / 3) * input_shape + 1 < 2 * input_shape, \"the number of hidden neurons should be less then two times the input dimension\"\n",
        "\n",
        "            layers_value = []\n",
        "            for i in range(2, number_layers_value_net + 1):\n",
        "              layers_value.append(trial.suggest_int(str(i) + \"_value_function\", (2 / 3) * input_shape + 1, (2 / 3) * input_shape + 1))\n",
        "              locals()[f\"output_value_layer_norm_{i}\"] = _ln(locals()[f\"output_value_concatenate_{i - 1}\"], f\"output_value_layer_norm_{i}\")\n",
        "              locals()[f\"output_value_dense_{i}\"] = SNdense(locals()[f\"output_value_layer_norm_{i}\"], f\"output_value_dense_{i}\", layers_value[i - 2])\n",
        "              locals()[f\"output_value_prelu_{i}\"] = prelu(locals()[f\"output_value_dense_{i}\"])\n",
        "              locals()[f\"output_value_dense_{i}_2\"] = SNdense(locals()[f\"output_value_prelu_{i}\"], f\"output_value_dense_{i}_2\", layers_value[i - 2])\n",
        "              if dense_connections:\n",
        "                locals()[f\"output_value_concatenate_{i}\"] = tf.concat([locals()[f\"output_value_dense_{i}_2\"], vf_h], axis = 1)\n",
        "              elif residual_connections:\n",
        "                locals()[f\"output_value_concatenate_{i}\"] = tf.add(locals()[f\"output_value_dense_{i}_2\"], SNdense(locals()[f\"output_value_concatenate_{i - 1}\"], f\"output_value_concatenate_{i}\", layers_value[i - 2]))\n",
        "              elif not_dense_connections_nor_residual_connections:\n",
        "                locals()[f\"output_value_concatenate_{i}\"] = locals()[f\"output_value_dense_{i}_2\"]\n",
        "            # start of the definition of the last layer\n",
        "            output_value_layer_norm_last = _ln(locals()[f\"output_value_concatenate_{number_layers_value_net}\"], \"output_value_layer_norm_last\")\n",
        "            output_value_dense_last = linear(output_value_layer_norm_last, \"output_value_dense_last\", n_hiddens_last_value)\n",
        "            output_value_prelu_last = prelu(output_value_dense_last)\n",
        "            output_value_dense_last_2 = linear(output_value_prelu_last, \"output_value_dense_last_2\", n_hiddens_last_value)\n",
        "            if residual_connections:\n",
        "              output_value_dense_last_3 = tf.add(output_value_dense_last_2, linear(locals()[f\"output_value_concatenate_{number_layers_value_net}\"], \"output_value_dense_last_3\", n_hiddens_last_value))\n",
        "            elif not_dense_connections_nor_residual_connections:\n",
        "              output_value_dense_last_2 = output_value_dense_last_2\n",
        "            # end of the definition of the last layer\n",
        "\n",
        "            value_fn_output = output_value_dense_last_3\n",
        "\n",
        "            vf_latent = locals()[f\"output_value_concatenate_{number_layers_value_net}\"]\n",
        "\n",
        "\n",
        "            self._proba_distribution, self._policy, self.q_value = self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
        "\n",
        "        self._value_fn = value_fn_output\n",
        "        self._setup_init()\n",
        "\n",
        "\n",
        "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
        "        if deterministic:\n",
        "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
        "                                                   {self.obs_ph: obs})\n",
        "        else:\n",
        "          # probablistic policy is used during training to enhance exploration of reinforcement learning\n",
        "          action, value, neglogp = self.sess.run([self.action, self._value_flat, self.neglogp],\n",
        "                                                    {self.obs_ph: obs})\n",
        "        return action, value, self.initial_state, neglogp\n",
        "\n",
        "    def proba_step(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
        "\n",
        "    def value(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self._value_flat, {self.obs_ph: obs})\n",
        "\n",
        "\n",
        "  # a seprate envrinment is defined for evaluation of the learned policy\n",
        "  # eval_env = Envir(accessibality_based_delivery_cost_weight = 1 / 2, deprivation_cost_weight = 1 / 2, reward_scaling = 10 ** 10,\n",
        "  #              reallocate_t = 5, no_time_dim_state = 2, no_loc_dim_state = 2)\n",
        "  # Stop training when the model reaches the reward threshold\n",
        "  # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-1 * 10 ** -75, verbose=1)\n",
        "  # eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1)\n",
        "\n",
        "  env = Envir(accessibality_based_delivery_cost_weight = 1 / 3, deprivation_cost_weight = 1 / 3,\n",
        "            reward_scaling = reward_scaling, reallocate_t = 5, no_time_dim_state = 5, no_loc_dim_state = 5,\n",
        "            is_eval = False, is_loc_dec = False)\n",
        "\n",
        "  global checkpoint_callback\n",
        "\n",
        "  checkpoint_callback = CheckpointCallback(number_of_allocation_time_periods * 100, '/content/check_points', st)\n",
        "\n",
        "  # checkpoint_callback = CheckpointCallback(envv, save_freq = 500, save_path = '/content/check_points',\n",
        "  #                                        name_prefix = f'model_ppo_', initial_steps_episod_stall = 6000,\n",
        "  #                                        n_episod_stall = 2, delta_episod_stall = 1000)\n",
        "\n",
        "  save_temp = np.sum(cap_dist) / cap_consum_mat[0]\n",
        "\n",
        "  global model_ppo\n",
        "\n",
        "  # model_ppo = ACKTR(policy, env, gamma=gamma, nprocs=None, n_steps=20, ent_coef=0.01, vf_coef=0.5, vf_fisher_coef=1.0,\n",
        "  #                   learning_rate=0.25, max_grad_norm=0.5, kfac_clip=0.001, lr_schedule='linear', verbose=2,\n",
        "  #                   tensorboard_log=f\"/content/drive/MyDrive/drl_ppo_({no_aa},_{list(save_temp)}_{number_of_allocation_time_periods})__{seed}_tensorboard_convergence_curves/\",\n",
        "  #                   _init_setup_model=True, async_eigen_decomp=False, kfac_update=1, gae_lambda=None, policy_kwargs=None,\n",
        "  #                   full_tensorboard_log=False, seed=seed, n_cpu_tf_sess=1)\n",
        "\n",
        "  # model_ppo = A2C(KerasPolicy, env, gamma=gamma, n_steps=5, ent_coef=0.01, learning_rate=0.00025, vf_coef=0.5,\n",
        "                              # max_grad_norm=0.5, alpha=0.99, momentum=0.0, epsilon=1e-05, lr_schedule='constant', verbose=2,\n",
        "                              # tensorboard_log=f\"/content/drive/MyDrive/drl_ppo_({no_aa},_{list(save_temp)}_{number_of_allocation_time_periods})__{seed}_tensorboard_convergence_curves/\",\n",
        "                              # _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False, seed=seed, n_cpu_tf_sess=None)\n",
        "\n",
        "  # model_ppo = TRPO(KerasPolicy, env, gamma=gamma, timesteps_per_batch=1024, max_kl=0.01, cg_iters=10,\n",
        "  #                  lam=0.95, entcoeff=0.01, cg_damping=0.01, vf_stepsize=0.0003, vf_iters=3, verbose=2,\n",
        "  #                  tensorboard_log=f\"/content/drive/MyDrive/drl_ppo_({no_aa},_{list(save_temp)}_{number_of_allocation_time_periods})__{seed}_tensorboard_convergence_curves/\",\n",
        "  #                  _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False,\n",
        "  #                  seed=seed, n_cpu_tf_sess=1)\n",
        "\n",
        "  model_ppo = PPO2(KerasPolicy, env, gamma=gamma, n_steps=128, ent_coef=0.01, learning_rate=0.00025,\n",
        "                   vf_coef=0.5, max_grad_norm=0.5, lam=0.95, nminibatches=4, noptepochs=4, cliprange=0.2,\n",
        "                   cliprange_vf=None, verbose=0,\n",
        "                   tensorboard_log = None,\n",
        "                   _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False, seed=seed,\n",
        "                   n_cpu_tf_sess=None)\n",
        "\n",
        "  model_ppo.learn(int(1e10), log_interval = 1, callback = checkpoint_callback)\n",
        "\n",
        "  number_eval_episodes = 1\n",
        "  avg_reward = 0\n",
        "  for i in range(number_eval_episodes):\n",
        "    accm_reward = 0\n",
        "    obs = env.reset()\n",
        "    for j in range(6):                             # here, the value to be put is equal to the \"number of time periods\" in an episod or planning horizon\n",
        "        action, _states = model_ppo.predict(obs)\n",
        "        obs, rewards, dones, info = env.step(action)\n",
        "        accm_reward = accm_reward + rewards\n",
        "    avg_reward = avg_reward + accm_reward\n",
        "  avg_reward = avg_reward / number_eval_episodes                     # calculating the average episode reward\n",
        "\n",
        "  if tiral_counter_saving_model == 0:\n",
        "    model_ppo.save(f\"/content/drive/MyDrive/drl_ppo_({no_aa},_{list(save_temp)}_{number_of_allocation_time_periods})__{seed}_tensorboard_convergence_curves_trained_policy\")\n",
        "  elif avg_reward > previous_reward:\n",
        "    model_ppo.save(\"ppo2_humanitarian_logistics_location_resource_allocation_without_quantization\")\n",
        "\n",
        "  previous_reward = avg_reward\n",
        "  tiral_counter_saving_model = tiral_counter_saving_model + 1                     # Updating trial counter to identify particular onward is not the first trial.\n",
        "  return avg_reward\n",
        "\n",
        "\n",
        "\n",
        "study_ppo_without_quantization = optuna.create_study(direction = \"maximize\", study_name = \"ppo_without_quantization_\", sampler = optuna.samplers.TPESampler(), pruner = optuna.pruners.MedianPruner)\n",
        "study_ppo_without_quantization.optimize(objective, n_trials = 1, timeout = int(1e10))\n",
        "print(np.abs(st - time.time()))\n",
        "\n",
        "# print(accm_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK0WLQVCEJz9"
      },
      "outputs": [],
      "source": [
        "# after how many steps the RL training is terminated (after satisfaction of our training termination critrion)\n",
        "\n",
        "# number of time steps for which te algorithm is trained\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Training Time: \", np.abs(st - time.time())))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Number of steps for training: \", checkpoint_callback.term_n_steps))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "print(checkpoint_callback.term_n_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvklo2EfUesG"
      },
      "source": [
        "##**Reward (stage-wise objective) Values (actor-critic algorithms)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_eval_ep = 1\n",
        "\n",
        "obj_val = 0\n",
        "\n",
        "ws.append((\"Stagewise objective function value or reward:\",))\n",
        "\n",
        "for i in range(0, n_eval_ep):     # loop over the number of episodes\n",
        "  obs = envv.reset()\n",
        "  for j in range(0, envv.number_of_allocation_time_periods):     # loop over all the time peirods inside an episode\n",
        "    action, _states = model_ppo.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = envv.step(action)\n",
        "\n",
        "    # storing all the stage wise objective values into an excel sheet\n",
        "    ws.append((f\"Stagewise objective function value at stage {j + 1}: \", -1 * reward))\n",
        "\n",
        "    obj_val = obj_val + (gamma ** j) * reward"
      ],
      "metadata": {
        "id": "E8cQ21Lzu5rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the objective function valeu (sum of all the rewards) into an excel sheet\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Objective function value: \", -1 * obj_val))\n",
        "print(-1 * obj_val)\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ],
      "metadata": {
        "id": "CB19Lxzis6z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the state variables value to an excel sheet\n",
        "ws.append((\"State Variables\",))\n",
        "for i in state_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "# storing all the agent's actions into an excel sheet\n",
        "ws.append((\"Decision Variables\",))\n",
        "for i in act_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))"
      ],
      "metadata": {
        "id": "VDCg0q9Qz-Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_store"
      ],
      "metadata": {
        "id": "eXC8A_ntn9Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "act_store"
      ],
      "metadata": {
        "id": "AYR3memfn_V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving of the excel workbook at the end of the last experient of an instance, the workbook is saved with the abbreviaed notation of the problem instance as the name\n",
        "save_temp = float((sum(cap_dist) / max(cap_consum_mat))[0])\n",
        "wb.save(f'/content/drive/MyDrive/drl_ppo_({no_aa},_{[save_temp]},_{number_of_allocation_time_periods})_{seed}_excel_convergence_curves.xlsx')      # this statement wil only be exceuted at the end of the last experieent"
      ],
      "metadata": {
        "id": "fDkcQ2D4cAHL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
