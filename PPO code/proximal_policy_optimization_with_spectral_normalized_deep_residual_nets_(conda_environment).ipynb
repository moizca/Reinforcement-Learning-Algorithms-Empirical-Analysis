{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Requirements for running the notebook**\n",
        "\n",
        "\n",
        "\n",
        "1.   Run this note notebook on google colaboratory plateform. Otherwise, changes must be made to avoid incompatibilites that may arise. Feel free to contact the code author in case of any issues regarding this.\n",
        "2.   Some cells may take long time to run (especially the last one under the \"Environment and the algorithm\" heading, which cotains deep learning training code), so please be patient during their execution.\n",
        "3.   Under the heading \"Installation of 3.7 Python version in newly created conda environment alongwith relavent packages\", there is the code for installing required python version along with various libraries due to which runtime evironment may restart after the execution of idividual cells. So execute each of these cells idividually.\n",
        "4.   In order to run various instaces of the problem formulation, you can change the model parameters by changing the python variables initialized at the start of the last cell (under the \"Environment and the algorithm\" heading). The place where these variables are specified is clearly marked.\n",
        "5.   Run the whole noteook with GPU enabled, otherwise it will take exorbitantly long time to execute due to the involvement of deep learing training.\n"
      ],
      "metadata": {
        "id": "jlfVOsjREiI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mounting Google Drive**"
      ],
      "metadata": {
        "id": "yy32I5VSjlv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your google drive so that experiment logs can be saved\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zuRIs8RPZisv",
        "outputId": "86bcaff7-d72c-46be-dcf4-d18970886a5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installation of 3.7 Python version in newly created conda environment alongwith relavent packages**"
      ],
      "metadata": {
        "id": "BUYSD1RsjZTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python --version\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda --version\n",
        "!conda search python\n",
        "!conda create -n myenv python=3.7 # PYTHON_VERSION can be 3.9 or any other release"
      ],
      "metadata": {
        "id": "P2uoWQl93jq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing the \"requirements.txt\" file\n",
        "\n",
        "f = open(\"requirements.txt\", \"w\")\n",
        "f.write(\"\"\"# the requirments are at \"https://github.com/RussTedrake/manipulation/blob/master/colab-requirements.txt\"\n",
        "#\n",
        "# This file is autogenerated by pip-compile with python 3.7\n",
        "# To update, run:\n",
        "#\n",
        "#    pip-compile --output-file=colab-requirements.txt colab-requirements.in setup.cfg\n",
        "#\n",
        "absl-py==0.12.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "bayesian-optimization==1.2.0\n",
        "    # via nevergrad\n",
        "bleach==4.1.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "cachetools==4.2.4\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   google-auth\n",
        "certifi==2021.10.8\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests\n",
        "chardet==3.0.4\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests\n",
        "cloudpickle==1.6.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   gym\n",
        "    #   manipulation (setup.cfg)\n",
        "    #   stable-baselines3\n",
        "cma==3.1.0\n",
        "    # via nevergrad\n",
        "cycler==0.11.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   matplotlib\n",
        "decorator==4.4.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "defusedxml==0.7.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "easyprocess==0.3\n",
        "    # via pyvirtualdisplay\n",
        "entrypoints==0.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "future==0.16.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   pyglet\n",
        "google-auth==1.35.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   google-auth-oauthlib\n",
        "    #   tensorboard\n",
        "google-auth-oauthlib==0.4.6\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "gradescope-utils==0.4.0\n",
        "    # via manipulation (setup.cfg)\n",
        "grpcio==1.42.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "gym==0.17.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   stable-baselines3\n",
        "idna==2.10\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests\n",
        "imageio==2.4.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-image\n",
        "imgaug==0.2.6\n",
        "    # via -r colab-requirements.in\n",
        "importlib-metadata==4.8.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   markdown\n",
        "ipykernel==4.10.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipywidgets\n",
        "    #   notebook\n",
        "ipython==5.5.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipykernel\n",
        "    #   ipywidgets\n",
        "    #   meshcat\n",
        "ipython-genutils==0.2.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipywidgets\n",
        "    #   nbformat\n",
        "    #   notebook\n",
        "ipywidgets==7.6.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   open3d\n",
        "jinja2==2.11.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   mpld3\n",
        "    #   nbconvert\n",
        "    #   notebook\n",
        "joblib==1.1.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-learn\n",
        "jsonschema==2.6.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbformat\n",
        "jupyter-client==5.3.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipykernel\n",
        "    #   notebook\n",
        "jupyter-core==4.9.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   jupyter-client\n",
        "    #   nbconvert\n",
        "    #   nbformat\n",
        "    #   notebook\n",
        "jupyter-console==5.0.0\n",
        "jupyterlab-widgets==1.0.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipywidgets\n",
        "kiwisolver==1.3.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   matplotlib\n",
        "markdown==3.3.6\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "markupsafe==2.0.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   jinja2\n",
        "matplotlib==3.2.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   drake\n",
        "    #   mpld3\n",
        "    #   open3d\n",
        "    #   scikit-image\n",
        "    #   stable-baselines3\n",
        "meshcat==0.3.2\n",
        "    # via drake\n",
        "mistune==0.8.4\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "mpld3==0.5.1\n",
        "    # via manipulation (setup.cfg)\n",
        "nbconvert==5.6.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   notebook\n",
        "nbformat==5.1.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipywidgets\n",
        "    #   nbconvert\n",
        "    #   notebook\n",
        "networkx==2.6.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-image\n",
        "nevergrad==0.4.3.post9\n",
        "    # via manipulation (setup.cfg)\n",
        "notebook==5.3.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   open3d\n",
        "    #   widgetsnbextension\n",
        "numpy==1.19.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   bayesian-optimization\n",
        "    #   drake\n",
        "    #   gym\n",
        "    #   imageio\n",
        "    #   imgaug\n",
        "    #   matplotlib\n",
        "    #   meshcat\n",
        "    #   nevergrad\n",
        "    #   open3d\n",
        "    #   pandas\n",
        "    #   pywavelets\n",
        "    #   scikit-image\n",
        "    #   scikit-learn\n",
        "    #   scipy\n",
        "    #   stable-baselines3\n",
        "    #   tensorboard\n",
        "    #   tifffile\n",
        "    #   torchvision\n",
        "oauthlib==3.1.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests-oauthlib\n",
        "open3d==0.10.0.0\n",
        "    # via manipulation (setup.cfg)\n",
        "packaging==21.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   bleach\n",
        "pandas==1.1.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   stable-baselines3\n",
        "pandocfilters==1.5.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "pexpect==4.8.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "pickleshare==0.7.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "pillow==7.1.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   imageio\n",
        "    #   meshcat\n",
        "    #   scikit-image\n",
        "    #   torchvision\n",
        "prompt-toolkit==1.0.18\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "protobuf==3.17.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "ptyprocess==0.7.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   pexpect\n",
        "    #   terminado\n",
        "pyasn1==0.4.8\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   pyasn1-modules\n",
        "    #   rsa\n",
        "pyasn1-modules==0.2.8\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   google-auth\n",
        "pydot==1.3.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   drake\n",
        "pygame==2.1.2\n",
        "    # via drake\n",
        "pyglet==1.5.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   gym\n",
        "pygments==2.6.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "    #   nbconvert\n",
        "pymcubes==0.1.2\n",
        "    # via manipulation (setup.cfg)\n",
        "pyngrok==5.0.2\n",
        "    # via\n",
        "    #   -r colab-requirements.in\n",
        "    #   meshcat\n",
        "pyparsing==3.0.6\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   matplotlib\n",
        "    #   packaging\n",
        "    #   pydot\n",
        "python-dateutil==2.8.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   jupyter-client\n",
        "    #   matplotlib\n",
        "    #   pandas\n",
        "pytz==2018.9\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   pandas\n",
        "pyvirtualdisplay==1.3.2\n",
        "    # via -r colab-requirements.in\n",
        "pywavelets==1.2.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-image\n",
        "pyyaml==3.13\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   drake\n",
        "    #   pyngrok\n",
        "pyzmq==22.3.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   jupyter-client\n",
        "    #   meshcat\n",
        "requests==2.23.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests-oauthlib\n",
        "    #   tensorboard\n",
        "requests-oauthlib==1.3.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   google-auth-oauthlib\n",
        "rsa==4.8\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   google-auth\n",
        "scikit-image==0.18.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   imgaug\n",
        "scikit-learn==1.0.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   bayesian-optimization\n",
        "scipy==1.5.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   bayesian-optimization\n",
        "    #   drake\n",
        "    #   gym\n",
        "    #   imgaug\n",
        "    #   scikit-image\n",
        "    #   scikit-learn\n",
        "send2trash==1.8.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   notebook\n",
        "simplegeneric==0.8.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipython\n",
        "six==1.15.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   absl-py\n",
        "    #   bleach\n",
        "    #   google-auth\n",
        "    #   grpcio\n",
        "    #   imgaug\n",
        "    #   prompt-toolkit\n",
        "    #   protobuf\n",
        "    #   python-dateutil\n",
        "tensorboard==1.15.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   manipulation (setup.cfg)\n",
        "tensorboard-data-server==0.6.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "tensorboard-plugin-wit==1.8.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "terminado==0.12.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   notebook\n",
        "testpath==0.5.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   nbconvert\n",
        "threadpoolctl==3.0.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-learn\n",
        "tifffile==2021.11.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   scikit-image\n",
        "timeout-decorator==0.5.0\n",
        "    # via manipulation (setup.cfg)\n",
        "tornado==5.1.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipykernel\n",
        "    #   jupyter-client\n",
        "    #   meshcat\n",
        "    #   notebook\n",
        "    #   terminado\n",
        "traitlets==5.1.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipykernel\n",
        "    #   ipython\n",
        "    #   ipywidgets\n",
        "    #   jupyter-client\n",
        "    #   jupyter-core\n",
        "    #   nbconvert\n",
        "    #   nbformat\n",
        "    #   notebook\n",
        "typing-extensions==3.10.0.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   importlib-metadata\n",
        "    #   nevergrad\n",
        "    #   torch\n",
        "u-msgpack-python==2.7.1\n",
        "    # via meshcat\n",
        "urllib3==1.24.3\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   requests\n",
        "wcwidth==0.2.5\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   prompt-toolkit\n",
        "webencodings==0.5.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   bleach\n",
        "werkzeug==1.0.1\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   tensorboard\n",
        "wheel==0.37.0\n",
        "    # via tensorboard\n",
        "widgetsnbextension==3.5.2\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   ipywidgets\n",
        "    #   open3d\n",
        "zipp==3.6.0\n",
        "    # via\n",
        "    #   -c htmlbook/colab-constraints-colab.txt\n",
        "    #   importlib-metadata\n",
        "\n",
        "# The following packages are considered to be unsafe in a requirements file:\n",
        "# setuptools\n",
        "tensorflow==1.15.0\n",
        "optuna==2.10.0                        # Optuna is a python library for hyperparameter optimization in machine learning. Optuna python library is framework agnostic.\n",
        "plotly==5.6.0\n",
        "pyflann-py3==0.1.0\n",
        "codecarbon==2.1.4\"\"\")\n",
        "f.close()\n",
        "! source activate myenv; python --version; pip -V; pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "o8bcHAio612z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "conda update -n base -c defaults conda\n",
        "conda install -n myenv pip\n",
        "pip install --upgrade pip\n",
        "source activate myenv\n",
        "conda config --set auto_activate_base false\n",
        "pip install tensorflow-gpu==1.15.0\n",
        "pip install gym==0.17.3\n",
        "pip install stable-baselines==2.10.1\n",
        "pip install optuna==2.10.0\n",
        "pip install openpyxl==3.1.2\n",
        "pip install protobuf==3.20.0\n",
        "pip install google.colab\n",
        "conda env list"
      ],
      "metadata": {
        "id": "tM2gI8zO54OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment and the algorithm**"
      ],
      "metadata": {
        "id": "YdkOpHkBjtBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate myenv\n",
        "python\n",
        "\n",
        "seed = 1\n",
        "\n",
        "import gym                       # This library is used to make the custom RL environment. It is the framework for making custom RL environment.\n",
        "import numpy as np\n",
        "\n",
        "####################################################################################################################################\n",
        "############################################# Model parameters #####################################################################\n",
        "####################################################################################################################################\n",
        "\n",
        "# inorder to run different problem instances, change these parameters accordingly\n",
        "\n",
        "no_aa = 3   # number of affected areas\n",
        "no_materials = 1                 # number of relief items, considered one in all of my experiments\n",
        "number_of_allocation_time_periods = 6    # number of time periods\n",
        "cap_dist = np.array([1])          # amount of relief in local resposnse center in one time period\n",
        "\n",
        "aaa = np.array([2.04 for i in range(0, no_aa * no_materials)])                       # first deprivation parameter used in the deprivation cost function\n",
        "bbb = np.array([0.24 for i in range(0, no_aa * no_materials)])                       # Second deprivation parameter used in the deprivation cost function\n",
        "\n",
        "cost_path = np.array([np.array([200 + 50 * i for i in range(0, no_aa * no_materials)]) for i in range(0, number_of_allocation_time_periods)]).reshape(number_of_allocation_time_periods, no_aa * no_materials)      # unit capacity's accessibilities for each affected area and local response center\n",
        "\n",
        "demand_aa_mat = 1.5 * np.array([np.array([1 for i in range(0, no_aa * no_materials)]) for i in range(0, number_of_allocation_time_periods)]).reshape(number_of_allocation_time_periods, no_aa * no_materials)       # robustified demand generated by an affected area for a relief item in one time period\n",
        "\n",
        "####################################################################################################################################\n",
        "####################################################################################################################################\n",
        "####################################################################################################################################\n",
        "\n",
        "from math import floor\n",
        "from math import exp                                                              # Exponential function from the \"math\"library is used to calculate the exponential in the non-linear equations for calculating the deprivation cost and terminal panelty cost.\n",
        "import itertools as it\n",
        "import scipy.stats                                                      # This library is used to generate the demand values using the truncated normal distrubution for a particular episode.\n",
        "import copy\n",
        "import openpyxl\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "# from tensorflow.keras.engine import *\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.python.keras import constraints\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.python.layers import base\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.python.util import deprecation\n",
        "from tensorflow.python.util.tf_export import tf_export\n",
        "\n",
        "import os\n",
        "from abc import ABC\n",
        "import warnings\n",
        "import typing\n",
        "from typing import Union, List, Dict, Any, Optional\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from math import exp\n",
        "\n",
        "from stable_baselines.common.vec_env import VecEnv, sync_envs_normalization, DummyVecEnv\n",
        "from stable_baselines.common.evaluation import evaluate_policy\n",
        "from stable_baselines import logger\n",
        "\n",
        "import tensorflow as tf\n",
        "import optuna\n",
        "\n",
        "from stable_baselines import A2C\n",
        "# from stable_baselines import TRPO\n",
        "from stable_baselines import PPO2\n",
        "\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.common import make_vec_env\n",
        "\n",
        "from stable_baselines.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import stable_baselines\n",
        "from stable_baselines.common.policies import ActorCriticPolicy\n",
        "from stable_baselines.common.input import observation_input\n",
        "# from stable_baselines.common.callbacks import CheckpointCallback\n",
        "\n",
        "import time\n",
        "\n",
        "from numpy.random import seed as sd\n",
        "sd(seed)\n",
        "import random\n",
        "random.seed(seed)\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.random.set_random_seed(seed)\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['CHAINER_SEED'] = str(seed)\n",
        "# import chainer\n",
        "# chainer.cuda.cupy.random.seed(seed)\n",
        "# chainer.backends.cuda.get_device_from_id(gpu).use()\n",
        "# chainer.backends.cuda.cupy.random.seed(seed)\n",
        "# pyrandom.seed(seed)\n",
        "# with chainer.cuda.get_device_from_id(gpu):\n",
        "#   chainer.cuda.cupy.random.seed(seed)\n",
        "\n",
        "wb = openpyxl.Workbook()     # creating a new workbook\n",
        "ws = wb.active   # activating the workbook for reading and writing of data into and from the workbook respectively\n",
        "ws.append((\"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", f\"Random seed: {seed}\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\", \"####\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "# definintion for te function which calculates total objective functions value given atates and resource allocation decisions in all the time periods (this function will be used for te calculation of the scaling factor fof the reward as well as for the evaluation of the solution from julia language)\n",
        "\n",
        "def eval_non_convex_obj_fn(dep_lvl_state_var_dict, no_time_periods, no_lrc, no_aa, no_m, aaa, bbb, Len,\n",
        "                               cost_path, logistics_cost_weight, absolute_deprivation_cost_weight, cap):\n",
        "\n",
        "  # calculation of deprivation cost\n",
        "  dep_cost_sum = 0                                            # Before calculating the total deprivation cost, the class attribute \"deprivation cost\" is set to \"0\".\n",
        "  for state_dict_temp in dep_lvl_state_var_dict:\n",
        "    for i in range(0, no_aa):                                               # This for loop loops through all the affected areas and calculate the total deprivation cost\n",
        "      for j in range(0, no_m):\n",
        "        if (state_dict_temp[i, j] >= 0):                                                                   # It the state value of the affected area is greater then or equal to \"0\" then the formula below is used.\n",
        "          dep_cost_sum = dep_cost_sum + exp(aaa[i, j]) * (exp(bbb[i, j] * Len) - 1) * (exp(bbb[i, j] * Len) ** state_dict_temp[i, j])                        # This formula is used to calculate the deprivation cost for a particular Affected Area, given a particular state value of that Affected Area.\n",
        "        else:\n",
        "          dep_cost_sum = dep_cost_sum                                                                 # If the state value of the Affected Area is less then \"0\" then the deprivation cost of that affected area will be set to \"0\".\n",
        "\n",
        "  # logistics cost\n",
        "  log_cost = 0\n",
        "  for j in range(0, no_aa):\n",
        "    for k in range(0, no_m):\n",
        "      for l in range(0, no_lrc):\n",
        "        log_cost = log_cost + cost_path * cap\n",
        "\n",
        "  return absolute_deprivation_cost_weight * dep_cost_sum + logistics_cost_weight * log_cost\n",
        "\n",
        "is_loc_dec = False\n",
        "no_dcl_loc_aval = 1\n",
        "cap_consum_mat = np.array([1]).reshape(no_materials, no_dcl_loc_aval)                   # capacity consumption by a particular relief item in a particular local response center\n",
        "Len = 4       # length (in terms of hours) of one time period\n",
        "planning_horizon = Len * number_of_allocation_time_periods\n",
        "gamma = 0.99999       # discount factor\n",
        "\n",
        "if is_loc_dec:\n",
        "  erect_cost = np.array([0])                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "  removal_cost = np.array([0])                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "  # a way to simplify the model and avoid unecessary complications (non-convexities)\n",
        "  fixed_cost = (erect_cost + removal_cost) / 2        # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "\n",
        "episode_counter = 0\n",
        "\n",
        "scaling_reward_arr = np.sum(demand_aa_mat, axis = 0).reshape(1, -1)\n",
        "reward_scaling = eval_non_convex_obj_fn(scaling_reward_arr.reshape(1, no_aa, no_materials), number_of_allocation_time_periods, no_dcl_loc_aval, no_aa, no_materials, aaa.reshape(no_aa, no_materials), bbb.reshape(no_aa, no_materials), Len,\n",
        "                                                                    np.max(cost_path), 1 / 3, 1 / 3, np.max(cap_dist) / np.max(cap_consum_mat))\n",
        "\n",
        "# the following variables are used to store states and action from trained jpolicy during evaluation\n",
        "# for storing states during evaluation simulation\n",
        "if is_loc_dec:\n",
        "  state_store = np.zeros((number_of_allocation_time_periods + 1, no_aa * no_materials + no_dcl_loc_aval))\n",
        "else:\n",
        "  state_store = np.zeros((number_of_allocation_time_periods + 1, no_aa * no_materials))\n",
        "\n",
        "# for storing action during evaluation simulation\n",
        "if is_loc_dec:\n",
        "  act_store = np.zeros((number_of_allocation_time_periods, no_aa * no_materials * no_dcl_loc_aval + no_dcl_loc_aval))\n",
        "else:\n",
        "  act_store = np.zeros((number_of_allocation_time_periods, no_aa * no_materials * no_dcl_loc_aval))\n",
        "\n",
        "class Envir(gym.Env):\n",
        "  def __init__(self, accessibality_based_delivery_cost_weight = 1 / 2, deprivation_cost_weight = 1 / 2, reward_scaling = 10 ** 10,\n",
        "               reallocate_t = 5, no_time_dim_state = 2, no_loc_dim_state = 2, is_eval = False, is_loc_dec = True):\n",
        "\n",
        "        self.is_eval = is_eval\n",
        "        self.is_loc_dec = is_loc_dec                 # it is a boolean variable which is \"True\" if location allocation decsions are included otherwise it will be \"False\"\n",
        "\n",
        "        self.accessibality_based_delivery_cost_weight = accessibality_based_delivery_cost_weight\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.fixed_cost_of_dist_centers_weight = accessibality_based_delivery_cost_weight\n",
        "\n",
        "        self.deprivation_cost_weight = deprivation_cost_weight\n",
        "        self.terminal_penality_cost_weight = deprivation_cost_weight\n",
        "        self.reward_scaling = reward_scaling\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.reallocate_t = reallocate_t                               # number of time periods after which location realloction decision for local response centers needs to be taken\n",
        "\n",
        "        self.no_time_dim_state = no_time_dim_state\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.no_loc_dim_state = no_loc_dim_state\n",
        "\n",
        "        global no_aa\n",
        "        global no_materials\n",
        "        global number_of_allocation_time_periods\n",
        "        global no_dcl_loc_aval\n",
        "        global cap_dist\n",
        "        global cap_consum_mat\n",
        "        global Len\n",
        "        global planning_horizon\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          global erect_cost\n",
        "          global removal_cost\n",
        "          # a way to simplify the model and avoid unecessary complications (non-convexities)\n",
        "          global fixed_cost\n",
        "\n",
        "        global aaa\n",
        "        global bbb\n",
        "        global cost_path\n",
        "\n",
        "        global demand_aa_mat\n",
        "\n",
        "        self.no_aa = no_aa                                                           # Number of Affected Areas (AAs), to be entered for every instance of the problem.\n",
        "        self.no_dcl_loc_aval = no_dcl_loc_aval                                                          # number of destrubution centers (DCs) or local response centers, to be entered for every instance of the problem.\n",
        "        self.no_materials = no_materials\n",
        "        self.number_of_allocation_time_periods = number_of_allocation_time_periods                                # The number of discrete time periods into which the whole planning time period is to be divided.\n",
        "        self.steps_per_episode = self.number_of_allocation_time_periods           # Steps per Episodes. (same as above)\n",
        "        self.cap_dist = cap_dist                            # Capacities of the Distribution Centers or local response center (Simulated by an RL agent). The size of this array is equal to \"self.no_materials\" times \"self.no_dcl\".\n",
        "        self.cap_consum_mat = cap_consum_mat                   # capacity consumption by a particular relief item in a particular local response center\n",
        "        self.Len = Len       # length (in terms of hours) of one time period\n",
        "        self.planning_horizon = planning_horizon\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.erect_cost =  erect_cost                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "          self.removal_cost = removal_cost                                # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "          # a way to simplify the model and avoid unecessary complications (non-convexities)\n",
        "          self.fixed_cost = fixed_cost        # fixed cost for the extablishment of local response centers (LRCs), the shape of the array is self.number_of_allocation_time_periods by self.no_dcl_loc_aval. The order is the same.\n",
        "\n",
        "        self.episode_counter = 0\n",
        "        self.aaa = aaa                       # The shape is of the form self.no_aa by self.no_materials.                        # In the numpy array the cofficient values for different materials for the same affected area are grouped together. The size of this array is equal to \"self.no_materials\" times \"self.no_dcl_loc_aval\".                                       # A constant used in calculating reward for the Agent (Depreivation cost and Terminal Penality cost).\n",
        "        self.bbb = bbb                       # The shape is of the form self.no_aa by self.no_materials.                        # In the numpy array the cofficient values for different materials for the same affected area are grouped together. The size of this array is equal to \"self.no_materials\" times \"self.no_dcl_loc_aval\".                                      # A constant used in calculating reward for the Agent (Depreivation cost and Terminal Penality cost).\n",
        "        # 200, 250, 300, 350, 400, 450, 500, 550, 600, 650\n",
        "        self.cost_path = cost_path\n",
        "\n",
        "        self.demand_aa_mat = demand_aa_mat                 # In each time period, there is demand associated with each affected area and material. For each time period, the demand is arranged by affected areas and material.\n",
        "\n",
        "        self.state_temp = np.array([-1 for i in range(0, self.steps_per_episode)])\n",
        "\n",
        "        # all or nothing disruption probabilities for potential LRC location\n",
        "        if self.is_loc_dec:\n",
        "          self.disrup_prob = np.array([[0, 1]])         # this must be two dimensional array with size equal to number of local response centers time \"2\" (\"2\" because there can be only two possible scenarios for an LRC, either it is disrupted or it is not disrupted)\n",
        "          self.loc_disrup_temp = np.zeros((self.no_dcl_loc_aval,))          # this array will be used in disruption calculations\n",
        "\n",
        "        # enumeration of location decisions for all local response centers or distribution centers\n",
        "        if self.is_loc_dec:\n",
        "          self.loc_dcl = []\n",
        "          a = 0\n",
        "          while a < self.no_dcl_loc_aval:\n",
        "            self.loc_dcl.append([0, 1])\n",
        "            a = a + 1\n",
        "          self.loc_dcl = np.array(self.loc_dcl)\n",
        "          self.loc_dcl_actions = np.array(np.meshgrid(*list(self.loc_dcl))).T.reshape(-1, self.no_dcl_loc_aval)      # len([0, 1]) = 2                     # All the possible actions related to loction are listed in \"self.loc_dcl_actions\".\n",
        "          del self.loc_dcl                                     # Deleting the 'self.loc_dcl' array as it is no longer required, to free up memory. (Make the code memory efficient)\n",
        "          del a\n",
        "\n",
        "          self.dc_loc_alr_const = np.array([1])                             # The length of the array is equal to the number of distrubution centers, \"self.no_dcl_loc_aval\".                   # The array representing the agent's decision for establishing distribution center at a particular avalilable location (to be taken from agent's action array).\n",
        "\n",
        "        # enumeration of resource allocation decision from relief material allocation from local response centers to affected areas\n",
        "        self.counter = 0                                                                  # Its value will increase until all the combinations of the affected areas and distrubuiton centers are exhausted.\n",
        "\n",
        "        self.res_alocat_dc = []\n",
        "        m = 0\n",
        "        while m < self.no_aa:                                             # These nested while loops have the task to generate the numpy array list of all possible allocation aciton, which the agent can do. In it, there may be some actions which are infeasible.\n",
        "          i = 0\n",
        "          while i < self.no_materials:\n",
        "            j = 0\n",
        "            while j < self.no_dcl_loc_aval:\n",
        "              k = 0\n",
        "              self.res_alocat_dc.append([])\n",
        "              while k <= self.cap_dist[j] / self.cap_consum_mat[i, j]:\n",
        "                self.res_alocat_dc[self.counter].append(k)\n",
        "                k = k + 1\n",
        "              self.counter = self.counter + 1\n",
        "              j = j + 1\n",
        "            i = i + 1\n",
        "          m = m + 1\n",
        "        self.res_alocat_dc = np.array(self.res_alocat_dc)\n",
        "        self.alot_dcl_actions = np.array(np.meshgrid(*self.res_alocat_dc)).T.reshape(-1, self.no_dcl_loc_aval * self.no_aa * self.no_materials)                           # This statement creates the real numpy array list of all the possible resource allocation decisions. \"self.alot_dcl_actions\" contains all the possible actions related to resource allocation. The arrangement in \"self.alot_dcl_actions\" is self.no_materials by self.no_dcl_loc_aval.\n",
        "\n",
        "        # determination of action validaty (are all constraints satisfied or not?) and the elimination of invalid actions\n",
        "        self.del_el = []                  # array elements to be deleated\n",
        "        # if the allocation capacity of any distribution center location is exceeded by the total allocation made from that distribution ceneter location then the action is eliminated in the following code.\n",
        "        for j in range(0, self.alot_dcl_actions.shape[0]):          # Action shaping implementation. (eliminating some obvious infeasible actions) Ref: A. Kanervisto, C. Scheller and V. Hautamäki, \"Action Space Shaping in Deep Reinforcement Learning,\" 2020 IEEE Conference on Games (CoG), 2020, pp. 479-486, doi: 10.1109/CoG47356.2020.9231687.\n",
        "          self.compare_for_action_shaping = np.sum(np.multiply(np.reshape(np.sum(np.reshape(self.alot_dcl_actions[j], (self.no_aa, self.no_materials * self.no_dcl_loc_aval)), axis = 0), (self.no_materials, self.no_dcl_loc_aval)), self.cap_consum_mat), axis = 0)               # As capacity constraint is the hard constraint so we use action shaping to eliminate the actions which violate this constraint to ensure that the capacity constraints must be satisfied, otherwise it makes no sense that the allocation exceeds the capacity of distribution centers.\n",
        "          for k in range(0, self.compare_for_action_shaping.shape[0]):\n",
        "            if self.compare_for_action_shaping[k] > self.cap_dist[k]:\n",
        "              self.del_el.append(j)\n",
        "              break\n",
        "\n",
        "        self.alot_dcl_actions = np.delete(self.alot_dcl_actions, self.del_el, axis = 0)\n",
        "\n",
        "        del self.counter\n",
        "        del m\n",
        "        del i\n",
        "        del j\n",
        "        del k\n",
        "        del self.res_alocat_dc                                                          # Deleting the variables array as it is no longer required, to free up memory. (Make the code memory efficient)\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          i = 0\n",
        "          while i < 2 ** self.no_dcl_loc_aval:                                          # Number of possible location allocation decisions are equal to 2 raise to power the total number of distrubution centers.\n",
        "            self.temp_arr_loc = np.repeat(self.loc_dcl_actions[i].reshape(1, -1), self.alot_dcl_actions.shape[0], axis = 0)\n",
        "            self.alot_loc_dcl_actions_full = np.concatenate((self.temp_arr_loc, np.copy(self.alot_dcl_actions)), axis = 1)\n",
        "            if i == 0:\n",
        "              self.alot_loc_dcl_actions_full_array = self.alot_loc_dcl_actions_full\n",
        "            else:\n",
        "              self.alot_loc_dcl_actions_full_array = np.concatenate((self.alot_loc_dcl_actions_full_array, self.alot_loc_dcl_actions_full), axis = 0)                              # \"self.alot_loc_dcl_actions_full_array\" numpy array contains all the possible action (location and resoruce allocation actions combined)\n",
        "            i = i + 1\n",
        "\n",
        "          del self.alot_loc_dcl_actions_full\n",
        "\n",
        "          # filteration based on location allocation and reallocation deicsions\n",
        "          # self.del_el_loc_constr = []                  # array elements to be deleated\n",
        "          # i = 0\n",
        "          # # if any allocation is made through the distribution center location such that this distribution center location is not used in a particular action then these actions are eliminated (deleted).\n",
        "          # while i < self.alot_loc_dcl_actions_full_array.shape[0]:                                              # Action shaping implementation. (eliminating some obvious infeasible actions) Ref: A. Kanervisto, C. Scheller and V. Hautamäki, \"Action Space Shaping in Deep Reinforcement Learning,\" 2020 IEEE Conference on Games (CoG), 2020, pp. 479-486, doi: 10.1109/CoG47356.2020.9231687.\n",
        "          #   self.alot_loc_dcl_actions_full_array_loc = self.alot_loc_dcl_actions_full_array[i, 0 : self.no_dcl_loc_aval]\n",
        "          #   self.alot_loc_dcl_actions_full_array_res = self.alot_loc_dcl_actions_full_array[i, self.no_dcl_loc_aval:].reshape(-1, self.no_dcl_loc_aval)\n",
        "          #   self.alot_loc_dcl_actions_full_array_res = np.sum(self.alot_loc_dcl_actions_full_array_res, axis = 0)\n",
        "          #   for j in range(0, self.no_dcl_loc_aval):\n",
        "          #     if self.alot_loc_dcl_actions_full_array_loc[j] == 0 and self.alot_loc_dcl_actions_full_array_res[j] != 0:\n",
        "          #       self.del_el_loc_constr.append(i)\n",
        "          #       break\n",
        "          #   i = i + 1\n",
        "\n",
        "          # self.alot_loc_dcl_actions_full_array = np.delete(self.alot_loc_dcl_actions_full_array, self.del_el_loc_constr, axis = 0)\n",
        "        else:\n",
        "         self.alot_loc_dcl_actions_full_array = self.alot_dcl_actions   # if location allocation and relocation decisions are not included, then all the possible actions are only valid resource allocation actions\n",
        "\n",
        "        self.actions_loc_alloct_dist = self.alot_loc_dcl_actions_full_array\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(self.alot_loc_dcl_actions_full_array.shape[0])                             # The action space of this custom environment, \"Discrete\" is used. As it the optimization problem which we are solving is combinatorial in nature.\n",
        "        if self.is_loc_dec:\n",
        "          self.observation_space = gym.spaces.Box(np.array([-np.inf for i in range(0, self.no_aa * self.no_materials + self.no_loc_dim_state * self.no_dcl_loc_aval + self.no_time_dim_state * (self.steps_per_episode))]), np.array([np.inf for i in range(0, self.no_aa * self.no_materials + self.no_loc_dim_state * self.no_dcl_loc_aval + self.no_time_dim_state * (self.steps_per_episode))]))\n",
        "        else:\n",
        "          self.observation_space = gym.spaces.Box(np.array([-np.inf for i in range(0, self.no_aa * self.no_materials + self.no_time_dim_state * (self.steps_per_episode))]), np.array([np.inf for i in range(0, self.no_aa * self.no_materials + self.no_time_dim_state * (self.steps_per_episode))]))\n",
        "\n",
        "\n",
        "  def step(self, act):                                                            # The agent just gives the index of the action which is taken by the RL agent, as the actions are discrete.\n",
        "\n",
        "        global state_store\n",
        "        global act_store\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.loc_state_temp[self.loc_state_temp == -1] = 0              # this modification is done for the ease of computation (so that we do not have to modify it repeatedly)\n",
        "\n",
        "        self.sstep = self.sstep + 1                                                   # The above statement is there as a counter. It countes the number of steps taken in an episode. (updating of the step counter.)\n",
        "\n",
        "        self.act_raw = self.actions_loc_alloct_dist[act]              # Taking the action vector from the agent and storing it in the \"step\" class attribute \"act\".                                                    # Step counter, which counts the number of steps in an episodes.\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          if ((self.sstep - 1) % self.reallocate_t != 0) and (self.sstep != self.steps_per_episode):    # location allocation and relocation action can only be taken in time periods whose index is divisiable by \"self.reallocate_t\"\n",
        "            self.act_loc = self.act_raw[:self.no_dcl_loc_aval]              # location allocation decision for local response centers (LRCs)\n",
        "\n",
        "          self.act_partial_raw = self.act_raw[self.no_dcl_loc_aval:]           # This statement extracts only the resoruce allocation portion of the action taken by the agent.\n",
        "\n",
        "        else:\n",
        "          self.act_partial_raw = self.act_raw\n",
        "\n",
        "        # the following conditional state will be executed only when the agent cannot make decisions on locations of LRCs. Thus, we must make the resoruce allocation actions feasible with the already made location allocation decisons in the previous time period (modified by disruption)\n",
        "        # making resource allocation action feasible to what LRCs locations are currently operational (for location allocation decisions less time periods)\n",
        "        if self.is_loc_dec:\n",
        "          # modification of \"self.act_partial_raw\"\n",
        "          self.act_partial_raw = np.multiply(np.tile(self.loc_state_temp, self.no_aa * self.no_materials).reshape(self.no_aa * self.no_materials, self.no_dcl_loc_aval),\n",
        "                                             np.reshape(self.act_partial_raw, (self.no_aa * self.no_materials, self.no_dcl_loc_aval))).reshape(-1,)\n",
        "\n",
        "        # the following code will be executed when the trained feedback policy is being evaluated\n",
        "        if self.is_eval:\n",
        "          act_store[self.sstep - 1, :] = self.act_partial_raw\n",
        "          print(\"resource_allocation_decisions: \", self.act_partial_raw)\n",
        "\n",
        "        self.act_partial_raw_reshape = np.reshape(self.act_partial_raw, (self.no_aa, self.no_materials * self.no_dcl_loc_aval))\n",
        "        self.act_partial_raw_reshape_sum = np.sum(self.act_partial_raw_reshape, axis = 0)\n",
        "\n",
        "        # transition function\n",
        "        self.state = self.state - np.sum(np.reshape(self.act_partial_raw, (self.no_aa * self.no_materials, self.no_dcl_loc_aval)), axis = 1) + self.demand_aa_mat[self.sstep - 1]                                         # The computation of the next state vector.\n",
        "\n",
        "        if self.is_eval:\n",
        "          state_store[self.sstep, :] = self.state\n",
        "\n",
        "        # local response centers' location update (part of transition function)\n",
        "        if self.is_loc_dec:\n",
        "          if ((self.sstep - 1) % self.reallocate_t == 0) and (self.sstep != self.steps_per_episode):      # there is no any reason to take location allocation decisons if you facilities are only for the response phase and cannot be used subsequently\n",
        "            # the following code will be executed if the the feedback policy is being evaluated\n",
        "            if self.is_eval:\n",
        "              print(\"LRCs'_location_allocation_and_relocation_decisions: \", self.act_loc)\n",
        "\n",
        "            self.loc_state = self.act_loc              # outgoing state variable\n",
        "            print(self.act_loc, \"llllllllllllll\")\n",
        "          else:\n",
        "            self.loc_state = self.loc_state_temp       # outgoing state variable\n",
        "\n",
        "          # self.loc_state = np.array([1 for i in range(0, self.no_dcl_loc_aval)])       # this statement will run only when location allocation decisons are not to be included\n",
        "\n",
        "        self.fixed_cost_of_dist_centers = 0\n",
        "        i = 0\n",
        "        self.act = []\n",
        "        self.accumulate = 0\n",
        "\n",
        "        self.cap_dist_matrix = self.cap_dist.reshape(-1, self.no_dcl_loc_aval)\n",
        "\n",
        "        # fixied cost for establishment of local response centers (LRCs) (installation or deinstallation cost)\n",
        "        if self.is_loc_dec and ((self.sstep - 1) % self.reallocate_t == 0) and (self.sstep != self.steps_per_episode):\n",
        "          self.fixed_cost_of_dist_centers = np.dot(np.abs(self.act_loc - self.loc_state_temp), self.fixed_cost)\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.loc_state_temp = copy.deepcopy(self.loc_state)\n",
        "\n",
        "        # before the system can move two the next time step (and after the complete execution of current time period's decisions) LRCs are exposed to all or nothing disruption\n",
        "        if self.is_loc_dec:\n",
        "          for i in range(0, self.no_dcl_loc_aval):\n",
        "            self.loc_disrup_temp[i] = np.random.choice(np.array([0, 1]), size=(1,), replace=True, p=self.disrup_prob[i])\n",
        "          self.loc_state_temp = np.multiply(self.loc_state_temp, self.loc_disrup_temp).reshape(-1,)\n",
        "\n",
        "          # the following code will be executed if the the feedback policy is being evaluated\n",
        "          if self.is_eval:\n",
        "            print(\"which_potential_LRCs'_locations_have_disruption_occured_on_them_in_the_current_time_period_which_will_impact_subsequent_time_period?: \", self.loc_disrup_temp)\n",
        "\n",
        "        if self.is_loc_dec and self.sstep == self.steps_per_episode:              # at the end of planning horizon, all the established location response centers (LRC) (as they are the temprary strcutures)\n",
        "          self.fixed_cost_of_dist_centers = self.fixed_cost_of_dist_centers + np.dot(self.loc_state_temp, self.fixed_cost)            # at the of planning horizon (episode) the removal cost for all established local response centers (LRCs) must be included in the incurred fixed cost\n",
        "\n",
        "        counter = 0\n",
        "\n",
        "        self.accessibility_based_delivery_cost = np.dot(self.act_partial_raw, self.cost_path[self.sstep - 1])               # \"Accesibility Based Delivery Cost\" is calculated by computing the dot product of allocation action vector, and the path cost vector.\n",
        "\n",
        "        self.dep_cost_sum = 0                                            # Before calculating the total deprivation cost, the class attribute \"deprivation cost\" is set to \"0\".\n",
        "        if self.sstep != self.steps_per_episode:\n",
        "          for s in self.state:                                               # This for loop loops through all the affected areas and calculate the total deprivation cost.\n",
        "            if s >= 0:                                                                   # It the state value of the affected area is greater then or equal to \"0\" then the formula below is used.\n",
        "              self.dep = exp(self.aaa[counter]) * (exp(self.bbb[counter] * self.Len) - 1) * (exp(self.bbb[counter] * self.Len) ** s)                        # This formula is used to calculate the deprivation cost for a particular Affected Area, given a particular state value of that Affected Area.\n",
        "            else:\n",
        "              self.dep = 0                                                                 # If the state value of the Affected Area is less then \"0\" then the deprivation cost of that affected area will be set to \"0\".\n",
        "            if self.sstep == 1:                                                              # If it is the first time step of an episode then there is an additional step is calculating the deprivation cost for the time period.\n",
        "              self.dep = self.dep + np.multiply(np.exp(self.aaa[counter]), (np.exp(self.bbb[counter] * self.Len) - 1))               # This is the additional step is calculating he deprivation cost for the first time period of the episode.\n",
        "\n",
        "            counter = counter + 1\n",
        "\n",
        "            self.dep_cost_sum = self.dep_cost_sum + self.dep\n",
        "\n",
        "\n",
        "        else:                                     # If the time step of and episode is the last time step then there is an additional cost called the \"penelty cost\".\n",
        "          self.terminal_panelty_cost_sum = 0                                                # Before calculating the terminal panelty cost, the class attribute \"terminal_penelty_cost\" is set to zero.\n",
        "          counter = 0\n",
        "          for s in self.state:                                                         # This for loop loop through all the Affected Areas to calculate the terminal penelty cost of each Affected Area and total terminal penelty cost.\n",
        "            if s >= 0:                                                                   # If the state value of the Affected Area is greater then or equal to \"0\" then the formula is the next statement is used to calcualte the terminal penelty cost.\n",
        "              self.term = exp(self.aaa[counter]) * (exp(self.bbb[counter] * self.Len)-1) * (exp(self.bbb[counter] * self.Len) ** s)              # The formula used for calculating the terminal penelty cost for a particular Affected Area.\n",
        "            else:\n",
        "              self.term = 0                                                              # If state value of the affected area is less then \"0\" then the terminal panelty cost is set to zero.\n",
        "\n",
        "            counter = counter + 1\n",
        "\n",
        "            self.terminal_panelty_cost_sum = self.terminal_panelty_cost_sum + self.term\n",
        "\n",
        "        # The following if-elif-else computes the reward which will be delivered to the agent.\n",
        "        if self.is_loc_dec:\n",
        "          if self.sstep != self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.dep_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost - self.accessibality_based_delivery_cost_weight * self.fixed_cost_of_dist_centers                 # If the Markovian time step is the first time step, then the reward is calculated by summing the negative of total deprivations cost and total accessibility based delivery cost.\n",
        "          elif self.sstep == self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.terminal_panelty_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost - self.accessibality_based_delivery_cost_weight * self.fixed_cost_of_dist_centers             # If the Markovian time step is the last time step then the reward for the RL agent is calculated by summing the negative of accessibility based delivery cost and terminal penality cost.\n",
        "\n",
        "          del self.fixed_cost_of_dist_centers\n",
        "\n",
        "        else:\n",
        "          if self.sstep != self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.dep_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost                 # If the Markovian time step is the first time step, then the reward is calculated by summing the negative of total deprivations cost and total accessibility based delivery cost.\n",
        "          elif self.sstep == self.steps_per_episode:\n",
        "            self.reward = - self.deprivation_cost_weight * self.terminal_panelty_cost_sum - self.accessibality_based_delivery_cost_weight * self.accessibility_based_delivery_cost             # If the Markovian time step is the last time step then the reward for the RL agent is calculated by summing the negative of accessibility based delivery cost and terminal penality cost.\n",
        "\n",
        "        del self.accessibility_based_delivery_cost\n",
        "\n",
        "        if self.sstep == self.steps_per_episode:\n",
        "          self.done = True                                                                 # If the time period is the last time period of the episode then set the class attribute \"done\" to True.\n",
        "        else:\n",
        "          self.done = False                                                                # If the time period is not the last time period of the episode then set the class attribute \"done\" to False.\n",
        "        self.info = {}                                                                     # Class attribute \"info\" is an empty python dictionary which is returned to the RL agent at every time step.\n",
        "        self.state_t = copy.deepcopy(self.state_temp)\n",
        "\n",
        "        # the following code will be executed if the the feedback policy is being evaluated\n",
        "        if self.is_eval:\n",
        "          print(\"reward_or_stagewise_objective_function_value: \", self.reward)\n",
        "\n",
        "        # print(self.loc_state, \"self.loc_stateself.loc_stateself.loc_state\")\n",
        "        self.state_t[self.sstep - 1] = 1\n",
        "        # print(self.state_t, \"vself.state_tself.state_t\")\n",
        "        # NewValue = (((OldValue - OldMin) * (NewMax - NewMin)) / (OldMax - OldMin)) + NewMin\n",
        "\n",
        "        if self.is_loc_dec:\n",
        "          self.loc_state_temp[self.loc_state_temp == 0] = -1\n",
        "          return np.concatenate(((((self.state.reshape(self.no_aa * self.no_materials,) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist) / np.min(self.cap_consum_mat)), repeats = self.no_aa * self.no_materials))) / (np.sum(self.demand_aa_mat, axis = 0) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist)), repeats = self.no_aa * self.no_materials)))) * (2 * np.ones((self.no_aa * self.no_materials,))) - 1), np.repeat(self.loc_state_temp, self.no_loc_dim_state), np.repeat(self.state_t, self.no_time_dim_state))),\\\n",
        "                        float(self.reward / self.reward_scaling), self.done, self.info           # The step method will return the state of the time period, the reward, \"done\" attribute (which shows weather the episode has terminated or not) and the empty python dictionary \"info\".\n",
        "        else:\n",
        "          return np.concatenate(((((self.state.reshape(self.no_aa * self.no_materials,) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist) / np.min(self.cap_consum_mat)), repeats = self.no_aa * self.no_materials))) / (np.sum(self.demand_aa_mat, axis = 0) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist)), repeats = self.no_aa * self.no_materials)))) * (2 * np.ones((self.no_aa * self.no_materials,))) - 1), np.repeat(self.state_t, self.no_time_dim_state))),\\\n",
        "                        float(self.reward / self.reward_scaling), self.done, self.info           # The step method will return the state of the time period, the reward, \"done\" attribute (which shows weather the episode has terminated or not) and the empty python dictionary \"info\".\n",
        "\n",
        "\n",
        "  def render(self):                                                               # The \"render\" method is used to render the environment or to visually simulate the RL environment. It is computationally costly to render the environment and also it is not necessary to render this environment. This method is mostly used for debug the environment.\n",
        "        pass                                                                      # Here, \"pass\" keyword is used to just pass the method, this method () is not being used (Environment is not rendered).\n",
        "\n",
        "  def reset(self):                                                                # The \"reset\" method, which is called when a new episode is started. It gives the initial state to the RL agent.\n",
        "\n",
        "        global state_store\n",
        "\n",
        "        self.sstep = 0                                                            # The step counter initialized at zero.\n",
        "        self.state = np.array([0 for i in range(0, self.no_aa * self.no_materials)])                                               # Initial state, which is zero for every affected area and material, Arrangement is affected areas and then number of materials.\n",
        "        if self.is_eval:\n",
        "          state_store[0, :] = self.state\n",
        "        self.state_t = copy.deepcopy(self.state_temp)\n",
        "        # print(self.state_t, \"vself.state_tself.state_t\")\n",
        "        if self.is_loc_dec:\n",
        "\n",
        "          self.loc_state_temp = np.array([-1 for i in range(0, self.no_dcl_loc_aval)])        # assuming no local response centers are pre-eracted\n",
        "          print(self.loc_state_temp, \"self.loc_stateself.loc_stateself.loc_state\")\n",
        "          # self.loc_state_temp = np.array([1 for i in range(0, self.no_dcl_loc_aval)])        # if location allocation decisions are not to be included the this state will run\n",
        "\n",
        "          return np.concatenate(((((self.state.reshape(self.no_aa * self.no_materials,) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist)), repeats = self.no_aa * self.no_materials))) / (np.sum(self.demand_aa_mat, axis = 0) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist) / np.min(self.cap_consum_mat)), repeats = self.no_aa * self.no_materials)))) * (2 * np.ones((self.no_aa * self.no_materials,))) - 1), np.repeat(self.loc_state_temp, self.no_loc_dim_state), np.repeat(self.state_t, self.no_time_dim_state)))\n",
        "\n",
        "        else:\n",
        "\n",
        "          return np.concatenate(((((self.state.reshape(self.no_aa * self.no_materials,) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist)), repeats = self.no_aa * self.no_materials))) / (np.sum(self.demand_aa_mat, axis = 0) - (np.sum(self.demand_aa_mat, axis = 0) - self.number_of_allocation_time_periods * np.repeat(np.array(np.max(self.cap_dist) / np.min(self.cap_consum_mat)), repeats = self.no_aa * self.no_materials)))) * (2 * np.ones((self.no_aa * self.no_materials,))) - 1), np.repeat(self.state_t, self.no_time_dim_state)))\n",
        "\n",
        "class DenseSN(Dense):\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        input_dim = input_shape[-1]\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
        "                                      initializer=initializers.RandomNormal(0, 1),\n",
        "                                      name='kernel',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint, trainable=True)\n",
        "        self.bias = self.add_weight(shape=(self.units,),\n",
        "                                      initializer=initializers.RandomNormal(0, 1),\n",
        "                                      name='bias',\n",
        "                                      regularizer=self.bias_regularizer,\n",
        "                                      constraint=self.bias_constraint, trainable=True)\n",
        "\n",
        "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                                 initializer=initializers.RandomNormal(0, 1),\n",
        "                                 name='sn',\n",
        "                                 trainable=False)\n",
        "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
        "        # self.input_spec = InputSpec(min_ndim=2)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        def _l2normalize(v, eps=1e-12):\n",
        "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "        def power_iteration(W, u):\n",
        "            _u = u\n",
        "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
        "            _u = _l2normalize(K.dot(_v, W))\n",
        "            return _u, _v\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        #Flatten the Tensor\n",
        "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
        "        _u, _v = power_iteration(W_reshaped, self.u)\n",
        "        #Calculate Sigma\n",
        "        sigma=K.dot(_v, W_reshaped)\n",
        "        sigma=K.dot(sigma, K.transpose(_u))\n",
        "        #normalize it\n",
        "        W_bar = W_reshaped / sigma\n",
        "        #reshape weight tensor\n",
        "        if training in {0, False}:\n",
        "            W_bar = K.reshape(W_bar, W_shape)\n",
        "        else:\n",
        "            with tf.control_dependencies([self.u.assign(_u)]):\n",
        "                 W_bar = K.reshape(W_bar, W_shape)\n",
        "        output = K.dot(inputs, W_bar)\n",
        "\n",
        "        output = K.bias_add(output, self.bias, data_format='channels_last')\n",
        "        return output\n",
        "\n",
        "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================\n",
        "\n",
        "# The following implementation is done so as to use the spectral normalization for weight matrix for the affine layers.\n",
        "\n",
        "\n",
        "\"\"\"Contains the core layers: Dense, Dropout.\n",
        "\n",
        "Also contains their functional aliases.\n",
        "\"\"\"\n",
        "\n",
        "class SNDense(DenseSN, base.Layer):\n",
        "  \"\"\"Densely-connected layer class.\n",
        "\n",
        "  This layer implements the operation:\n",
        "  `outputs = activation(inputs * kernel + bias)`\n",
        "  Where `activation` is the activation function passed as the `activation`\n",
        "  argument (if not `None`), `kernel` is a weights matrix created by the layer,\n",
        "  and `bias` is a bias vector created by the layer\n",
        "  (only if `use_bias` is `True`).\n",
        "\n",
        "  Arguments:\n",
        "    units: Integer or Long, dimensionality of the output space.\n",
        "    activation: Activation function (callable). Set it to None to maintain a\n",
        "      linear activation.\n",
        "    use_bias: Boolean, whether the layer uses a bias.\n",
        "    kernel_initializer: Initializer function for the weight matrix.\n",
        "      If `None` (default), weights are initialized using the default\n",
        "      initializer used by `tf.compat.v1.get_variable`.\n",
        "    bias_initializer: Initializer function for the bias.\n",
        "    kernel_regularizer: Regularizer function for the weight matrix.\n",
        "    bias_regularizer: Regularizer function for the bias.\n",
        "    activity_regularizer: Regularizer function for the output.\n",
        "    kernel_constraint: An optional projection function to be applied to the\n",
        "        kernel after being updated by an `Optimizer` (e.g. used to implement\n",
        "        norm constraints or value constraints for layer weights). The function\n",
        "        must take as input the unprojected variable and must return the\n",
        "        projected variable (which must have the same shape). Constraints are\n",
        "        not safe to use when doing asynchronous distributed training.\n",
        "    bias_constraint: An optional projection function to be applied to the\n",
        "        bias after being updated by an `Optimizer`.\n",
        "    trainable: Boolean, if `True` also add variables to the graph collection\n",
        "      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
        "    name: String, the name of the layer. Layers with the same name will\n",
        "      share weights, but to avoid mistakes we require reuse=True in such cases.\n",
        "    _reuse: Boolean, whether to reuse the weights of a previous layer\n",
        "      by the same name.\n",
        "\n",
        "  Properties:\n",
        "    units: Python integer, dimensionality of the output space.\n",
        "    activation: Activation function (callable).\n",
        "    use_bias: Boolean, whether the layer uses a bias.\n",
        "    kernel_initializer: Initializer instance (or name) for the kernel matrix.\n",
        "    bias_initializer: Initializer instance (or name) for the bias.\n",
        "    kernel_regularizer: Regularizer instance for the kernel matrix (callable)\n",
        "    bias_regularizer: Regularizer instance for the bias (callable).\n",
        "    activity_regularizer: Regularizer instance for the output (callable)\n",
        "    kernel_constraint: Constraint function for the kernel matrix.\n",
        "    bias_constraint: Constraint function for the bias.\n",
        "    kernel: Weight matrix (TensorFlow variable or tensor).\n",
        "    bias: Bias vector, if applicable (TensorFlow variable or tensor).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, units,\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer=None,\n",
        "               bias_initializer=init_ops.random_normal_initializer(),\n",
        "               kernel_regularizer=init_ops.random_normal_initializer(),\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               trainable=True,\n",
        "               name=None,\n",
        "               **kwargs):\n",
        "    super(SNDense, self).__init__(units=units,\n",
        "                                activation=activation,\n",
        "                                use_bias=use_bias,\n",
        "                                kernel_initializer=kernel_initializer,\n",
        "                                bias_initializer=bias_initializer,\n",
        "                                kernel_regularizer=kernel_regularizer,\n",
        "                                bias_regularizer=bias_regularizer,\n",
        "                                activity_regularizer=activity_regularizer,\n",
        "                                kernel_constraint=kernel_constraint,\n",
        "                                bias_constraint=bias_constraint,\n",
        "                                trainable=trainable,\n",
        "                                name=name,\n",
        "                                **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def SNdense(\n",
        "    inputs, scope, units,\n",
        "    activation=None,\n",
        "    use_bias=True,\n",
        "    kernel_initializer=init_ops.random_normal_initializer(),\n",
        "    bias_initializer=init_ops.random_normal_initializer(),\n",
        "    kernel_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    trainable=True,\n",
        "    name=None,\n",
        "    reuse=None):\n",
        "  \"\"\"Functional interface for the densely-connected layer.\n",
        "\n",
        "  This layer implements the operation:\n",
        "  `outputs = activation(inputs * kernel + bias)`\n",
        "  where `activation` is the activation function passed as the `activation`\n",
        "  argument (if not `None`), `kernel` is a weights matrix created by the layer,\n",
        "  and `bias` is a bias vector created by the layer\n",
        "  (only if `use_bias` is `True`).\n",
        "\n",
        "  Arguments:\n",
        "    inputs: Tensor input.\n",
        "    units: Integer or Long, dimensionality of the output space.\n",
        "    activation: Activation function (callable). Set it to None to maintain a\n",
        "      linear activation.\n",
        "    use_bias: Boolean, whether the layer uses a bias.\n",
        "    kernel_initializer: Initializer function for the weight matrix.\n",
        "      If `None` (default), weights are initialized using the default\n",
        "      initializer used by `tf.compat.v1.get_variable`.\n",
        "    bias_initializer: Initializer function for the bias.\n",
        "    kernel_regularizer: Regularizer function for the weight matrix.\n",
        "    bias_regularizer: Regularizer function for the bias.\n",
        "    activity_regularizer: Regularizer function for the output.\n",
        "    kernel_constraint: An optional projection function to be applied to the\n",
        "        kernel after being updated by an `Optimizer` (e.g. used to implement\n",
        "        norm constraints or value constraints for layer weights). The function\n",
        "        must take as input the unprojected variable and must return the\n",
        "        projected variable (which must have the same shape). Constraints are\n",
        "        not safe to use when doing asynchronous distributed training.\n",
        "    bias_constraint: An optional projection function to be applied to the\n",
        "        bias after being updated by an `Optimizer`.\n",
        "    trainable: Boolean, if `True` also add variables to the graph collection\n",
        "      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
        "    name: String, the name of the layer.\n",
        "    reuse: Boolean, whether to reuse the weights of a previous layer\n",
        "      by the same name.\n",
        "\n",
        "  Returns:\n",
        "    Output tensor the same shape as `inputs` except the last dimension is of\n",
        "    size `units`.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if eager execution is enabled.\n",
        "  \"\"\"\n",
        "  layer = SNDense(units,\n",
        "                activation=None,\n",
        "                use_bias=True,\n",
        "                kernel_initializer=init_ops.random_normal_initializer(),\n",
        "                bias_initializer=init_ops.random_normal_initializer(),\n",
        "                kernel_regularizer=None,\n",
        "                bias_regularizer=None,\n",
        "                activity_regularizer=None,\n",
        "                kernel_constraint=None,\n",
        "                bias_constraint=None,\n",
        "                trainable=True,\n",
        "                name=None,\n",
        "                _scope=scope,\n",
        "                _reuse=None\n",
        "                )\n",
        "  return layer.apply(inputs)\n",
        "\n",
        "# to be used for the calculation of q-values\n",
        "def linear(input_tensor, scope, n_hidden, *, init_scale=1.0, init_bias=0.0, reuse = False):\n",
        "    \"\"\"\n",
        "    Creates a fully connected layer for TensorFlow\n",
        "    :param input_tensor: (TensorFlow Tensor) The input tensor for the fully connected layer\n",
        "    :param scope: (str) The TensorFlow variable scope\n",
        "    :param n_hidden: (int) The number of hidden neurons\n",
        "    :param init_scale: (int) The initialization scale\n",
        "    :param init_bias: (int) The initialization offset bias\n",
        "    :return: (TensorFlow Tensor) fully connected layer\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope, reuse = reuse):\n",
        "        n_input = input_tensor.get_shape()[1].value\n",
        "        weight = tf.get_variable(\"w\", [n_input, n_hidden], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=seed))\n",
        "        bias = tf.get_variable(\"b\", [n_hidden], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=seed))\n",
        "        return tf.matmul(input_tensor, weight) + bias\n",
        "\n",
        "# layer normalization implementation in tensorflow\n",
        "def _ln(input_tensor, scope, epsilon=1e-5, axis=None):\n",
        "    \"\"\"\n",
        "    Apply layer normalisation.\n",
        "    :param input_tensor: (TensorFlow Tensor) The input tensor for the Layer normalization\n",
        "    :param gain: (TensorFlow Tensor) The scale tensor for the Layer normalization\n",
        "    :param bias: (TensorFlow Tensor) The bias tensor for the Layer normalization\n",
        "    :param epsilon: (float) The epsilon value for floating point calculations\n",
        "    :param axes: (tuple, list or int) The axes to apply the mean and variance calculation\n",
        "    :return: (TensorFlow Tensor) a normalizing layer\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope):\n",
        "      gain = tf.get_variable(\"gain_layer_normalization\", input_tensor.get_shape()[-1],\n",
        "                                 dtype=input_tensor.dtype, initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=seed, dtype=tf.dtypes.float32))\n",
        "      bias = tf.get_variable(\"bias_layer_normalization\", input_tensor.get_shape()[-1],\n",
        "                                 dtype=input_tensor.dtype, initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=seed, dtype=tf.dtypes.float32))\n",
        "      if axis is None:\n",
        "          axis = [1]\n",
        "      mean, variance = tf.nn.moments(input_tensor, axes=axis, keep_dims=True)\n",
        "      input_tensor = (input_tensor - mean) / tf.sqrt(variance + epsilon)\n",
        "      input_tensor = input_tensor * gain + bias\n",
        "      return input_tensor\n",
        "\n",
        "# implementation of prelu activation function, as in tensorflow version 1.15, perelu is not implemented\n",
        "import tensorflow as tf\n",
        "def prelu(_x, scope=None):\n",
        "    \"\"\"parametric ReLU activation\"\"\"\n",
        "    with tf.variable_scope(name_or_scope = scope, default_name=\"prelu\"):\n",
        "        _alpha = tf.get_variable(\"prelu\", _x.get_shape()[-1],\n",
        "                                 dtype=_x.dtype, initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=seed, dtype=tf.dtypes.float32))\n",
        "        return tf.maximum(0.0, _x) + _alpha * tf.minimum(0.0, _x)\n",
        "\n",
        "\n",
        "if typing.TYPE_CHECKING:\n",
        "    from stable_baselines.common.base_class import BaseRLModel  # pytype: disable=pyi-error\n",
        "\n",
        "from stable_baselines.common.callbacks import CheckpointCallback\n",
        "\n",
        "envv = Envir(accessibality_based_delivery_cost_weight = 1 / 3, deprivation_cost_weight = 1 / 3,\n",
        "            reward_scaling = 1, reallocate_t = 5, no_time_dim_state = 5, no_loc_dim_state = 5, is_eval = True,\n",
        "             is_loc_dec = False)\n",
        "\n",
        "# for polting the convergence curve\n",
        "ws.append((\"Data for convergence curves: \",))\n",
        "ws.append((\"Wall Time\", \"Step\", \"Value\"))   # for the creation of header in te excel sheet\n",
        "\n",
        "class CheckpointCallback(CheckpointCallback):\n",
        "\n",
        "  def __init__(self, save_freq, save_path, st_time, name_prefix = f'model_ppo_', verbose = 0):\n",
        "          super(CheckpointCallback, self).__init__(save_freq, save_path, name_prefix, verbose)\n",
        "\n",
        "          envv = Envir(accessibality_based_delivery_cost_weight = 1 / 3, deprivation_cost_weight = 1 / 3,\n",
        "          reward_scaling = 1, reallocate_t = 5, no_time_dim_state = 5, no_loc_dim_state = 5,\n",
        "          is_eval = False, is_loc_dec = False)\n",
        "\n",
        "          verbose = 0\n",
        "          self.env = envv\n",
        "          self.np_arr_eval = np.zeros((int(10 ** 5),))\n",
        "          self.save_freq = save_freq\n",
        "          self.initial_steps_episod_stall = 10 ** 6\n",
        "          self.delta_episod_stall = 50\n",
        "          self.n_episod_stall = 200\n",
        "\n",
        "          self.save_path = save_path\n",
        "          self.name_prefix = name_prefix\n",
        "          self.count_eval = 0\n",
        "          self.term_n_steps = 0\n",
        "          self.st_time = st_time\n",
        "\n",
        "\n",
        "  def _on_step(self) -> bool:\n",
        "\n",
        "          global envv\n",
        "          global model_ppo\n",
        "\n",
        "          # for te evaluatoin of the agent after predetermined number of steps\n",
        "          if self.n_calls % (1000 * number_of_allocation_time_periods) == 0:\n",
        "            eval_tuple = stable_baselines.common.evaluation.evaluate_policy(model = model_ppo, env = envv, n_eval_episodes= 1,\n",
        "                                                                          deterministic = True, return_episode_rewards = True)\n",
        "            ws.append((time.time(), self.n_calls, eval_tuple[0][0]))\n",
        "\n",
        "          if (self.n_calls > self.initial_steps_episod_stall) and (self.n_calls % self.save_freq == 0):\n",
        "              path = os.path.join(self.save_path, '{}_{}_steps'.format(self.name_prefix, self.num_timesteps))\n",
        "              # self.model.save(path)\n",
        "              self.np_arr_eval[int(self.count_eval)] = evaluate_policy(model = self.model, env = self.env, n_eval_episodes= 1,\n",
        "                              deterministic = True, return_episode_rewards = True)[0][0]\n",
        "\n",
        "              if (self.count_eval >= self.n_episod_stall):\n",
        "                y = np.sort(np.abs(self.np_arr_eval[int(self.count_eval - self.n_episod_stall) : int(self.count_eval)] - self.np_arr_eval[int(self.count_eval)]))[0 : self.n_episod_stall - 100]\n",
        "                if ((time.time() - self.st_time) >= 18000) or (np.max(y) <= self.delta_episod_stall):\n",
        "                  self.term_n_steps = self.n_calls\n",
        "                  return False\n",
        "              self.count_eval = self.count_eval + 1\n",
        "              if self.verbose > 1:\n",
        "                  print(\"Saving model checkpoint to {}\".format(path))\n",
        "          return True\n",
        "\n",
        "import sys\n",
        "maxSize = sys.float_info.max\n",
        "minSize = sys.float_info.min\n",
        "criteria_obj_hyperparameter_previous = sys.float_info.max\n",
        "\n",
        "# While training, first modify the \"adam.py\" file at \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/adam.py\" by changing the implementation of \"adam\" optimizer so that it will execute \"amsgrad\" optimizer, and then also modify the file \"optimizers.py\" at \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizers.py\" by changing the variable \"amsgrad\" to \"True\" in the \"__init__\" method of \"adam\" optimizer.\n",
        "\n",
        "# PPO MLP agent (Without quantization)\n",
        "\n",
        "# env = make_vec_env.VecEnv(env, n_envs=4)\n",
        "# env = DummyVecEnv([lambda: env])\n",
        "\n",
        "previous_reward = -maxSize                 # Maximum size of the integer possible is used as the starting value for \"previous_reward\", so as to decide, wether to save the model or not.\n",
        "\n",
        "tiral_counter_saving_model = 0                  # Trial counter for determining weather a trial is the first trial or not for saving the model\n",
        "st = time.time()\n",
        "\n",
        "def objective(trial, fast_check=True, target_meter=0, return_info=False) -> float:\n",
        "\n",
        "  # Only one of \"dense_connections\", \"residual_connections\" and \"not_dense_connections_nor_residual_connections\" will be true\n",
        "  dense_connections = False\n",
        "  residual_connections = True\n",
        "  not_dense_connections_nor_residual_connections = False\n",
        "\n",
        "\n",
        "  # Only one of \"use_attention_layer\", \"use_transformer_layer\" and \"not_attention_layer_nor_transformer_layer\" will be true\n",
        "  use_attention_layer = False\n",
        "  use_transformer_layer = False\n",
        "  not_attention_layer_nor_transformer_layer = True\n",
        "\n",
        "\n",
        "  # if previous decison is to use attention layer, then only one of the following must be true\n",
        "  use_attention_Luong_style =  False                                       # for dot-product and scaled dot-product attention layer (we will do hyperparameter tuning on weather to do scaling or not)\n",
        "  use_attention_Bahdanau_style = False                                # for additive attention layer (we will do hyperparameter tuning on weather to do scaling or not)\n",
        "\n",
        "  global previous_reward\n",
        "  global tiral_counter_saving_model\n",
        "\n",
        "  # Int parameters\n",
        "  noptepochs = trial.suggest_int('noptepochs', 3, 3)            # number of epochs to use are to be optimized\n",
        "  number_layers_policy_net = trial.suggest_int('number_layers_policy_net', 4, 4)           # total nubmer of layers to be used in the policy deep neural network\n",
        "  number_layers_value_net = trial.suggest_int('number_layers_value_net', 5, 5)         # total nubmer of layers to be used in the value function deep neural network\n",
        "\n",
        "  # Uniform parameter\n",
        "  learning_rate = trial.suggest_uniform('learning_rate', 0.0002, 0.0003)               # learning rate to be used for both policy and value function deep neural network\n",
        "  gamma = trial.suggest_uniform('gamma', 1, 1)            # discound factor\n",
        "\n",
        "  global no_aa\n",
        "  global no_materials\n",
        "  global no_dcl_loc_aval\n",
        "  global number_of_allocation_time_periods\n",
        "  global reward_scaling\n",
        "\n",
        "  input_shape = no_aa * no_materials + 5                           # The shape of the observation at a single time step. It is also used to determine the reward scaling factor.\n",
        "\n",
        "  # here we are setting only the first and the last layers' number of neurons as the number of neurons in the hidden layers are set during the definition of the hidden layers\n",
        "  n_hiddens_first_policy = trial.suggest_int(\"n_hiddens_first_policy\", input_shape + 2, input_shape + 2)\n",
        "  n_hiddens_first_value = trial.suggest_int(\"n_hiddens_first_value\", input_shape + 2, input_shape + 2)\n",
        "\n",
        "  n_hiddens_second_last_policy = trial.suggest_int(\"n_hiddens_second_last_policy\", input_shape + 2, input_shape + 2)\n",
        "  n_hiddens_last_value = 1\n",
        "\n",
        "\n",
        "  class KerasPolicy(ActorCriticPolicy):\n",
        "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **kwargs):\n",
        "        super(KerasPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse, scale=False)\n",
        "\n",
        "        n_hiddens_last_policy = ac_space.n\n",
        "\n",
        "        obs_ph, processed_obs = observation_input(ob_space, n_batch, name='Ob', scale=False)               # As the observation space is box, so input is scaled between 0 and 1.\n",
        "\n",
        "        self._obs_ph = obs_ph\n",
        "\n",
        "        with tf.variable_scope(\"model\", reuse=reuse):\n",
        "            extracted_features = tf.layers.flatten(processed_obs)\n",
        "            pi_h = extracted_features\n",
        "\n",
        "            # definition of attention mechinism\n",
        "            if use_attention_Luong_style and use_attention_layer:\n",
        "              attention_input_stack_policy = tf.expand_dims(pi_h, 1)\n",
        "              output_policy_attention_1_attention_policy = tf.keras.layers.Attention(use_scale = False)([attention_input_stack_policy, attention_input_stack_policy, attention_input_stack_policy])              # for Luong style attention\n",
        "              output_policy_attention_1 = tf.squeeze(output_policy_attention_1_attention_policy, [1])\n",
        "            elif use_attention_Bahdanau_style and use_attention_layer:\n",
        "              attention_input_stack_policy = tf.expand_dims(pi_h, 1)\n",
        "              output_policy_attention_1_attention_policy = tf.keras.layers.AdditiveAttention(use_scale = False)([attention_input_stack_policy, attention_input_stack_policy, attention_input_stack_policy])              # for Luong style attention\n",
        "              output_policy_attention_1 = tf.squeeze(output_policy_attention_1_attention_policy, [1])\n",
        "            else:\n",
        "              output_policy_attention_1 = pi_h\n",
        "\n",
        "            # reshaping a tensor to be a 2-D tensor\n",
        "\n",
        "            # the start of the definition of the first layer\n",
        "\n",
        "            output_policy_layer_norm_1 = _ln(output_policy_attention_1, \"output_policy_layer_norm_1\")\n",
        "            output_policy_dense_1 = linear(output_policy_layer_norm_1, \"output_policy_dense_1\", n_hiddens_first_policy)\n",
        "            output_policy_prelu_1 = prelu(output_policy_dense_1)\n",
        "            output_policy_dense_1_2 = linear(output_policy_prelu_1, \"output_policy_dense_1_2\", n_hiddens_first_policy)\n",
        "            # the end of the definition of the first layer\n",
        "            if dense_connections:\n",
        "              output_policy_concatenate_1 = tf.concat([output_policy_dense_1_2, pi_h], axis = 1)                  # contatination is done so as to implement dense connections\n",
        "            elif residual_connections:\n",
        "              output_policy_concatenate_1 = tf.add(output_policy_dense_1_2, linear(output_policy_attention_1, \"output_policy_concatenate_1\", n_hiddens_first_policy))                  # contatination is done so as to implement dense connections\n",
        "            elif not_dense_connections_nor_residual_connections:\n",
        "              output_policy_concatenate_1 = output_policy_dense_1_2\n",
        "            # the following \"for\" loop will define the hidden layers\n",
        "            layers_policy = []\n",
        "            for i in range(2, number_layers_policy_net + 1):\n",
        "              layers_policy.append(trial.suggest_int(str(i) + \"_hidden_layer_policy_function_number_of_neurons\", n_hiddens_last_policy, n_hiddens_last_policy))           # number of neurons in the hidden layers of the policy network\n",
        "              locals()[f\"output_policy_layer_norm_{i}\"] = _ln(locals()[f\"output_policy_concatenate_{i - 1}\"], f\"output_policy_layer_norm_{i}\")\n",
        "              locals()[f\"output_policy_dense_{i}\"] = SNdense(locals()[f\"output_policy_layer_norm_{i}\"], f\"output_policy_dense_{i}\", layers_policy[i - 2])\n",
        "              locals()[f\"output_policy_prelu_{i}\"] = prelu(locals()[f\"output_policy_dense_{i}\"])\n",
        "              locals()[f\"output_policy_dense_{i}_2\"] = SNdense(locals()[f\"output_policy_prelu_{i}\"], f\"output_policy_dense_{i}_2\", layers_policy[i - 2])\n",
        "              if dense_connections:\n",
        "                locals()[f\"output_policy_concatenate_{i}\"] = tf.concat([locals()[f\"output_policy_dense_{i}_2\"], pi_h], axis = 1)                  # contatination is done so as to implement dense connections\n",
        "              elif residual_connections:\n",
        "                locals()[f\"output_policy_concatenate_{i}\"] = tf.add(locals()[f\"output_policy_dense_{i}_2\"], SNdense(locals()[f\"output_policy_concatenate_{i - 1}\"], f\"output_policy_concatenate_{i}\", layers_policy[i - 2]))                  # contatination is done so as to implement dense connections\n",
        "              elif not_dense_connections_nor_residual_connections:\n",
        "                locals()[f\"output_policy_concatenate_{i}\"] = locals()[f\"output_policy_dense_{i}_2\"]\n",
        "            # the start of the definition of the last layer\n",
        "            locals()[f\"output_policy_layer_norm_{number_layers_policy_net + 1}\"] = _ln(locals()[f\"output_policy_concatenate_{i}\"], f\"output_policy_layer_norm_{number_layers_policy_net + 1}\")\n",
        "            locals()[f\"output_policy_dense_{number_layers_policy_net + 1}\"] = linear(locals()[f\"output_policy_layer_norm_{number_layers_policy_net + 1}\"], f\"output_policy_dense_{number_layers_policy_net + 1}\", n_hiddens_last_policy)\n",
        "            locals()[f\"output_policy_prelu_{number_layers_policy_net + 1}\"] = prelu(locals()[f\"output_policy_dense_{number_layers_policy_net + 1}\"])\n",
        "            locals()[f\"output_policy_dense_{number_layers_policy_net + 1}_2\"] = linear(locals()[f\"output_policy_prelu_{number_layers_policy_net + 1}\"], f\"output_policy_dense_{number_layers_policy_net + 1}_2\", n_hiddens_last_policy)\n",
        "            if residual_connections:\n",
        "              output_policy_dense = tf.add(locals()[f\"output_policy_dense_{number_layers_policy_net + 1}_2\"], linear(locals()[f\"output_policy_concatenate_{i}\"], \"output_policy_dense\", n_hiddens_last_policy))                  # contatination is done so as to implement dense connections\n",
        "            elif not_dense_connections_nor_residual_connections:\n",
        "              output_policy_dense = locals()[f\"output_policy_dense_{number_layers_policy_net + 1}_2\"]\n",
        "            # the end of the definition of the last layer\n",
        "\n",
        "            output_policy_output = output_policy_dense\n",
        "            # the end of the definition of the last layer\n",
        "\n",
        "            pi_latent = output_policy_output\n",
        "\n",
        "\n",
        "            # Value Function Deep Neural Network.\n",
        "            vf_h = extracted_features\n",
        "            output_value_1 = vf_h                 # input to the value network\n",
        "            # the start of the definition of the first layer\n",
        "\n",
        "            # implementation of attention layer in state value network\n",
        "            if use_attention_Luong_style and use_attention_layer:\n",
        "              attention_input_stack_value = tf.expand_dims(vf_h, 1)\n",
        "              output_attention_1_attention_value = tf.keras.layers.Attention(use_scale = False)([attention_input_stack_value, attention_input_stack_value, attention_input_stack_value])              # for Luong style attention\n",
        "              output_attention_1 = tf.squeeze(output_attention_1_attention_value, [1])\n",
        "            elif use_attention_Bahdanau_style and use_attention_layer:\n",
        "              attention_input_stack_value = tf.expand_dims(vf_h, 1)\n",
        "              output_attention_1_attention_value = tf.keras.layers.AdditiveAttention(use_scale = False)([attention_input_stack_value, attention_input_stack_value, attention_input_stack_value])              # for Luong style attention\n",
        "              output_attention_1 = tf.squeeze(output_attention_1_attention_value, [1])\n",
        "            else:\n",
        "              output_attention_1 = vf_h\n",
        "\n",
        "            output_value_layer_norm_1 = _ln(output_attention_1, \"output_value_layer_norm_1\")\n",
        "            output_value_dense_1 = linear(output_value_layer_norm_1, \"output_value_dense_1\", n_hiddens_first_value)\n",
        "            output_value_prelu_1 = prelu(output_value_dense_1)\n",
        "            output_value_dense_1_2 = linear(output_value_prelu_1, \"output_value_dense_1_2\", n_hiddens_first_value)\n",
        "            # the end of the definition of the first layer\n",
        "            if dense_connections:\n",
        "              output_value_concatenate_1 = tf.concat([output_value_dense_1_2, vf_h], axis = 1)\n",
        "            elif residual_connections:\n",
        "              output_value_concatenate_1 = tf.add(output_value_dense_1_2, linear(output_attention_1, \"output_value_concatenate_1\", n_hiddens_first_value))\n",
        "            elif not_dense_connections_nor_residual_connections:\n",
        "              output_value_concatenate_1 = output_value_dense_1_2\n",
        "            # the following \"for\" loop will define the hidden layers in the value function\n",
        "\n",
        "            # assertion of some conditions related to number of hidden neurons in the value function network\n",
        "            assert (2 / 3) * input_shape + 1 > min(input_shape, 1), \"the number of hidden neurons should be greater then the minimum of input dimension and the output dimension\"\n",
        "            assert (2 / 3) * input_shape + 1 < max(input_shape, 1), \"the number of hidden neurons should be less then the maximum of input dimension and the output dimension\"\n",
        "            assert (2 / 3) * input_shape + 1 < 2 * input_shape, \"the number of hidden neurons should be less then two times the input dimension\"\n",
        "\n",
        "            layers_value = []\n",
        "            for i in range(2, number_layers_value_net + 1):\n",
        "              layers_value.append(trial.suggest_int(str(i) + \"_value_function\", (2 / 3) * input_shape + 1, (2 / 3) * input_shape + 1))\n",
        "              locals()[f\"output_value_layer_norm_{i}\"] = _ln(locals()[f\"output_value_concatenate_{i - 1}\"], f\"output_value_layer_norm_{i}\")\n",
        "              locals()[f\"output_value_dense_{i}\"] = SNdense(locals()[f\"output_value_layer_norm_{i}\"], f\"output_value_dense_{i}\", layers_value[i - 2])\n",
        "              locals()[f\"output_value_prelu_{i}\"] = prelu(locals()[f\"output_value_dense_{i}\"])\n",
        "              locals()[f\"output_value_dense_{i}_2\"] = SNdense(locals()[f\"output_value_prelu_{i}\"], f\"output_value_dense_{i}_2\", layers_value[i - 2])\n",
        "              if dense_connections:\n",
        "                locals()[f\"output_value_concatenate_{i}\"] = tf.concat([locals()[f\"output_value_dense_{i}_2\"], vf_h], axis = 1)\n",
        "              elif residual_connections:\n",
        "                locals()[f\"output_value_concatenate_{i}\"] = tf.add(locals()[f\"output_value_dense_{i}_2\"], SNdense(locals()[f\"output_value_concatenate_{i - 1}\"], f\"output_value_concatenate_{i}\", layers_value[i - 2]))\n",
        "              elif not_dense_connections_nor_residual_connections:\n",
        "                locals()[f\"output_value_concatenate_{i}\"] = locals()[f\"output_value_dense_{i}_2\"]\n",
        "            # start of the definition of the last layer\n",
        "            output_value_layer_norm_last = _ln(locals()[f\"output_value_concatenate_{number_layers_value_net}\"], \"output_value_layer_norm_last\")\n",
        "            output_value_dense_last = linear(output_value_layer_norm_last, \"output_value_dense_last\", n_hiddens_last_value)\n",
        "            output_value_prelu_last = prelu(output_value_dense_last)\n",
        "            output_value_dense_last_2 = linear(output_value_prelu_last, \"output_value_dense_last_2\", n_hiddens_last_value)\n",
        "            if residual_connections:\n",
        "              output_value_dense_last_3 = tf.add(output_value_dense_last_2, linear(locals()[f\"output_value_concatenate_{number_layers_value_net}\"], \"output_value_dense_last_3\", n_hiddens_last_value))\n",
        "            elif not_dense_connections_nor_residual_connections:\n",
        "              output_value_dense_last_2 = output_value_dense_last_2\n",
        "            # end of the definition of the last layer\n",
        "\n",
        "            value_fn_output = output_value_dense_last_3\n",
        "\n",
        "            vf_latent = locals()[f\"output_value_concatenate_{number_layers_value_net}\"]\n",
        "\n",
        "\n",
        "            self._proba_distribution, self._policy, self.q_value = self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
        "\n",
        "        self._value_fn = value_fn_output\n",
        "        self._setup_init()\n",
        "\n",
        "\n",
        "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
        "        if deterministic:\n",
        "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
        "                                                   {self.obs_ph: obs})\n",
        "        else:\n",
        "          # probablistic policy is used during training to enhance exploration of reinforcement learning\n",
        "          action, value, neglogp = self.sess.run([self.action, self._value_flat, self.neglogp],\n",
        "                                                    {self.obs_ph: obs})\n",
        "        return action, value, self.initial_state, neglogp\n",
        "\n",
        "    def proba_step(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
        "\n",
        "    def value(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self._value_flat, {self.obs_ph: obs})\n",
        "\n",
        "\n",
        "  # a seprate envrinment is defined for evaluation of the learned policy\n",
        "  # eval_env = Envir(accessibality_based_delivery_cost_weight = 1 / 2, deprivation_cost_weight = 1 / 2, reward_scaling = 10 ** 10,\n",
        "  #              reallocate_t = 5, no_time_dim_state = 2, no_loc_dim_state = 2)\n",
        "  # Stop training when the model reaches the reward threshold\n",
        "  # callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-1 * 10 ** -75, verbose=1)\n",
        "  # eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1)\n",
        "\n",
        "  env = Envir(accessibality_based_delivery_cost_weight = 1 / 3, deprivation_cost_weight = 1 / 3,\n",
        "            reward_scaling = reward_scaling, reallocate_t = 5, no_time_dim_state = 5, no_loc_dim_state = 5,\n",
        "            is_eval = False, is_loc_dec = False)\n",
        "\n",
        "  global checkpoint_callback\n",
        "\n",
        "  checkpoint_callback = CheckpointCallback(number_of_allocation_time_periods * 100, '/content/check_points', st)\n",
        "\n",
        "  # checkpoint_callback = CheckpointCallback(envv, save_freq = 500, save_path = '/content/check_points',\n",
        "  #                                        name_prefix = f'model_ppo_', initial_steps_episod_stall = 6000,\n",
        "  #                                        n_episod_stall = 2, delta_episod_stall = 1000)\n",
        "\n",
        "  save_temp = np.sum(cap_dist) / cap_consum_mat[0]\n",
        "\n",
        "  global model_ppo\n",
        "\n",
        "  # model_ppo = ACKTR(policy, env, gamma=gamma, nprocs=None, n_steps=20, ent_coef=0.01, vf_coef=0.5, vf_fisher_coef=1.0,\n",
        "  #                   learning_rate=0.25, max_grad_norm=0.5, kfac_clip=0.001, lr_schedule='linear', verbose=2,\n",
        "  #                   tensorboard_log=f\"/content/drive/MyDrive/drl_ppo_({no_aa},_{list(save_temp)}_{number_of_allocation_time_periods})__{seed}_tensorboard_convergence_curves/\",\n",
        "  #                   _init_setup_model=True, async_eigen_decomp=False, kfac_update=1, gae_lambda=None, policy_kwargs=None,\n",
        "  #                   full_tensorboard_log=False, seed=seed, n_cpu_tf_sess=1)\n",
        "\n",
        "  # model_ppo = A2C(KerasPolicy, env, gamma=gamma, n_steps=5, ent_coef=0.01, learning_rate=0.00025, vf_coef=0.5,\n",
        "                              # max_grad_norm=0.5, alpha=0.99, momentum=0.0, epsilon=1e-05, lr_schedule='constant', verbose=2,\n",
        "                              # tensorboard_log=f\"/content/drive/MyDrive/drl_ppo_({no_aa},_{list(save_temp)}_{number_of_allocation_time_periods})__{seed}_tensorboard_convergence_curves/\",\n",
        "                              # _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False, seed=seed, n_cpu_tf_sess=None)\n",
        "\n",
        "  # model_ppo = TRPO(KerasPolicy, env, gamma=gamma, timesteps_per_batch=1024, max_kl=0.01, cg_iters=10,\n",
        "  #                  lam=0.95, entcoeff=0.01, cg_damping=0.01, vf_stepsize=0.0003, vf_iters=3, verbose=2,\n",
        "  #                  tensorboard_log=f\"/content/drive/MyDrive/drl_ppo_({no_aa},_{list(save_temp)}_{number_of_allocation_time_periods})__{seed}_tensorboard_convergence_curves/\",\n",
        "  #                  _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False,\n",
        "  #                  seed=seed, n_cpu_tf_sess=1)\n",
        "\n",
        "  model_ppo = PPO2(KerasPolicy, env, gamma=gamma, n_steps=128, ent_coef=0.01, learning_rate=0.00025,\n",
        "                   vf_coef=0.5, max_grad_norm=0.5, lam=0.95, nminibatches=4, noptepochs=4, cliprange=0.2,\n",
        "                   cliprange_vf=None, verbose=0,\n",
        "                   tensorboard_log = None,\n",
        "                   _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False, seed=seed,\n",
        "                   n_cpu_tf_sess=None)\n",
        "\n",
        "  model_ppo.learn(int(1e10), log_interval = 1, callback = checkpoint_callback)\n",
        "\n",
        "  number_eval_episodes = 1\n",
        "  avg_reward = 0\n",
        "  for i in range(number_eval_episodes):\n",
        "    accm_reward = 0\n",
        "    obs = env.reset()\n",
        "    for j in range(6):                             # here, the value to be put is equal to the \"number of time periods\" in an episod or planning horizon\n",
        "        action, _states = model_ppo.predict(obs)\n",
        "        obs, rewards, dones, info = env.step(action)\n",
        "        accm_reward = accm_reward + rewards\n",
        "    avg_reward = avg_reward + accm_reward\n",
        "  avg_reward = avg_reward / number_eval_episodes                     # calculating the average episode reward\n",
        "\n",
        "  if tiral_counter_saving_model == 0:\n",
        "    model_ppo.save(f\"/content/drive/MyDrive/drl_ppo_({no_aa},_{list(save_temp)}_{number_of_allocation_time_periods})__{seed}_tensorboard_convergence_curves_trained_policy\")\n",
        "  elif avg_reward > previous_reward:\n",
        "    model_ppo.save(\"ppo2_humanitarian_logistics_location_resource_allocation_without_quantization\")\n",
        "\n",
        "  previous_reward = avg_reward\n",
        "  tiral_counter_saving_model = tiral_counter_saving_model + 1                     # Updating trial counter to identify particular onward is not the first trial.\n",
        "  return avg_reward\n",
        "\n",
        "study_ppo_without_quantization = optuna.create_study(direction = \"maximize\", study_name = \"ppo_without_quantization_\", sampler = optuna.samplers.TPESampler(), pruner = optuna.pruners.MedianPruner)\n",
        "study_ppo_without_quantization.optimize(objective, n_trials = 1, timeout = int(1e10))\n",
        "print(np.abs(st - time.time()))\n",
        "\n",
        "# print(accm_reward)\n",
        "\n",
        "# after how many steps the RL training is terminated (after satisfaction of our training termination critrion)\n",
        "\n",
        "# number of time steps for which te algorithm is trained\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Training Time: \", np.abs(st - time.time())))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Number of steps for training: \", checkpoint_callback.term_n_steps))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "print(checkpoint_callback.term_n_steps)\n",
        "\n",
        "n_eval_ep = 1\n",
        "\n",
        "obj_val = 0\n",
        "\n",
        "ws.append((\"Stagewise objective function value or reward:\",))\n",
        "\n",
        "for i in range(0, n_eval_ep):     # loop over the number of episodes\n",
        "  obs = envv.reset()\n",
        "  for j in range(0, envv.number_of_allocation_time_periods):     # loop over all the time peirods inside an episode\n",
        "    action, _states = model_ppo.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = envv.step(action)\n",
        "\n",
        "    # storing all the stage wise objective values into an excel sheet\n",
        "    ws.append((f\"Stagewise objective function value at stage {j + 1}: \", -1 * reward))\n",
        "\n",
        "    obj_val = obj_val + (gamma ** j) * reward\n",
        "\n",
        "# storing all the objective function valeu (sum of all the rewards) into an excel sheet\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"Objective function value: \", -1 * obj_val))\n",
        "print(-1 * obj_val)\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "# storing all the state variables value to an excel sheet\n",
        "ws.append((\"State Variables\",))\n",
        "for i in state_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "# storing all the agent's actions into an excel sheet\n",
        "ws.append((\"Decision Variables\",))\n",
        "for i in act_store:\n",
        "  ws.append(tuple(i))\n",
        "\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "ws.append((\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"))\n",
        "\n",
        "# saving of the excel workbook at the end of the last experient of an instance, the workbook is saved with the abbreviaed notation of the problem instance as the name\n",
        "save_temp = float((sum(cap_dist) / max(cap_consum_mat))[0])\n",
        "wb.save(f'/content/drive/MyDrive/drl_ppo_({no_aa},_{[save_temp]},_{number_of_allocation_time_periods})_{seed}_excel_convergence_curves.xlsx')      # this statement wil only be exceuted at the end of the last experieent"
      ],
      "metadata": {
        "id": "s1aXk1oofR79"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}